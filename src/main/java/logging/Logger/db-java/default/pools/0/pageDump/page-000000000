/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac-1.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/Users/burakyetistiren/Desktop/Users/burakyetistiren/Users/apple.awt.application.nameclassworlds.conffile.encodingUTF-8file.separatorftp.nonProxyHostsguice.disable.misplaced.annotation.checkhttp.nonProxyHostsjava.class.path/opt/homebrew/Cellar/maven/3.9.7/libexec/boot/plexus-classworlds-2.8.0.jarjava.class.version65.0java.home/Library/Java/JavaVirtualMachines/openlogic-openjdk-21.jdk/Contents/Homejava.io.tmpdir/var/folders/fg/w4tmrn0j6zl36ffgcnsyd26h0000gn/T/java.library.path/Users/burakyetistiren/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.java.runtime.nameOpenJDK Runtime Environmentjava.runtime.version21.0.3+9-adhoc.admin.jdk21ujava.specification.nameJava Platform API Specificationjava.specification.vendorOracle Corporationjava.specification.version21java.vendor.url.bughttps://techsupport.roguewave.com/java.vendor.urlhttps://openlogic.com/java.vendor.versionOpenLogic-OpenJDKjava.vendorOpenLogicjava.version.date2024-04-16java.version21.0.3java.vm.compressedOopsModeZero basedjava.vm.infomixed mode, sharingjava.vm.nameOpenJDK 64-Bit Server VMjava.vm.specification.nameJava Virtual Machine Specificationjava.vm.specification.vendorjava.vm.specification.versionjava.vm.vendorjava.vm.versionjdk.debugreleaselibrary.jansi.pathline.separator
maven.confmaven.homemaven.multiModuleProjectDirectorynative.encodingos.archx86_64os.detected.archos.detected.classifierosx-x86_64os.detected.nameosxos.detected.version.major14os.detected.version.minor1os.detected.version14.1os.nameMac OS Xos.versionpath.separator:socksNonProxyHostsstderr.encodingstdout.encodingsun.arch.data.model64sun.boot.library.path/Library/Java/JavaVirtualMachines/openlogic-openjdk-21.jdk/Contents/Home/libsun.cpu.endianlittlesun.io.unicode.encodingUnicodeBigsun.java.commandorg.codehaus.plexus.classworlds.launcher.Launcher clean compilesun.java.launcherSUN_STANDARDsun.jnu.encodingsun.management.compilerHotSpot 64-Bit Tiered Compilersuser.countryuser.diruser.homeuser.languageuser.nameuser.timezone/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac-10.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac-11.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac-12.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac-13.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac-14.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac-15.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac-16.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac-17.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac-18.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac-19.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac-2.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac-3.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac-4.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac-5.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac-6.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac-7.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac-8.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac-9.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/Logger/log/ext/javac.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/build-support/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/build-support
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to You under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
http://maven.apache.org/POM/4.0.0xsihttp://www.w3.org/2001/XMLSchema-instanceprojectschemaLocationhttp://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsdmodelVersion4.0.0groupIdorg.apache.flumeartifactIdbuild-supportversion1.11.1-SNAPSHOTnameBuild SupportdescriptionBuild tools and Configurationpropertiesmaven.site.skiptruedeploy.plugin.version2.8.2mvn-gpg-plugin.version1.6buildpluginspluginorg.apache.maven.pluginsmaven-deploy-plugin${deploy.plugin.version}configurationskipmaven-gpg-plugin${mvn-gpg-plugin.version}executionsexecutioniddefault-cliphasepackagegoalsgoalsign/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/build-support/src/main/resources/config/checkstyle/checkstyle-suppressions.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/build-support/src/main/resources/config/checkstyle/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/build-support/src/main/resources/config/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/build-support/src/main/resources/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/build-support/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/build-support/src
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
suppressions-//Puppy Crawl//DTD Suppressions 1.0//ENhttp://www.puppycrawl.com/dtds/suppressions_1_0.dtd NOTE: All path separators must be specified as [/\\] in order to maintain
     build compatibility with both UNIX and Windows.  Suppress all style checks for generated code suppresschecks.*filesgenerated-sources|com[/\\]cloudera[/\\]flume[/\\]handlers[/\\]thrift|org[/\\]apache[/\\]flume[/\\]thrift[/\\]|org[/\\]apache[/\\]flume[/\\]source[/\\]scribe|ProtosFactory.java|avro The "legacy" sources have a weird camelCaps package name PackageNameorg[/\\]apache[/\\]flume[/\\]source[/\\]avroLegacy|org[/\\]apache[/\\]flume[/\\]source[/\\]thriftLegacy Allow unicode escapes in tests AvoidEscapedUnicodeCharactersTest.*\.java TODO: Rearrange methods in below classes to keep overloaded methods adjacent OverloadMethodsDeclarationOrderchannel[/\\]file|RpcClientFactory\.java|BucketPath\.java|SinkGroup\.java|DefaultSinkProcessor\.java|RegexExtractorInterceptorMillisSerializer\.java|SimpleAsyncHbaseEventSerializer\.java|hdfs[/\\]BucketWriter\.java|AbstractBasicChannelSemanticsTest\.java TODO: Fix inner class names to follow standard convention TypeNameSyslogUDPSource\.java|SyslogTcpSource\.java|TaildirSource\.java TODO: Method names must follow standard Java naming conventions MethodNameCheckTestBucketWriter\.java|TestSyslogUtils\.java TODO: Add default cases to switch statements MissingSwitchDefaultSyslogUtils\.java|ReliableTaildirEventReader\.java|AbstractBasicChannelSemanticsTest\.java TODO: Avoid empty catch blocks EmptyCatchBlockchannel[/\\]file[/\\]LogFile\.java|TestDatasetSink\.java|CountingSourceRunner\.java|CountingSinkRunner\.java|TestKafkaChannel\.java|TestTaildirSource\.java|TestChannelProcessor\.java|TestHiveSink\.java|AbstractBasicChannelSemanticsTest\.java|TestJMSSource\.java|TestEmbeddedAgent\.java|TestAsyncHBaseSink\.java TODO: Avoid empty if blocks EmptyBlockCheckElasticSearchClientFactory\.java TODO: Fix line length issues LineLengthCheckchannel[/\\]MemoryChannel\.java|ReliableSpoolingFileEventReader\.java|TestAvroSink\.java TODO: Move helper classes to their own files OneTopLevelClassKafkaSource\.java|KafkaChannel\.java|KafkaSink\.java|TestElasticSearchSink\.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/build-support/src/main/resources/config/checkstyle/checkstyle.xmlmodule-//Checkstyle//DTD Checkstyle Configuration 1.3//ENhttps://checkstyle.org/dtds/configuration_1_3.dtd
    Checkstyle configuration that checks the Google coding conventions from Google Java Style
    that can be found at https://google.github.io/styleguide/javaguide.html.

    Checkstyle is very configurable. Be sure to read the documentation at
    http://checkstyle.sf.net (or in your downloaded distribution).

    To completely disable a check, just comment it out or delete it from the file.

    Authors: Max Vetrenko, Ruslan Diachenko, Roman Ivanov.
 CheckerpropertycharsetvalueseverityerrorfileExtensionsjava, properties, xml Checks for whitespace                                See http://checkstyle.sf.net/config_whitespace.html FileTabCharactereachLine Suppress generated sources SuppressionFilterfile${checkstyle.suppressions.file}optionalfalseLineLengthmax120ignorePattern^package.*|^import.*|a href|href|http://|https://|ftp://TreeWalkerOuterTypeFilenameIllegalTokenTexttokensSTRING_LITERAL, CHAR_LITERALformat\\u00(08|09|0(a|A)|0(c|C)|0(d|D)|22|27|5(C|c))|\\(0(10|11|12|14|15|42|47)|134)messageAvoid using corresponding octal or Unicode escape.allowEscapesForControlCharactersallowByTailCommentallowNonPrintableEscapesAvoidStarImportallowStaticMemberImportsNoLineWrapEmptyBlockoptionTEXTLITERAL_TRY, LITERAL_FINALLY, LITERAL_IF, LITERAL_ELSE, LITERAL_SWITCHNeedBracesallowSingleLineStatementLeftCurlyRightCurlyaloneCLASS_DEF, METHOD_DEF, CTOR_DEF, LITERAL_FOR, STATIC_INIT, INSTANCE_INITWhitespaceAroundallowEmptyConstructorsallowEmptyMethodsallowEmptyTypesallowEmptyLoopskeyws.notFollowedWhitespaceAround: ''{0}'' is not followed by whitespace. Empty blocks may only be represented as '{}' when not part of a multi-block statement (4.1.3)ws.notPrecededWhitespaceAround: ''{0}'' is not preceded with whitespace.OneStatementPerLineArrayTypeStyleFallThroughUpperEllModifierOrderEmptyLineSeparatorallowNoEmptyLineBetweenFieldsallowMultipleEmptyLinesallowMultipleEmptyLinesInsideClassMembersIMPORT, CLASS_DEF, INTERFACE_DEF, ENUM_DEF, STATIC_INIT, INSTANCE_INIT, CTOR_DEF, VARIABLE_DEFSeparatorWrapDOTnlCOMMAEOL^[a-z]+(\.[a-z][a-z0-9]*)*$name.invalidPatternPackage name ''{0}'' must match pattern ''{1}''.Type name ''{0}'' must match pattern ''{1}''.ClassTypeParameterName(^[A-Z][0-9]?)$|([A-Z][a-zA-Z0-9]*[T]$)Class type name ''{0}'' must match pattern ''{1}''.MethodTypeParameterNameMethod type name ''{0}'' must match pattern ''{1}''.InterfaceTypeParameterNameInterface type name ''{0}'' must match pattern ''{1}''.NoFinalizerGenericWhitespacews.followedGenericWhitespace ''{0}'' is followed by whitespace.ws.precededGenericWhitespace ''{0}'' is preceded with whitespace.ws.illegalFollowGenericWhitespace ''{0}'' should followed by whitespace.GenericWhitespace ''{0}'' is not preceded with whitespace.IndentationbasicOffset2braceAdjustment0caseIndentthrowsIndent4lineWrappingIndentationarrayInitIndentMethodParamPadAnnotationLocationCLASS_DEF, INTERFACE_DEF, ENUM_DEF, METHOD_DEF, CTOR_DEFVARIABLE_DEFallowSamelineMultipleAnnotationsAtclauseOrdertagOrder@param, @return, @throws, @deprecatedtargetCLASS_DEF, INTERFACE_DEF, ENUM_DEF, METHOD_DEF, CTOR_DEF, VARIABLE_DEFMissingJavadocMethodexcludeScopepublicJavadocMethodaccessModifiersallowMissingParamTagsallowMissingReturnTagallowedAnnotationsOverride, TestMethodName^[a-z][a-z0-9][a-zA-Z0-9_]*$Method name ''{0}'' must match pattern ''{1}''.SingleLineJavadocignoreInlineTagsexceptionVariableNameexpectedCommentsIndentationBLOCK_COMMENT_BEGINUnusedImportsRedundantImport/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/build-support/target/classes/config/checkstyle/checkstyle-suppressions.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/build-support/target/classes/config/checkstyle/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/build-support/target/classes/config/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/build-support/target/classes/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/build-support/target/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/build-support/target/classes/config/checkstyle/checkstyle.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/conf/log4j2.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/conf
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
 this work for additional information regarding copyright ownership.
 The ASF licenses this file to You under the Apache License, Version 2.0
 (the "License"); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.

ConfigurationstatusERRORPropertiesPropertyLOG_DIR.AppendersConsoleSYSTEM_ERRPatternLayoutpattern%d (%t) [%p - %l] %m%nRollingFileLogFilefileName${LOG_DIR}/flume.logfilePattern${LOG_DIR}/archive/flume.log.%d{yyyyMMdd}-%i%d{dd MMM yyyy HH:mm:ss,SSS} %-5p [%t] (%C.%M:%L) %equals{%x}{[]}{} - %m%nPolicies Roll every night at midnight or when the file reaches 100MB SizeBasedTriggeringPolicysize100 MBCronTriggeringPolicyschedule0 0 0 * * ?DefaultRolloverStrategymin20DeletebasePath${LOG_DIR}/archive Nested conditions: the inner condition is only evaluated on files for which the outer conditions are true. IfFileNameglobflume.log.* Only allow 1 GB of files to accumulate IfAccumulatedFileSizeexceeds1 GBLoggersLoggerorg.apache.flume.lifecyclelevelinfoorg.jbossWARNorg.apache.avro.ipc.netty.NettyTransceiverorg.apache.hadoopINFOorg.apache.hadoop.hiveRootAppenderRefref/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-bom/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-bomparentflume-project1.11.0flume-dependenciespackagingpomApache Flume Artifactsflume-hadoop.version2.0.0-SNAPSHOTflume-http.versionflume-irc.versionflume-jdbc.versionflume-jms.versionflume-kafka.versionflume-log4jappender.versionflume-scribe.versionflume-spring-boot.versionflume-taildir.versionflume-tools.versionflume-twitter.versionflume-legacy.versionmorphline.versiondependencyManagementdependenciesdependencyflume-ng-auth${project.version}flume-ng-configfiltersflume-ng-configurationflume-ng-coreflume-ng-embedded-agentflume-ng-config-filter-apiflume-ng-environment-variable-config-filterflume-ng-external-process-config-filterflume-hadoop-credential-store-config-filter${flume-hadoop.version}flume-ng-sdkflume-ng-nodeflume-spring-boot${flume-spring-boot.version}flume-twitter-source${flume-twitter.version}flume-jdbc-channel${flume-jdbc.version}org.apache.flume.flume-ng-channelsflume-file-channelflume-kafka-channel${flume-kafka.version}flume-recoverable-memory-channelflume-spillable-memory-channelorg.apache.flume.flume-ng-clientsflume-ng-log4jappender${flume-log4jappender.version}classifierjar-with-dependenciesflume-legacy-thrift-source${flume-legacy.version}flume-legacy-avro-sourceorg.apache.flume.flume-ng-sinksflume-ng-morphline-solr-sink${morphline.version}flume-hdfs-sinkflume-hive-sinkflume-http-sink${flume-http.version}flume-irc-sink${flume-irc.version}flume-hbase2-sinkflume-kafka-sinkflume-kudu-sinkorg.apache.flume.flume-ng-sourcesflume-jms-source${flume-jms.version}flume-dataset-sinkflume-kafka-sourceflume-scribe-source${flume-scribe.version}flume-taildir-source${flume-taildir.version}flume-shared-kafkaflume-tools${flume-tools.version}/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-authflume-parentrelativePath../flume-parent/pom.xmlFlume AuthFlume Authentication TODO fix spotbugs/pmd violations spotbugs.maxAllowedViolationspmd.maxAllowedViolations7module.nameorg.apache.flume.authjunitscopetestorg.slf4jslf4j-apiorg.apache.logging.log4jlog4j-slf4j-impllog4j-1.2-apihadoop-commonhadoop-minikdc add this to satisfy the dependency requirement of apacheds-jdbm1 in minikdcorg.apache.directory.jdbmapacheds-jdbm1com.google.guavaguava/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/src/main/java/org/apache/flume/api/SecureRpcClientFactory.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/src/main/java/org/apache/flume/api/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/src/main/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/src/main/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/src/main/java/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/src/main/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/srcorg.apache.flume.apivoidpropsclientLicensed to the Apache Software Foundation (ASF) under oneor more contributor license agreements.  See the NOTICE filedistributed with this work for additional informationregarding copyright ownership.  The ASF licenses this fileto you under the Apache License, Version 2.0 (the"License"); you may not use this file except in compliancewith the License.  You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing,software distributed under the License is distributed on an"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANYKIND, either express or implied.  See the License for thespecific language governing permissions and limitationsunder the License.Factory class to construct Flume {@link RPCClient} implementations.Return a secure {@linkplain org.apache.flume.api.RpcClient} that uses Thrift for communicatingwith the next hop.@param@return- An {@linkplain org.apache.flume.api.RpcClient} which uses thrift configured with thegiven parameters./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/src/main/java/org/apache/flume/api/SecureThriftRpcClient.javaprivilegedExecutor/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/target/classes/org/apache/flume/api/SecureThriftRpcClient.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/target/classes/org/apache/flume/api/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/target/classes/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/target/classes/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/target/classes/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/target/classes/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/targetprivateserverPrincipalSERVER_PRINCIPALstaticfinalCLIENT_KEYTABCLIENT_PRINCIPAL<clinit><clinit>()"client-principal"client-principal"client-keytab"client-keytab"server-principal"server-principalclientPrincipalkeytabboolean<nulltype>"Flume in secure mode, but Flume config doesn't "
              + "specify a server principal to use for Kerberos auth."Flume in secure mode, but Flume config doesn't specify a server principal to use for Kerberos auth."Authentication failed in Kerberos mode for " +
                               "principal "Authentication failed in Kerberos mode for principal " keytab " keytab tsocketjava.utilMap<String,String>/modules/java.base/java/util/Map.class/modules/java.base/java/util/modules/java.base/java/modules/java.base/modulesabstractsaslPropertiesHashMap<String,String>/modules/java.base/java/util/HashMap.classAbstractMap<String,String>/modules/java.base/java/util/AbstractMap.classHashMap<String,String>()Map<>KVMap<K,V>copyOfcopyOf(java.util.Map)? extends K? extends VMap<? extends K,? extends V>Entry<>/modules/java.base/java/util/Map$Entry.classEntry<K,V>entryentry(java.lang.Object,java.lang.Object)Entry[]lengthcloneclone()Entry<>[]ofEntriesofEntries(java.util.Map.Entry[])Entry<? extends K,? extends V>Entry<? extends K,? extends V>[]ofof(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)of(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)of(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)of(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)of(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)of(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)of(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)of(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)of(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)of(java.lang.Object,java.lang.Object)of()? super Vjava.util.functionBiFunction<? super V,? super V,? extends V>/modules/java.base/java/util/function/BiFunction.class/modules/java.base/java/util/functionmergemerge(java.lang.Object,java.lang.Object,java.util.function.BiFunction)default? super String? extends StringBiFunction<? super String,? super String,? extends String>merge(java.lang.String,java.lang.String,java.util.function.BiFunction)? super KBiFunction<? super K,? super V,? extends V>computecompute(java.lang.Object,java.util.function.BiFunction)compute(java.lang.String,java.util.function.BiFunction)computeIfPresentcomputeIfPresent(java.lang.Object,java.util.function.BiFunction)computeIfPresent(java.lang.String,java.util.function.BiFunction)Function<? super K,? extends V>/modules/java.base/java/util/function/Function.classcomputeIfAbsentcomputeIfAbsent(java.lang.Object,java.util.function.Function)Function<? super String,? extends String>computeIfAbsent(java.lang.String,java.util.function.Function)replacereplace(java.lang.Object,java.lang.Object)replace(java.lang.String,java.lang.String)replace(java.lang.Object,java.lang.Object,java.lang.Object)replace(java.lang.String,java.lang.String,java.lang.String)removeremove(java.lang.Object,java.lang.Object)putIfAbsentputIfAbsent(java.lang.Object,java.lang.Object)putIfAbsent(java.lang.String,java.lang.String)replaceAllreplaceAll(java.util.function.BiFunction)BiConsumer<? super K,? super V>/modules/java.base/java/util/function/BiConsumer.classforEachforEach(java.util.function.BiConsumer)BiConsumer<? super String,? super String>getOrDefaultgetOrDefault(java.lang.Object,java.lang.Object)getOrDefault(java.lang.Object,java.lang.String)inthashCodehashCode()equalsequals(java.lang.Object)Set<>/modules/java.base/java/util/Set.classCollection<>/modules/java.base/java/util/Collection.classjava.langIterable<>/modules/java.base/java/lang/Iterable.class/modules/java.base/java/langSet<Entry<K,V>>Collection<Entry<K,V>>Iterable<Entry<K,V>>entrySetentrySet()Entry<String,String>Set<Entry<String,String>>Collection<Entry<String,String>>Iterable<Entry<String,String>>Collection<V>Iterable<V>valuesvalues()Collection<String>Iterable<String>Set<K>Collection<K>Iterable<K>keySetkeySet()Set<String>clearclear()putAllputAll(java.util.Map)Map<? extends String,? extends String>remove(java.lang.Object)putput(java.lang.Object,java.lang.Object)put(java.lang.String,java.lang.String)getget(java.lang.Object)containsValuecontainsValue(java.lang.Object)containsKeycontainsKey(java.lang.Object)isEmptyisEmpty()size()protectedtoStringtoString()AbstractMapAbstractMap()AbstractMap<String,String>()transientHashMap<>AbstractMap<>HashMap<K,V>AbstractMap<K,V>newHashMapnewHashMap(int)calculateHashMapCapacitycalculateHashMapCapacity(int)internalWriteEntriesinternalWriteEntries(java.io.ObjectOutputStream)Node<K,V>/modules/java.base/java/util/HashMap$Node.classafterNodeRemovalafterNodeRemoval(java.util.HashMap.Node)Node<String,String>afterNodeInsertionafterNodeInsertion(boolean)afterNodeAccessafterNodeAccess(java.util.HashMap.Node)reinitializereinitialize()TreeNode<>/modules/java.base/java/util/HashMap$TreeNode.class/modules/java.base/java/util/LinkedHashMap$Entry.classNode<>TreeNode<K,V>replacementTreeNodereplacementTreeNode(java.util.HashMap.Node,java.util.HashMap.Node)TreeNode<String,String>newTreeNodenewTreeNode(int,java.lang.Object,java.lang.Object,java.util.HashMap.Node)newTreeNode(int,java.lang.String,java.lang.String,java.util.HashMap.Node)replacementNodereplacementNode(java.util.HashMap.Node,java.util.HashMap.Node)newNodenewNode(int,java.lang.Object,java.lang.Object,java.util.HashMap.Node)newNode(int,java.lang.String,java.lang.String,java.util.HashMap.Node)capacitycapacity()floatloadFactorloadFactor()Object[]TT[]valuesToArrayvaluesToArray(java.lang.Object[])keysToArraykeysToArray(java.lang.Object[])prepareArrayprepareArray(java.lang.Object[])removeNoderemoveNode(int,java.lang.Object,java.lang.Object,boolean,boolean)Node[]Node<K,V>[]treeifyBintreeifyBin(java.util.HashMap.Node[],int)Node<String,String>[]Node<>[]resizeresize()putValputVal(int,java.lang.Object,java.lang.Object,boolean,boolean)putVal(int,java.lang.String,java.lang.String,boolean,boolean)getNodegetNode(java.lang.Object)putMapEntriesputMapEntries(java.util.Map,boolean)HashMapHashMap(java.util.Map)HashMap<String,String>(java.util.Map)HashMap()HashMap(int)HashMap<String,String>(int)HashMap(int,float)HashMap<String,String>(int,float)tableSizeFortableSizeFor(int)?Class<?>/modules/java.base/java/lang/Class.classjava.lang.invokeOfField<Class<?>>/modules/java.base/java/lang/invoke/TypeDescriptor$OfField.class/modules/java.base/java/lang/invokecompareComparablescompareComparables(java.lang.Class,java.lang.Object,java.lang.Object)Class<>OfField<>comparableClassForcomparableClassFor(java.lang.Object)hashhash(java.lang.Object)thresholdmodCounttableMIN_TREEIFY_CAPACITYUNTREEIFY_THRESHOLDTREEIFY_THRESHOLDDEFAULT_LOAD_FACTORMAXIMUM_CAPACITYDEFAULT_INITIAL_CAPACITYString[]names"auth"authe"Error while trying to resolve Principal name - "Error while trying to resolve Principal name - "GSSAPI"GSSAPIcallSuperClassOpencallSuperClassOpen()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/target/classes/org/apache/flume/api/SecureThriftRpcClient$UgiSaslClientTransport.classmechanismauthorizationIdprotocolserverNamecbhtransportjava.securityPrivilegedExceptionAction<>/modules/java.base/java/security/PrivilegedExceptionAction.class/modules/java.base/java/security/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/target/classes/org/apache/flume/api/SecureThriftRpcClient$UgiSaslClientTransport$1.classPrivilegedExceptionAction<Void>runrun()()"Interrupted while opening underlying transport"Interrupted while opening underlying transport"Failed to open SASL transport"Failed to open SASL transportThis transport wraps the Sasl transports to set up the right UGI context for open().Open the SASL transport with using the current UserGroupInformation.This is needed to get the current login context stored this is a workaround to using UgiSaslClientTransport.super.open() which results in IllegalAccessError/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/src/main/java/org/apache/flume/auth/FlumeAuthenticationUtil.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/src/main/java/org/apache/flume/authclearCredentialsclearCredentials()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/target/classes/org/apache/flume/auth/FlumeAuthenticationUtil.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/target/classes/org/apache/flume/authkerbAuthenticatorFlumeAuthenticationUtilFlumeAuthenticationUtil()principal"Principal can not be null when keytab is provided"Principal can not be null when keytab is provided"Keytab can not be null when Principal is provided"Keytab can not be null when Principal is providedresolvedPrinc""Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an "AS IS" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.FlumeAuthentication utility class that provides methods to get anAuthenticator. If proper credentials are provided KerberosAuthenticator isreturned which can be used to execute as the authenticated principal ,or else a SimpleAuthenticator which executes without any authenticationIf principal and keytab are null, this method returns a SimpleAuthenticatorwhich executes without authentication. If valid credentials areprovided KerberosAuthenitcator is returned which can be used to execute asthe authenticated principal. Invalid credentials result inIllegalArgumentException and Failure to authenticate results in SecurityExceptionFlumeAuthenticator@throwsorg.apache.flume.auth.SecurityExceptionReturns the standard SaslGssCallbackHandler from the hadoop common moduleCallbackHandlerResolves the principal using Hadoop common's SecurityUtil and splitsthe kerberos principal into three parts user name, host and kerberos realmString[] of username, hostname and kerberos realmIOException/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/src/main/java/org/apache/flume/auth/FlumeAuthenticator.javaproxyUserNameFlumeAuthenticator extends on a PrivilegedExecutor providing capabilities toproxy as a different userReturns the current instance if proxyUsername is null orreturns the proxied Executor if proxyUserName is validPrivilegedExecutorReturns true, if the underlying Authenticator was obtained bysuccessful kerberos authenticationFor Authenticators backed by credentials, this method refreshes thecredentials periodically/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/src/main/java/org/apache/flume/auth/KerberosAuthenticator.javaHADOOP_SECURITY_AUTHENTICATIONgetUserNamegetUserName()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/target/classes/org/apache/flume/auth/KerberosAuthenticator.classprintUGIprintUGI(org.apache.hadoop.security.UserGroupInformation)Map<String,PrivilegedExecutor>proxyCachevolatileprevUserugiLOGKerberosAuthenticatorKerberosAuthenticator()<obinit><obinit>()HashMap<String,PrivilegedExecutor>AbstractMap<String,PrivilegedExecutor>HashMap<String,PrivilegedExecutor>()? super PrivilegedExecutor? extends PrivilegedExecutorBiFunction<? super PrivilegedExecutor,? super PrivilegedExecutor,? extends PrivilegedExecutor>merge(java.lang.String,org.apache.flume.auth.PrivilegedExecutor,java.util.function.BiFunction)BiFunction<? super String,? super PrivilegedExecutor,? extends PrivilegedExecutor>Function<? super String,? extends PrivilegedExecutor>replace(java.lang.String,org.apache.flume.auth.PrivilegedExecutor)replace(java.lang.String,org.apache.flume.auth.PrivilegedExecutor,org.apache.flume.auth.PrivilegedExecutor)putIfAbsent(java.lang.String,org.apache.flume.auth.PrivilegedExecutor)BiConsumer<? super String,? super PrivilegedExecutor>getOrDefault(java.lang.Object,org.apache.flume.auth.PrivilegedExecutor)Entry<String,PrivilegedExecutor>Set<Entry<String,PrivilegedExecutor>>Collection<Entry<String,PrivilegedExecutor>>Iterable<Entry<String,PrivilegedExecutor>>Collection<PrivilegedExecutor>Iterable<PrivilegedExecutor>Map<? extends String,? extends PrivilegedExecutor>put(java.lang.String,org.apache.flume.auth.PrivilegedExecutor)AbstractMap<String,PrivilegedExecutor>()Node<String,PrivilegedExecutor>TreeNode<String,PrivilegedExecutor>newTreeNode(int,java.lang.String,org.apache.flume.auth.PrivilegedExecutor,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.flume.auth.PrivilegedExecutor,java.util.HashMap.Node)Node<String,PrivilegedExecutor>[]putVal(int,java.lang.String,org.apache.flume.auth.PrivilegedExecutor,boolean,boolean)HashMap<String,PrivilegedExecutor>(java.util.Map)HashMap<String,PrivilegedExecutor>(int)HashMap<String,PrivilegedExecutor>(int,float)Class<KerberosAuthenticator>PrivilegedAction<>/modules/java.base/java/security/PrivilegedAction.classPrivilegedAction<T>actionPrivilegedExceptionAction<T>proxyUgikeytabFileresolvedPrincipalnewUsercurUser"Invalid Kerberos principal: "Invalid Kerberos principal: "Invalid Kerberos keytab: "Invalid Kerberos keytab: "Keytab is not a readable file: "Keytab is not a readable file: "Host lookup error resolving kerberos principal ("Host lookup error resolving kerberos principal ("). Exception follows."). Exception follows."Resolved Principal must not be null"Resolved Principal must not be null"Cannot use multiple kerberos principals in the same agent. " +
        " Must restart agent to use new principal or keytab. " +
        "Previous = %s, New = %s"Cannot use multiple kerberos principals in the same agent.  Must restart agent to use new principal or keytab. Previous = %s, New = %sconf"kerberos"kerberos"User unexpectedly had no active login. Continuing with " +
              "authentication"User unexpectedly had no active login. Continuing with authentication"Using existing principal login: {}"Using existing principal login: {}"Attempting kerberos Re-login as principal ({}) "Attempting kerberos Re-login as principal ({}) "Attempting kerberos login as principal ({}) from keytab " +
                "file ({})"Attempting kerberos login as principal ({}) from keytab file ({})"Authentication error while attempting to "
        + "login as kerberos principal ("Authentication error while attempting to login as kerberos principal (") using "
        + "keytab (") using keytab (authMethod"\n{} \nUser: {} \nAuth method: {} \nKeytab: {} \n"
{} 
User: {} 
Auth method: {} 
Keytab: {} 
Enum<AuthenticationMethod>/modules/java.base/java/lang/Enum.classComparable<AuthenticationMethod>/modules/java.base/java/lang/Comparable.classcompareTocompareTo(java.lang.Object)compareTo(org.apache.hadoop.security.UserGroupInformation.AuthenticationMethod)finalizefinalize()Enum<>Comparable<>Enum<T>Comparable<T>valueOfvalueOf(java.lang.Class,java.lang.String)Class<T>Optional<>/modules/java.base/java/util/Optional.classEnumDesc<E>/modules/java.base/java/lang/Enum$EnumDesc.classjava.lang.constantDynamicConstantDesc<E>/modules/java.base/java/lang/constant/DynamicConstantDesc.class/modules/java.base/java/lang/constantOptional<EnumDesc<E>>describeConstabledescribeConstable()EnumDesc<AuthenticationMethod>DynamicConstantDesc<AuthenticationMethod>Optional<EnumDesc<AuthenticationMethod>>Class<E>getDeclaringClassgetDeclaringClass()Class<AuthenticationMethod>compareTo(java.lang.Enum)EnumEnum(java.lang.String,int)Enum<AuthenticationMethod>(java.lang.String,int)ordinalordinal()name()"Proxy as: "Proxy as: "Logged as: "Logged as: CHECK_TGT_INTERVALschedulerjava.util.concurrentScheduledFuture<?>/modules/java.base/java/util/concurrent/ScheduledFuture.class/modules/java.base/java/util/concurrentFuture<?>/modules/java.base/java/util/concurrent/Future.classlongScheduledFuture<>Future<>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/target/classes/org/apache/flume/auth/KerberosAuthenticator$1.class"Error occured during checkTGTAndReloginFromKeytab() for user "Error occured during checkTGTAndReloginFromKeytab() for user A kerberos authenticator, which authenticates using the supplied principaland keytab and executes with  authenticated privilegesWhen valid principal and keytab are provided and if authentication hasnot yet been done for this object, this method authenticates thecredentials and populates the ugi. In case of null or invalid credentialsIllegalArgumentException is thrown. In case of failure to authenticate,SecurityException is thrown. If authentication has already happened onthis KerberosAuthenticator object, then this method checks to see if the currentcredentials passed are same as the validated credentials. If not, it throwsan exception as this authenticator can represent only one Principal. sanity checking resolve the requested principal resolves _HOST pattern using standard Hadoop search/replace via DNS lookup when 2nd argument is empty be cruel and unusual when user tries to login as multiple principals this isn't really valid with a reconfigure but this should be rare enough to warrant a restart of the agent JVM TODO: find a way to interrogate the entire current config state, since we don't have to be unnecessarily protective if they switch all HDFS sinks to use a different principal all at once. enable the kerberos mode of UGI, before doing anything else We are interested in currently logged in user with kerberos credsif ugi is not null,if ugi matches currently logged in kerberos user, we are goodelse we are logged out, so relogin our ugielse if ugi is null, login and populate state dump login informationstartCredentialRefresher should be used only for long runningmethods like Thrift source. For all privileged methods that use a UGI, thecredentials are checked automatically and refreshed before theprivileged method is executed in the UGIExecutor seconds/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/src/main/java/org/apache/flume/auth/KerberosUser.javakeyTab/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/target/classes/org/apache/flume/auth/KerberosUser.classobjother? extends KerberosUserClass<? extends KerberosUser>? extends ObjectClass<? extends Object>41"{ principal: "{ principal: ", keytab: ", keytab: " }" }Licensed to the Apache Software Foundation (ASF) under one or morecontributor license agreements. See the NOTICE file distributed with thiswork for additional information regarding copyright ownership. The ASFlicenses this file to you under the Apache License, Version 2.0 (the"License"); you may not use this file except in compliance with the License.You may obtain a copy of the License atdistributed under the License is distributed on an "AS IS" BASIS, WITHOUTWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See theLicense for the specific language governing permissions and limitations underthe License.Simple Pair class used to define a unique (principal, keyTab) combination./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/src/main/java/org/apache/flume/auth/PrivilegedExecutor.javaPrivilegedExecutor provides the ability to execute a PrivilegedActionor a PrivilegedExceptionAction. Implementors of this class, can chose to executein normal mode or secure authenticated modeThis method is used to execute a privileged action, the implementor canchose to execute the action using the appropriate privilegesA PrivilegedExceptionAction to perform as the desired user<T>The return type of the actionT the T value returned by action.run()ExceptionA PrivilegedAction to perform as the desired user/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/src/main/java/org/apache/flume/auth/SecurityException.javacauseSecurityException thrown in the Flume security module/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/src/main/java/org/apache/flume/auth/SimpleAuthenticator.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/target/classes/org/apache/flume/auth/SimpleAuthenticator.classSimpleAuthenticatorSimpleAuthenticator()SimpleAuthenticatorHolderSimpleAuthenticatorHolder()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/target/classes/org/apache/flume/auth/SimpleAuthenticator$SimpleAuthenticatorHolder.class"Unable to create proxy User"Unable to create proxy UserA no-op authenticator, which does not authenticate and executeswithout any authenticated privileges no-op/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/src/main/java/org/apache/flume/auth/UGIExecutor.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-auth/target/classes/org/apache/flume/auth/UGIExecutor.classreloginUGIreloginUGI(org.apache.hadoop.security.UserGroupInformation)ensureValidAuthensureValidAuth()UGIExecutorUGIExecutor(org.apache.hadoop.security.UserGroupInformation)lastReloginAttemptMIN_TIME_BEFORE_RELOGIN5601000L1000now"Error trying to relogin from keytab for user "Error trying to relogin from keytab for user lastReloginAttempt is introduced to avoid making the synchronized callugi.checkTGTAndReloginFromKeytab() often, Hence this method isintentionally not synchronized, so that multiple threads can execute withoutthe need to lock, which may result in an edge case where multiple threadssimultaneously reading the lastReloginAttempt, and finding it > 5 minutes, canresult in all of them attempting the checkTGT method, which is fine/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels
 Licensed to the Apache Software Foundation (ASF) under one
 or more contributor license agreements.  See the NOTICE file
 distributed with this work for additional information
 regarding copyright ownership.  The ASF licenses this file
 to you under the Apache License, Version 2.0 (the
 "License"); you may not use this file except in compliance
 with the License.  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing,
 software distributed under the License is distributed on an
 "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 KIND, either express or implied.  See the License for the
 specific language governing permissions and limitations
 under the License.
flume-ng-channelsFlume NG file-based channel TODO fix spotbugs violations 86544org.apache.flume.channel.filecommons-collectionscommons-langorg.mockitomockito-allorg.easytestingfest-reflectlog4j-jclcom.google.protobufprotobuf-javacompileorg.mapdbmapdborg.xerial.snappysnappy-javaorg.xolstice.maven.pluginsprotobuf-maven-plugincompile-protocgenerate-sourcesprotocArtifactcom.google.protobuf:protoc:${external.protobuf.version}:exe:${os.detected.classifier}protoSourceRoot${basedir}/src/main/proto/clearOutputDirectorycheckStaleness/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/BadCheckpointException.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/srcserialVersionUID/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/BadCheckpointException.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target5038652693746472779L5038652693746472779msgtException thrown when the checkpoint directory contains invalid data,probably due to the channel stopping while the checkpoint was written./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/CheckpointRebuilder.javawriteCheckpointwriteCheckpoint()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/CheckpointRebuilder.classfsyncPerTransactioncom.google.common.collectSetMultimap<Long,ComparableFlumeEventPointer>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/Users/burakyetistiren/.m2/repository/com/google/guava/guava/Users/burakyetistiren/.m2/repository/com/google/guava/Users/burakyetistiren/.m2/repository/com/google/Users/burakyetistiren/.m2/repository/com/Users/burakyetistiren/.m2/repository/Users/burakyetistiren/.m2/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/SetMultimap.classMultimap<Long,ComparableFlumeEventPointer>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/Multimap.class"Use ImmutableMultimap, HashMultimap, or another implementation"Use ImmutableMultimap, HashMultimap, or another implementationuncommittedTakesuncommittedPutsSet<ComparableFlumeEventPointer>Collection<ComparableFlumeEventPointer>Iterable<ComparableFlumeEventPointer>pendingTakescommittedPutsqueueList<File>/modules/java.base/java/util/List.classSequencedCollection<File>/modules/java.base/java/util/SequencedCollection.classCollection<File>Iterable<File>logFilesHashSet<ComparableFlumeEventPointer>/modules/java.base/java/util/HashSet.classAbstractSet<ComparableFlumeEventPointer>/modules/java.base/java/util/AbstractSet.classAbstractCollection<ComparableFlumeEventPointer>/modules/java.base/java/util/AbstractCollection.classHashSet<>AbstractSet<>AbstractCollection<>HashMultimap<Long,ComparableFlumeEventPointer>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/HashMultimap.classHashMultimapGwtSerializationDependencies<Long,ComparableFlumeEventPointer>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/HashMultimapGwtSerializationDependencies.classAbstractSetMultimap<Long,ComparableFlumeEventPointer>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/AbstractSetMultimap.classAbstractMapBasedMultimap<Long,ComparableFlumeEventPointer>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/AbstractMapBasedMultimap.classAbstractMultimap<Long,ComparableFlumeEventPointer>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/AbstractMultimap.classHashMultimap<>HashMultimapGwtSerializationDependencies<>AbstractSetMultimap<>AbstractMapBasedMultimap<>AbstractMultimap<>Multimap<>SetMultimap<>Class<CheckpointRebuilder>List<SequentialReader>SequencedCollection<SequentialReader>Collection<SequentialReader>Iterable<SequentialReader>logReadersArrayList<SequentialReader>/modules/java.base/java/util/ArrayList.classAbstractList<SequentialReader>/modules/java.base/java/util/AbstractList.classAbstractCollection<SequentialReader>ArrayList<>AbstractList<>List<>SequencedCollection<>transactionIDSeedwriteOrderIDSeedsortedPutsTreeSet<ComparableFlumeEventPointer>/modules/java.base/java/util/TreeSet.classNavigableSet<ComparableFlumeEventPointer>/modules/java.base/java/util/NavigableSet.classSortedSet<ComparableFlumeEventPointer>/modules/java.base/java/util/SortedSet.classSequencedSet<ComparableFlumeEventPointer>/modules/java.base/java/util/SequencedSet.classSequencedCollection<ComparableFlumeEventPointer>TreeSet<>NavigableSet<>SortedSet<>SequencedSet<>count"Attempting to fast replay the log files."Attempting to fast replay the log files.logFileaddadd(org.apache.flume.channel.file.LogFile.SequentialReader)Spliterator<>/modules/java.base/java/util/Spliterator.classSpliterator<T>spliteratorspliterator()Spliterator<SequentialReader>? super TConsumer<? super T>/modules/java.base/java/util/function/Consumer.classforEach(java.util.function.Consumer)? super SequentialReaderConsumer<? super SequentialReader>Iterator<>/modules/java.base/java/util/Iterator.classIterator<T>iteratoriterator()Iterator<SequentialReader>java.util.streamStream<>/modules/java.base/java/util/stream/Stream.class/modules/java.base/java/util/streamBaseStream<>/modules/java.base/java/util/stream/BaseStream.classStream<E>BaseStream<E,Stream<E>>parallelStreamparallelStream()Stream<SequentialReader>BaseStream<SequentialReader,Stream<SequentialReader>>streamstream()Spliterator<E>Collection<?>Iterable<?>retainAllretainAll(java.util.Collection)? super EPredicate<? super E>/modules/java.base/java/util/function/Predicate.classremoveIfremoveIf(java.util.function.Predicate)Predicate<? super SequentialReader>removeAllremoveAll(java.util.Collection)? extends ECollection<? extends E>Iterable<? extends E>addAlladdAll(java.util.Collection)? extends SequentialReaderCollection<? extends SequentialReader>Iterable<? extends SequentialReader>containsAllcontainsAll(java.util.Collection)add(java.lang.Object)IntFunction<>/modules/java.base/java/util/function/IntFunction.classtoArraytoArray(java.util.function.IntFunction)IntFunction<T[]>toArray(java.lang.Object[])toArray()Iterator<E>containscontains(java.lang.Object)removeLastremoveLast()removeFirstremoveFirst()getLastgetLast()getFirstgetFirst()addLastaddLast(java.lang.Object)addLast(org.apache.flume.channel.file.LogFile.SequentialReader)addFirstaddFirst(java.lang.Object)addFirst(org.apache.flume.channel.file.LogFile.SequentialReader)reversedreversed()EList<E>SequencedCollection<E>Collection<E>Iterable<E>copyOf(java.util.Collection)of(java.lang.Object[])E[]of(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)of(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)of(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)of(java.lang.Object,java.lang.Object,java.lang.Object)of(java.lang.Object)subListsubList(int,int)ListIterator<>/modules/java.base/java/util/ListIterator.classListIterator<E>listIteratorlistIterator(int)ListIterator<SequentialReader>listIterator()lastIndexOflastIndexOf(java.lang.Object)indexOfindexOf(java.lang.Object)remove(int)add(int,java.lang.Object)add(int,org.apache.flume.channel.file.LogFile.SequentialReader)setset(int,java.lang.Object)set(int,org.apache.flume.channel.file.LogFile.SequentialReader)get(int)Comparator<? super E>/modules/java.base/java/util/Comparator.classsortsort(java.util.Comparator)Comparator<? super SequentialReader>UnaryOperator<E>/modules/java.base/java/util/function/UnaryOperator.classFunction<E,E>replaceAll(java.util.function.UnaryOperator)UnaryOperator<SequentialReader>Function<SequentialReader,SequentialReader>addAll(int,java.util.Collection)"Ignoring "Ignoring " due to EOF" due to EOFlogfileIDoffsetrecordtranswriteOrderIDshortput(java.lang.Long,org.apache.flume.channel.file.CheckpointRebuilder.ComparableFlumeEventPointer)Map<K,Collection<V>>asMapasMap()Map<Long,Collection<ComparableFlumeEventPointer>>? super Long? super ComparableFlumeEventPointerBiConsumer<? super Long,? super ComparableFlumeEventPointer>entriesentries()Entry<Long,ComparableFlumeEventPointer>Collection<Entry<Long,ComparableFlumeEventPointer>>Iterable<Entry<Long,ComparableFlumeEventPointer>>Multiset<>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/Multiset.classMultiset<K>keyskeys()Multiset<Long>Collection<Long>Iterable<Long>Set<Long>get(java.lang.Long)removeAll(java.lang.Object)Iterable<? extends V>replaceValuesreplaceValues(java.lang.Object,java.lang.Iterable)? extends ComparableFlumeEventPointerIterable<? extends ComparableFlumeEventPointer>replaceValues(java.lang.Long,java.lang.Iterable)Multimap<? extends K,? extends V>putAll(com.google.common.collect.Multimap)? extends LongMultimap<? extends Long,? extends ComparableFlumeEventPointer>putAll(java.lang.Object,java.lang.Iterable)putAll(java.lang.Long,java.lang.Iterable)containsEntrycontainsEntry(java.lang.Object,java.lang.Object)takecommitputsSet<Entry<Long,ComparableFlumeEventPointer>>Set<V>Spliterator<ComparableFlumeEventPointer>Consumer<? super ComparableFlumeEventPointer>Iterator<ComparableFlumeEventPointer>Stream<ComparableFlumeEventPointer>BaseStream<ComparableFlumeEventPointer,Stream<ComparableFlumeEventPointer>>Predicate<? super ComparableFlumeEventPointer>Collection<? extends ComparableFlumeEventPointer>add(org.apache.flume.channel.file.CheckpointRebuilder.ComparableFlumeEventPointer)Set<E>takes"Error while generating checkpoint using fast generation logic"Error while generating checkpoint using fast generation logicreader"Replayed {} events using fast replay logic."Replayed {} events using fast replay logic.checkpointLogOrderIDList<MetaDataWriter>SequencedCollection<MetaDataWriter>Collection<MetaDataWriter>Iterable<MetaDataWriter>metaDataWritersArrayList<MetaDataWriter>AbstractList<MetaDataWriter>AbstractCollection<MetaDataWriter>add(org.apache.flume.channel.file.LogFile.MetaDataWriter)Spliterator<MetaDataWriter>? super MetaDataWriterConsumer<? super MetaDataWriter>Iterator<MetaDataWriter>Stream<MetaDataWriter>BaseStream<MetaDataWriter,Stream<MetaDataWriter>>Predicate<? super MetaDataWriter>? extends MetaDataWriterCollection<? extends MetaDataWriter>Iterable<? extends MetaDataWriter>addLast(org.apache.flume.channel.file.LogFile.MetaDataWriter)addFirst(org.apache.flume.channel.file.LogFile.MetaDataWriter)ListIterator<MetaDataWriter>add(int,org.apache.flume.channel.file.LogFile.MetaDataWriter)set(int,org.apache.flume.channel.file.LogFile.MetaDataWriter)Comparator<? super MetaDataWriter>UnaryOperator<MetaDataWriter>Function<MetaDataWriter,MetaDataWriter>char'-'-metaDataWriterorderID/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/CheckpointRebuilder$ComparableFlumeEventPointer.classpointerComparable<ComparableFlumeEventPointer>"FlumeEventPointer cannot be"
          + "null while creating a ComparableFlumeEventPointer"FlumeEventPointer cannot benull while creating a ComparableFlumeEventPointeroClass<? extends ComparableFlumeEventPointer>argsoptionsopt"c"c"checkpoint directory"checkpoint directoryparserclicheckpointDirlogDirs"l"l",",ArrayList<File>AbstractList<File>AbstractCollection<File>"t"checkpointFile"checkpoint"checkpoint"comma-separated list of log directories"comma-separated list of log directories"capacity of the channel"capacity of the channellogDir? extends FileCollection<? extends File>Iterable<? extends File>Spliterator<File>? super FileConsumer<? super File>Iterator<File>Stream<File>BaseStream<File,Stream<File>>Predicate<? super File>add(java.io.File)addLast(java.io.File)addFirst(java.io.File)ListIterator<File>add(int,java.io.File)set(int,java.io.File)Comparator<? super File>UnaryOperator<File>Function<File,File>"Cannot execute fast replay"Cannot execute fast replay"Checkpoint exists"Checkpoint existsbackingStore"channel"channel"Main"Main"inflighttakes"inflighttakes"inflightputs"inflightputsrebuilder"Could not rebuild the checkpoint due to errors."Could not rebuild the checkpoint due to errors.Unfortunately same log order id does not mean same eventfor older logs./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/Commit.javagetRecordTypegetRecordType()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/Commit.classgetTypegetType()readProtosreadProtos(java.io.InputStream)writeProtoswriteProtos(java.io.OutputStream)CommitCommit(java.lang.Long,java.lang.Long,short)Commit(java.lang.Long,java.lang.Long)typetransactionIDlogWriteOrderIDinoutcommitBuilder"Commit cannot be null"Commit cannot be nullbuilder"Commit [type="Commit [type=", getLogWriteOrderID()=", getLogWriteOrderID()=", getTransactionID()=", getTransactionID()="]"]Represents a Commit on diskType of Commit Take|Put/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/CorruptEventException.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/CorruptEventException.class2986946303540798416L2986946303540798416th/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/EventQueueBackingStore.javagetLogWriteOrderIDgetLogWriteOrderID()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/EventQueueBackingStore.classgetNamegetName()getCapacitygetCapacity()setHeadsetHead(int)getHeadgetHead()setSizesetSize(int)getSizegetSize()closeclose()syncRequiredsyncRequired()put(int,long)ImmutableSortedSet<>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/ImmutableSortedSet.classImmutableSortedSetFauxverideShim<>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/ImmutableSortedSetFauxverideShim.classImmutableSet<>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/ImmutableSet.classImmutableCollection<>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/ImmutableCollection.class"Use ImmutableList.of or another implementation"Use ImmutableList.of or another implementationSortedIterable<>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/SortedIterable.classImmutableSortedSet<Integer>ImmutableSortedSetFauxverideShim<Integer>ImmutableSet<Integer>ImmutableCollection<Integer>AbstractCollection<Integer>Collection<Integer>Iterable<Integer>Set<Integer>NavigableSet<Integer>SortedSet<Integer>SequencedSet<Integer>SequencedCollection<Integer>SortedIterable<Integer>getReferenceCountsgetReferenceCounts()decrementFileIDdecrementFileID(int)incrementFileIDincrementFileID(int)checkpoint()beginCheckpointbeginCheckpoint()queueHeadqueueSize"backupComplete"backupCompleteindexhead/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/EventQueueBackingStoreFactory.javaupgradeupgrade(java.io.File,int,java.lang.String,java.io.File,boolean,boolean,org.apache.flume.channel.file.instrumentation.FileChannelCounter)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/EventQueueBackingStoreFactory.classget(java.io.File,java.io.File,int,java.lang.String,org.apache.flume.channel.file.instrumentation.FileChannelCounter,boolean,boolean,boolean)get(java.io.File,int,java.lang.String,org.apache.flume.channel.file.instrumentation.FileChannelCounter,boolean)get(java.io.File,int,java.lang.String,org.apache.flume.channel.file.instrumentation.FileChannelCounter)EventQueueBackingStoreFactoryEventQueueBackingStoreFactory()Class<EventQueueBackingStoreFactory>counterbackupCheckpointDirshouldBackupcompressBackupmetaDataFilecheckpointFileHandlecheckpointExistsmetaDataExists"MetaData file for checkpoint "
              + " exists but checkpoint does not. Checkpoint = "MetaData file for checkpoint  exists but checkpoint does not. Checkpoint = ", metaDataFile = ", metaDataFile = "The last checkpoint was not completed correctly, " +
                  "since Checkpoint file does not exist while metadata " +
                  "file does."The last checkpoint was not completed correctly, since Checkpoint file does not exist while metadata file does."Cannot create "Cannot create "r"r"Found version "Found version " in " in "Checkpoint file exists with "Checkpoint file exists with " but no metadata file found." but no metadata file found."Unable to close "Unable to close backingStoreV2backupName"-backup-"-backup-"Attempting upgrade of "Attempting upgrade of " for " for  if we have a metadata file but no checkpoint file, we have a problem delete everything in the checkpoint directory and force a full replay. brand new, use v3 v3 due to meta file, version will be checked by backing store/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/EventQueueBackingStoreFile.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/EventQueueBackingStoreFile.classstartBackupThreadstartBackupThread()checkpointBackUpExecutorbackupDirbackupCompletedSemafileChannelCounterMAX_ALLOC_BUFFER_SIZEMap<Integer,Long>HashMap<Integer,Long>AbstractMap<Integer,Long>HashMap<Integer,Long>()BiFunction<? super Long,? super Long,? extends Long>merge(java.lang.Integer,java.lang.Long,java.util.function.BiFunction)? super IntegerBiFunction<? super Integer,? super Long,? extends Long>compute(java.lang.Integer,java.util.function.BiFunction)computeIfPresent(java.lang.Integer,java.util.function.BiFunction)Function<? super Integer,? extends Long>computeIfAbsent(java.lang.Integer,java.util.function.Function)replace(java.lang.Integer,java.lang.Long)replace(java.lang.Integer,java.lang.Long,java.lang.Long)putIfAbsent(java.lang.Integer,java.lang.Long)BiConsumer<? super Integer,? super Long>getOrDefault(java.lang.Object,java.lang.Long)Entry<Integer,Long>Set<Entry<Integer,Long>>Collection<Entry<Integer,Long>>Iterable<Entry<Integer,Long>>? extends IntegerMap<? extends Integer,? extends Long>put(java.lang.Integer,java.lang.Long)AbstractMap<Integer,Long>()Node<Integer,Long>TreeNode<Integer,Long>newTreeNode(int,java.lang.Integer,java.lang.Long,java.util.HashMap.Node)newNode(int,java.lang.Integer,java.lang.Long,java.util.HashMap.Node)Node<Integer,Long>[]putVal(int,java.lang.Integer,java.lang.Long,boolean,boolean)HashMap<Integer,Long>(java.util.Map)HashMap<Integer,Long>(int)HashMap<Integer,Long>(int,float)Map<Integer,AtomicInteger>HashMap<Integer,AtomicInteger>AbstractMap<Integer,AtomicInteger>Class<EventQueueBackingStoreFile>10241029".snappy".snappycheckpointBackupDirbackupCheckpointtotalBytescheckpointComplete"rw"rw"Preallocated "Preallocated " to " to " for capacity " for capacity "Configured capacity is "Configured capacity is " but the "
          + " checkpoint file capacity is " but the  checkpoint file capacity is ". See FileChannel documentation on how to change a channels" +
          " capacity.". See FileChannel documentation on how to change a channels capacity."Invalid version: "Invalid version: " " ", expected ", expected "Checkpoint was not completed correctly,"
          + " probably because the agent stopped while the channel was"
          + " checkpointing."Checkpoint was not completed correctly, probably because the agent stopped while the channel was checkpointing." - CheckpointBackUpThread" - CheckpointBackUpThreadbackupDirectoryavailablePermitsbackupFileFile[]checkpointFiles"Expected no permits to be available in the backup semaphore, " +
            "but "Expected no permits to be available in the backup semaphore, but " permits were available." permits were available.10ex"Error while doing backup of checkpoint. Could " +
            "not remove"Error while doing backup of checkpoint. Could not remove".""Could not retrieve files " +
        "from the checkpoint directory. Cannot complete backup of the " +
        "checkpoint."Could not retrieve files from the checkpoint directory. Cannot complete backup of the checkpoint.origFileSpliterator<String>Consumer<? super String>Iterator<String>Stream<String>BaseStream<String,Stream<String>>Predicate<? super String>Collection<? extends String>Iterable<? extends String>add(java.lang.String)"The backup file exists " +
        "while it is not supposed to. Are multiple channels configured to use " +
        "this directory: "The backup file exists while it is not supposed to. Are multiple channels configured to use this directory: " as backup?" as backup?"Could not create backup file. Backup of checkpoint will " +
          "not be used during replay even if checkpoint is bad."Could not create backup file. Backup of checkpoint will not be used during replay even if checkpoint is bad.backupFiles"Start checkpoint for "Start checkpoint for ", elements to sync = ", elements to sync = permits"Expected only one or less " +
          "permits to checkpoint, but got "Expected only one or less permits to checkpoint, but got " permits" permits"Previous backup of checkpoint files is still " +
            "in progress. Will attempt to checkpoint only at the end of the " +
            "next checkpoint interval. Try increasing the checkpoint interval " +
            "if this error happens often."Previous backup of checkpoint files is still in progress. Will attempt to checkpoint only at the end of the next checkpoint interval. Try increasing the checkpoint interval if this error happens often.Iterator<Integer>itSpliterator<Integer>Consumer<? super Integer>Stream<Integer>BaseStream<Integer,Stream<Integer>>Predicate<? super Integer>Collection<? extends Integer>Iterable<? extends Integer>add(java.lang.Integer)"Updating checkpoint metadata: logWriteOrderID: "Updating checkpoint metadata: logWriteOrderID: ", queueSize: ", queueSize: ", queueHead: ", queueHead: "Error writing metadata"Error writing metadatahasNexthasNext()Consumer<? super E>forEachRemainingforEachRemaining(java.util.function.Consumer)remove()nextnext()"concurrent update detected "concurrent update detected "Expected the checkpoint backup exector to be non-null, " +
            "but it is null. Checkpoint will not be backed up."Expected the checkpoint backup exector to be non-null, but it is null. Checkpoint will not be backed up."Attempting to back up checkpoint."Attempting to back up checkpoint./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/EventQueueBackingStoreFile$1.classthrowable"Backing up of checkpoint directory failed."Backing up of checkpoint directory failed."Checkpoint backup completed."Checkpoint backup completed."Error closing "Error closing "Interrupted while waiting for checkpoint backup to " +
                 "complete"Interrupted while waiting for checkpoint backup to completerealIndexresult? super AtomicInteger? extends AtomicIntegerBiFunction<? super AtomicInteger,? super AtomicInteger,? extends AtomicInteger>merge(java.lang.Integer,java.util.concurrent.atomic.AtomicInteger,java.util.function.BiFunction)BiFunction<? super Integer,? super AtomicInteger,? extends AtomicInteger>Function<? super Integer,? extends AtomicInteger>replace(java.lang.Integer,java.util.concurrent.atomic.AtomicInteger)replace(java.lang.Integer,java.util.concurrent.atomic.AtomicInteger,java.util.concurrent.atomic.AtomicInteger)putIfAbsent(java.lang.Integer,java.util.concurrent.atomic.AtomicInteger)BiConsumer<? super Integer,? super AtomicInteger>getOrDefault(java.lang.Object,java.util.concurrent.atomic.AtomicInteger)Entry<Integer,AtomicInteger>Set<Entry<Integer,AtomicInteger>>Collection<Entry<Integer,AtomicInteger>>Iterable<Entry<Integer,AtomicInteger>>Collection<AtomicInteger>Iterable<AtomicInteger>Map<? extends Integer,? extends AtomicInteger>put(java.lang.Integer,java.util.concurrent.atomic.AtomicInteger)"null counter "null counter successbytebyte[]initBufferremainingBytesinflightTakesFileinflightPutsFilequeueSetDir38L8"debug"debugSetMultimap<Long,Long>Multimap<Long,Long>putMaptakeMap"File "File " does not exist" does not exist" is empty" is empty"File Reference Counts"File Reference Counts"Queue Capacity "Queue Capacity "Queue Size "Queue Size "Queue Head "Queue Head 32":"" fileID = " fileID = ", offset = ", offset = "Inflight Puts:"Inflight Puts:txnIDMap<Long,Collection<Long>>BiConsumer<? super Long,? super Long>Entry<Long,Long>Collection<Entry<Long,Long>>Iterable<Entry<Long,Long>>Iterable<? extends Long>Multimap<? extends Long,? extends Long>put(java.lang.Long,java.lang.Long)Set<Entry<Long,Long>>"Transaction ID: "Transaction ID: "Inflight takes:"Inflight takes: 2MBThis method backs up the checkpoint and its metadata files. This methodis called once the checkpoint is completely written and is calledfrom a separate thread which runs in the background while the file channelcontinues operation.- the directory to which the backup files should becopied.- if the copy failed, or if there is not enough diskspace to copy the checkpoint files over.Restore the checkpoint, if it is found to be bad.true - if the previous backup was successfully completed andrestore was successfully completed.- If restore failed due to IOException Force the checkpoint to not happen by throwing an exception. Start checkpoint Finish checkpointThis method starts backing up the checkpoint in the background. Wait till the executor dies.totalBytes <= MAX_ALLOC_BUFFER_SIZE, so this can be cast to intwithout a problem.At this point, remainingBytes is < MAX_ALLOC_BUFFER_SIZE,so casting to int is fine./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/EventQueueBackingStoreFileV2.javaencodeActiveLogCounterencodeActiveLogCounter(int,int)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/EventQueueBackingStoreFileV2.classPair<>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/Pair.classPair<Integer,Integer>deocodeActiveLogCounterdeocodeActiveLogCounter(long)EventQueueBackingStoreFileV2EventQueueBackingStoreFileV2(java.io.File,int,java.lang.String,org.apache.flume.channel.file.instrumentation.FileChannelCounter)MAX_ACTIVE_LOGSINDEX_ACTIVE_LOGINDEX_HEADINDEX_SIZEindexMaxLog"capacity must be greater than 0 "capacity must be greater than 0 inextFileCodeidAndCountgetLeftgetLeft()LRPair<L,R>getRightgetRight()PairPair(java.lang.Object,java.lang.Object)Pair<Integer,Integer>(java.lang.Integer,java.lang.Integer)"Too many active logs "Too many active logs fileIdList<Long>SequencedCollection<Long>fileIdAndCountEncodedArrayList<Long>AbstractList<Long>AbstractCollection<Long>ArrayList<Long>()Spliterator<Long>Consumer<? super Long>Iterator<Long>Stream<Long>BaseStream<Long,Stream<Long>>Predicate<? super Long>Collection<? extends Long>add(java.lang.Long)AbstractCollectionAbstractCollection()AbstractCollection<Long>()addLast(java.lang.Long)addFirst(java.lang.Long)ListIterator<Long>add(int,java.lang.Long)set(int,java.lang.Long)Comparator<? super Long>UnaryOperator<Long>Function<Long,Long>removeRangeremoveRange(int,int)subListRangeChecksubListRangeCheck(int,int,int)AbstractListAbstractList()AbstractList<Long>()checkInvariantscheckInvariants()removeIf(java.util.function.Predicate,int,int)batchRemovebatchRemove(java.util.Collection,boolean,int,int)hashCodeRangehashCodeRange(int,int)List<?>SequencedCollection<?>equalsRangeequalsRange(java.util.List,int,int)elementAtelementAt(java.lang.Object[],int)elementDataelementData(int)lastIndexOfRangelastIndexOfRange(java.lang.Object,int,int)indexOfRangeindexOfRange(java.lang.Object,int,int)ensureCapacityensureCapacity(int)trimToSizetrimToSize()ArrayListArrayList(java.util.Collection)ArrayList<Long>(java.util.Collection)ArrayList()ArrayList(int)ArrayList<Long>(int)emptySlots0L/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/EventQueueBackingStoreFileV3.javaupgrade(org.apache.flume.channel.file.EventQueueBackingStoreFileV2,java.io.File,java.io.File)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/EventQueueBackingStoreFileV3.classgetMetaDataFilegetMetaDataFile()EventQueueBackingStoreFileV3EventQueueBackingStoreFileV3(java.io.File,int,java.lang.String,org.apache.flume.channel.file.instrumentation.FileChannelCounter,java.io.File,boolean,boolean)EventQueueBackingStoreFileV3(java.io.File,int,java.lang.String,org.apache.flume.channel.file.instrumentation.FileChannelCounter)Class<EventQueueBackingStoreFileV3>"Starting up with "Starting up with " and " and inputStream"Reading checkpoint metadata from "Reading checkpoint metadata from "The checkpoint metadata file does "
              + "not exist or has zero length"The checkpoint metadata file does not exist or has zero length"Checkpoint and Meta files have differing " +
              "logWriteOrderIDs "Checkpoint and Meta files have differing logWriteOrderIDs ", and ", and activeLogList<ActiveLog>SequencedCollection<ActiveLog>Collection<ActiveLog>Iterable<ActiveLog>logFileID"Checkpoint metadata file is invalid. "
            + "The agent might have been stopped while it was being "
            + "written"Checkpoint metadata file is invalid. The agent might have been stopped while it was being writtencheckpointBuilderoutputStream"The checkpoint metadata file does " +
            "not exist, but a backup exists"The checkpoint metadata file does not exist, but a backup existsactiveLogBuilderreferenceCounts If a backup exists, then throw an exception to recover checkpoint/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/EventUtils.javatransactionEventRecordReturns the Event encapsulated by a Put wrapperTransactionEventEvent if Put instance is present, null otherwise/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/FileChannel.javagetChannelCountergetChannelCounter()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/FileChannel.classgetLoggetLog()checkpointBackupRestoredcheckpointBackupRestored()setOpensetOpen(boolean)didFullReplayDueToBadCheckpointExceptiondidFullReplayDueToBadCheckpointException()didFastReplaydidFastReplay()createLogBuildercreateLogBuilder()checkpointOnClosefsyncIntervalcompressBackupCheckpointuseDualCheckpointsencryptionCipherProviderencryptionActiveKeyencryptionKeyProvideruseFastReplayuseLogReplayV1channelCounterchannelNameDescriptorThreadLocal<FileBackedTransaction>/modules/java.base/java/lang/ThreadLocal.classtransactionsqueueRemainingstartupErroropendataDirsminimumRequiredSpacemaxFileSizecheckpointIntervalkeepAliveThreadLocal<FileBackedTransaction>()dumpStackIfVirtualThreaddumpStackIfVirtualThread()childValuechildValue(java.lang.Object)childValue(org.apache.flume.channel.file.FileChannel.FileBackedTransaction)createInheritedMapcreateInheritedMap(java.lang.ThreadLocal.ThreadLocalMap)createMapcreateMap(java.lang.Thread,java.lang.Object)createMap(java.lang.Thread,org.apache.flume.channel.file.FileChannel.FileBackedTransaction)getMapgetMap(java.lang.Thread)removeCarrierThreadLocalremoveCarrierThreadLocal()setCarrierThreadLocalsetCarrierThreadLocal(java.lang.Object)setCarrierThreadLocal(org.apache.flume.channel.file.FileChannel.FileBackedTransaction)set(java.lang.Object)set(org.apache.flume.channel.file.FileChannel.FileBackedTransaction)isCarrierThreadLocalPresentisCarrierThreadLocalPresent()getCarrierThreadLocalgetCarrierThreadLocal()get()ThreadLocalThreadLocal()Supplier<>/modules/java.base/java/util/function/Supplier.classThreadLocal<>SThreadLocal<S>withInitialwithInitial(java.util.function.Supplier)? extends SSupplier<? extends S>initialValueinitialValue()"[channel=unknown]"[channel=unknown]Class<FileChannel>"[channel="[channel=contexthomePath"user.home"'\\'\'/'strCheckpointDir"/.flume/file-channel/checkpoint"/.flume/file-channel/checkpointstrBackupCheckpointDirstrDataDirsClass<String>"/.flume/file-channel/data"/.flume/file-channel/dataencryptionContextencryptionKeyProviderName"Dual checkpointing is enabled, but the backup directory is not set. " +
              "Please set "Dual checkpointing is enabled, but the backup directory is not set. Please set " " +
              "to enable dual checkpointing" to enable dual checkpointing"Could not configure "Could not configure ". The checkpoint backup " +
              "directory and the checkpoint directory are " +
              "configured to be the same.". The checkpoint backup directory and the checkpoint directory are configured to be the same."Invalid capacity specified, initializing channel to "
          + "default capacity of {}"Invalid capacity specified, initializing channel to default capacity of {}"Invalid transaction capacity specified, " +
          "initializing channel to default " +
          "capacity of {}"Invalid transaction capacity specified, initializing channel to default capacity of {}"File Channel transaction capacity cannot be greater than the " +
            "capacity of the channel."File Channel transaction capacity cannot be greater than the capacity of the channel."Checkpoint interval is invalid: "Checkpoint interval is invalid: ", using default: ", using default: keyProviderContext"Encryption configuration problem: "Encryption configuration problem: " is missing" is missing" is present while key " +
              "provider name is not." is present while key provider name is not." is present while " +
              "key provider name is not.""Starting {}..."Starting {}...depth"Unable to acquire "Unable to acquire " permits " permits "Queue Size after replay: "Queue Size after replay: "Failed to start the file channel "Failed to start the file channel "Stopping {}..."Stopping {}..."FileChannel "FileChannel " { dataDirs: " { dataDirs: "Channel closed "Channel closed ". Due to ". Due to ? extends ThrowableClass<? extends Throwable>arrayTypearrayType()componentTypecomponentType()isPrimitiveisPrimitive()isArrayisArray()isSealedisSealed()Class[]Class<>[]Class<?>[]getPermittedSubclassesgetPermittedSubclasses()isHiddenisHidden()nativeOptional<ClassDesc>descriptorStringdescriptorString()getNestMembersgetNestMembers()isNestmateOfisNestmateOf(java.lang.Class)getNestHostgetNestHost()AnnotatedType[]getAnnotatedInterfacesgetAnnotatedInterfaces()getAnnotatedSuperclassgetAnnotatedSuperclass()? extends AnnotationClass<? extends Annotation>Map<Class<? extends Annotation>,Annotation>getDeclaredAnnotationMapgetDeclaredAnnotationMap()getAnnotationTypegetAnnotationType()casAnnotationTypecasAnnotationType(sun.reflect.annotation.AnnotationType,sun.reflect.annotation.AnnotationType)Annotation[]getDeclaredAnnotationsgetDeclaredAnnotations()AA[]getDeclaredAnnotationsByTypegetDeclaredAnnotationsByType(java.lang.Class)Class<A>getDeclaredAnnotationgetDeclaredAnnotation(java.lang.Class)getAnnotationsgetAnnotations()getAnnotationsByTypegetAnnotationsByType(java.lang.Class)isAnnotationPresentisAnnotationPresent(java.lang.Class)getAnnotationgetAnnotation(java.lang.Class)U? extends UClass<? extends U>asSubclassasSubclass(java.lang.Class)Class<U>castcast(java.lang.Object)Map<String,T>enumConstantDirectoryenumConstantDirectory()Map<String,? extends Throwable>getEnumConstantsSharedgetEnumConstantsShared()Throwable[]getEnumConstantsgetEnumConstants()isRecordisRecord()isEnumisEnum()desiredAssertionStatusdesiredAssertionStatus()getConstantPoolgetConstantPool()getExecutableTypeAnnotationBytesgetExecutableTypeAnnotationBytes(java.lang.reflect.Executable)getRawTypeAnnotationsgetRawTypeAnnotations()getRawAnnotationsgetRawAnnotations()getPrimitiveClassgetPrimitiveClass(java.lang.String)protectionDomainprotectionDomain()getProtectionDomaingetProtectionDomain()getResourcegetResource(java.lang.String)getResourceAsStreamgetResourceAsStream(java.lang.String)java.lang.reflectConstructor<>/modules/java.base/java/lang/reflect/Constructor.class/modules/java.base/java/lang/reflectConstructor<T>getDeclaredConstructorgetDeclaredConstructor(java.lang.Class[])Constructor<? extends Throwable>List<Method>SequencedCollection<Method>Collection<Method>Iterable<Method>getDeclaredPublicMethodsgetDeclaredPublicMethods(java.lang.String,java.lang.Class[])getDeclaredMethodgetDeclaredMethod(java.lang.String,java.lang.Class[])getDeclaredFieldgetDeclaredField(java.lang.String)Constructor[]Constructor<>[]Constructor<?>Constructor<?>[]getDeclaredConstructorsgetDeclaredConstructors()Method[]getDeclaredMethodsgetDeclaredMethods()RecordComponent[]getRecordComponentsgetRecordComponents()Field[]getDeclaredFieldsgetDeclaredFields()getDeclaredClassesgetDeclaredClasses()getConstructorgetConstructor(java.lang.Class[])getMethodgetMethod(java.lang.String,java.lang.Class[])getFieldgetField(java.lang.String)getConstructorsgetConstructors()getMethodsgetMethods()getFieldsgetFields()getClassesgetClasses()isMemberClassisMemberClass()isLocalClassisLocalClass()isAnonymousClassisAnonymousClass()isUnnamedClassisUnnamedClass()getCanonicalNamegetCanonicalName()getTypeNamegetTypeName()getSimpleNamegetSimpleName()getEnclosingClassgetEnclosingClass()getEnclosingConstructorgetEnclosingConstructor()getEnclosingMethodgetEnclosingMethod()setSignerssetSigners(java.lang.Object[])getSignersgetSigners()Set<AccessFlag>Collection<AccessFlag>Iterable<AccessFlag>accessFlagsaccessFlags()getModifiersgetModifiers()getComponentTypegetComponentType()Type[]getGenericInterfacesgetGenericInterfaces()getInterfacesgetInterfaces()getPackageNamegetPackageName()getPackagegetPackage()getGenericSuperclassgetGenericSuperclass()Class<? super T>getSuperclassgetSuperclass()TypeVariable[]TypeVariable<>/modules/java.base/java/lang/reflect/TypeVariable.classTypeVariable<>[]TypeVariable<Class>TypeVariable<Class>[]getTypeParametersgetTypeParameters()TypeVariable<Class<? extends Throwable>>TypeVariable<Class<? extends Throwable>>[]getClassDatagetClassData()getModulegetModule()getClassLoader0getClassLoader0()getClassLoadergetClassLoader()isSyntheticisSynthetic()isAnnotationisAnnotation()isInterfaceisInterface()isAssignableFromisAssignableFrom(java.lang.Class)isInstanceisInstance(java.lang.Object)newInstancenewInstance()forNameforName(java.lang.Module,java.lang.String)forName(java.lang.String,boolean,java.lang.ClassLoader)forName(java.lang.String)TypeVariable<?>typeVarBoundstypeVarBounds(java.lang.reflect.TypeVariable)toGenericStringtoGenericString()classValueMap": ": "Thread has transaction which is still open: "Thread has transaction which is still open: "Channel closed"Channel closed"log""queue""Error while trying to close the log."Error while trying to close the log.getStateAsStringgetStateAsString()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/FileChannel$FileBackedTransaction.classisClosedisClosed()LinkedBlockingDeque<FlumeEventPointer>/modules/java.base/java/util/concurrent/LinkedBlockingDeque.classAbstractQueue<FlumeEventPointer>/modules/java.base/java/util/AbstractQueue.classAbstractCollection<FlumeEventPointer>Collection<FlumeEventPointer>Iterable<FlumeEventPointer>Queue<FlumeEventPointer>/modules/java.base/java/util/Queue.classBlockingDeque<FlumeEventPointer>/modules/java.base/java/util/concurrent/BlockingDeque.classBlockingQueue<FlumeEventPointer>/modules/java.base/java/util/concurrent/BlockingQueue.classDeque<FlumeEventPointer>/modules/java.base/java/util/Deque.classSequencedCollection<FlumeEventPointer>putListtakeListtransCapacityLinkedBlockingDeque<FlumeEventPointer>(int)Spliterator<FlumeEventPointer>? super FlumeEventPointerConsumer<? super FlumeEventPointer>Iterator<FlumeEventPointer>Stream<FlumeEventPointer>BaseStream<FlumeEventPointer,Stream<FlumeEventPointer>>Predicate<? super FlumeEventPointer>? extends FlumeEventPointerCollection<? extends FlumeEventPointer>Iterable<? extends FlumeEventPointer>add(org.apache.flume.channel.file.FlumeEventPointer)AbstractCollection<FlumeEventPointer>()peekpeek()elementelement()pollpoll()offeroffer(java.lang.Object)offer(org.apache.flume.channel.file.FlumeEventPointer)AbstractQueueAbstractQueue()AbstractQueue<FlumeEventPointer>()Collection<? super E>Iterable<? super E>drainTodrainTo(java.util.Collection,int)Collection<? super FlumeEventPointer>Iterable<? super FlumeEventPointer>drainTo(java.util.Collection)remainingCapacityremainingCapacity()poll(long,java.util.concurrent.TimeUnit)take()offer(java.lang.Object,long,java.util.concurrent.TimeUnit)offer(org.apache.flume.channel.file.FlumeEventPointer,long,java.util.concurrent.TimeUnit)put(java.lang.Object)put(org.apache.flume.channel.file.FlumeEventPointer)addLast(org.apache.flume.channel.file.FlumeEventPointer)addFirst(org.apache.flume.channel.file.FlumeEventPointer)Deque<>Queue<>descendingIteratordescendingIterator()poppop()pushpush(java.lang.Object)push(org.apache.flume.channel.file.FlumeEventPointer)removeLastOccurrenceremoveLastOccurrence(java.lang.Object)removeFirstOccurrenceremoveFirstOccurrence(java.lang.Object)peekLastpeekLast()peekFirstpeekFirst()pollLastpollLast()pollFirstpollFirst()offerLastofferLast(java.lang.Object)offerLast(org.apache.flume.channel.file.FlumeEventPointer)offerFirstofferFirst(java.lang.Object)offerFirst(org.apache.flume.channel.file.FlumeEventPointer)pollLast(long,java.util.concurrent.TimeUnit)pollFirst(long,java.util.concurrent.TimeUnit)takeLasttakeLast()takeFirsttakeFirst()offerLast(java.lang.Object,long,java.util.concurrent.TimeUnit)offerLast(org.apache.flume.channel.file.FlumeEventPointer,long,java.util.concurrent.TimeUnit)offerFirst(java.lang.Object,long,java.util.concurrent.TimeUnit)offerFirst(org.apache.flume.channel.file.FlumeEventPointer,long,java.util.concurrent.TimeUnit)putLastputLast(java.lang.Object)putLast(org.apache.flume.channel.file.FlumeEventPointer)putFirstputFirst(java.lang.Object)putFirst(org.apache.flume.channel.file.FlumeEventPointer)Node<E>/modules/java.base/java/util/concurrent/LinkedBlockingDeque$Node.classforEachFromforEachFrom(java.util.function.Consumer,java.util.concurrent.LinkedBlockingDeque.Node)Node<FlumeEventPointer>succsucc(java.util.concurrent.LinkedBlockingDeque.Node)unlinkunlink(java.util.concurrent.LinkedBlockingDeque.Node)LinkedBlockingDequeLinkedBlockingDeque(java.util.Collection)LinkedBlockingDeque<FlumeEventPointer>(java.util.Collection)LinkedBlockingDeque(int)LinkedBlockingDeque()LinkedBlockingDeque<FlumeEventPointer>()locklastfirstEnum<State>Comparable<State>compareTo(org.apache.flume.channel.BasicTransactionSemantics.State)EnumDesc<State>DynamicConstantDesc<State>Optional<EnumDesc<State>>Class<State>Enum<State>(java.lang.String,int)event"Put queue for FileBackedTransaction " +
            "of capacity "Put queue for FileBackedTransaction of capacity " full, consider " +
            "committing more frequently, increasing capacity or " +
            "increasing thread count. " full, consider committing more frequently, increasing capacity or increasing thread count. "The channel has reached it's capacity. "
            + "This might be the result of a sink on the channel having too "
            + "low of batch size, a downstream system running slower than "
            + "normal, or that the channel capacity is just too low. "The channel has reached it's capacity. This might be the result of a sink on the channel having too low of batch size, a downstream system running slower than normal, or that the channel capacity is just too low. ptr"putList offer failed "putList offer failed "Put failed due to IO error "Put failed due to IO error "Take list for FileBackedTransaction, capacity "Take list for FileBackedTransaction, capacity " full, consider committing more frequently, " +
            "increasing capacity, or increasing thread count. " full, consider committing more frequently, increasing capacity, or increasing thread count. "takeList offer failed "takeList offer failed "Take failed due to IO error "Take failed due to IO error "Corrupt record replaced by File Channel Integrity " +
                  "tool found. Will retrieve next event"Corrupt record replaced by File Channel Integrity tool found. Will retrieve next event"Corrupt record found. Event will be " +
                  "skipped, and next event will be read."Corrupt record found. Event will be skipped, and next event will be read."nonzero puts and takes "nonzero puts and takes "Queue add failed, this shouldn't be able to "Queue add failed, this shouldn't be able to "happen. A portion of the transaction has been "happen. A portion of the transaction has been "added to the queue but the remaining portion "added to the queue but the remaining portion "cannot be added. Those messages will be consumed "cannot be added. Those messages will be consumed "despite this transaction failing. Please report."despite this transaction failing. Please report."Commit failed due to IO error "Commit failed due to IO error "Queue add failed, this shouldn't be able to happen "Queue add failed, this shouldn't be able to happen <p>A durable {@link Channel} implementation that uses the local file system forits storage.</p>FileChannel works by writing all transactions to a set of directoriesspecified in the configuration. Additionally, when a commit occursthe transaction is synced to disk.FileChannel is marked{@link org.apache.flume.annotations.InterfaceAudience.Private} because itshould only be instantiated via a configuration. For example, users shouldcertainly use FileChannel but not by instantiating FileChannel objects.Meaning the label Private applies to user-developers not user-operators.In cases where a Channel is required by instantiated by user-developers{@link org.apache.flume.channel.MemoryChannel} should be used.If the backup directory is the same as the checkpoint directory,then throw an exception and force the config system to ignore thischannel. cannot be over FileChannelConfiguration.DEFAULT_MAX_FILE_SIZEThis method makes sure that <code>this.open</code> and <code>channelCounter.open</code>are in sync.Only for internal use, call from synchronized methods only. It also assumes that<code>channelCounter</code> is not null.Did this channel recover a backup of the checkpoint to restart?true if the channel recovered using a backup.Transaction backed by a file. This transaction supports either putsor takes but not both. this does not need to be in the critical section as it does not modify the structure of the log or queue. release slot obtained in the case the put fails for any reason1. Take an event which is in the queue.2. If getting that event does not throw NoopRecordException,then return it.3. Else try to retrieve the next event from the queue4. Repeat 2 and 3 until queue is empty or an event is returned. first add to takeList so that if write to disk fails rollback actually does it's work write take to disk since rollback is being called, puts will never make it on to the queue and we need to be sure to release the resources/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/FileChannelConfiguration.java"checkpointDir""backupCheckpointDir""dataDirs""transactionCapacity"transactionCapacity10000"checkpointInterval"30L30"maxFileSize"500L5001024L"minimumRequiredSpace"1L"capacity"1000000"keep-alive"keep-alive"use-log-replay-v1"use-log-replay-v1"use-fast-replay"use-fast-replay"useDualCheckpoints""compressBackupCheckpoint""fsyncPerTransaction""fsyncInterval""checkpointOnClose"Directory Checkpoints will be written inThe directory to which the checkpoint must be backed upDirectories data files will be written in. Multiple directoriescan be specified as comma separated values. Writes willbe written in a round robin fashion.Maximum number of put/take events in a transaction. Default: 1000Interval at which checkpoints should be taken. Default 30s (ms)Max file size for data files, cannot exceed the default. Default~ 1.5GB ~1.52 GMinimum space required defaults to 500MBMinimum space floor is 1MBMaximum capacity of the channel.Default: 1,000,000The length of time we will wait for space available to do a Put.Default: 3 (seconds)Turn on Flume 1.2 log replay logic seconds./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/FlumeEvent.javafromfrom(java.io.DataInput)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/FlumeEvent.classFlumeEventFlumeEvent(java.util.Map,byte[])FlumeEvent()bodyheadersThreadLocal<CharsetDecoder>DECODER_FACTORYThreadLocal<CharsetEncoder>ENCODER_FACTORYEVENT_MAP_TEXT_WRITABLE_ID116/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/FlumeEvent$1.classchildValue(java.nio.charset.CharsetEncoder)createMap(java.lang.Thread,java.nio.charset.CharsetEncoder)setCarrierThreadLocal(java.nio.charset.CharsetEncoder)set(java.nio.charset.CharsetEncoder)ThreadLocal<CharsetEncoder>()"UTF-8"/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/FlumeEvent$2.classchildValue(java.nio.charset.CharsetDecoder)createMap(java.lang.Thread,java.nio.charset.CharsetDecoder)setCarrierThreadLocal(java.nio.charset.CharsetDecoder)set(java.nio.charset.CharsetDecoder)ThreadLocal<CharsetDecoder>()writeHeadersencoderkeyByteschar[]keyLengthvalueBytesvalueLengthnewClassesnewHeadersnumEntriesdecoderbodyLengthkeyClassIdvalueClassIdPersistable wrapper for Event newClasses from AbstractMapWritable in Hadoop Common skip over newClasses since only Text is used/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/FlumeEventPointer.javagetOffsetgetOffset()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/FlumeEventPointer.classgetFileIDgetFileID()FlumeEventPointerFlumeEventPointer(int,int)"offset = "offset = "("(")" + ", fileID = "), fileID = ")")prime31Class<? extends FlumeEventPointer>"FlumeEventPointer [fileID="FlumeEventPointer [fileID=", offset=", offset=Pointer to an Event on disk. This is represented in memoryas a long. As such there are methods to convert from thisobject to a long and from a long to this object.Log files used to have a header, now metadata is ina separate file so data starts at offset 0./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/FlumeEventQueue.javagetCopyCountgetCopyCount()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/FlumeEventQueue.classgetSearchCountgetSearchCount()replayCompletereplayComplete()synchronizedcompleteTransactioncompleteTransaction(long)set(int,long)getFileIDsgetFileIDs()remove(org.apache.flume.channel.file.FlumeEventPointer)addWithoutCommitaddWithoutCommit(org.apache.flume.channel.file.FlumeEventPointer,long)addTailaddTail(org.apache.flume.channel.file.FlumeEventPointer)addHeadaddHead(org.apache.flume.channel.file.FlumeEventPointer)removeHeadremoveHead(long)checkpoint(boolean)deserializeInflightTakesdeserializeInflightTakes()deserializeInflightPutsdeserializeInflightPuts()FlumeEventQueueFlumeEventQueue(org.apache.flume.channel.file.EventQueueBackingStore,java.io.File,java.io.File,java.io.File)queueSetdbcopyCountcopyTimesearchCountsearchTimeinflightPutsinflightTakesEMPTYClass<FlumeEventQueue>queueSetDBDirdbFile"db"start"Capacity must be greater than zero"Capacity must be greater than zero"backingStore""inflightTakesFile""inflightPutsFile""queueSetDBDir""Could not read checkpoint."Could not read checkpoint."QueueSetDir "QueueSetDir " is a file and"
          + " could not be deleted" is a file and could not be deleted"Could not create QueueSet Dir "Could not create QueueSet Dir DBMaker<>/Users/burakyetistiren/.m2/repository/org/mapdb/mapdb/0.9.9/mapdb-0.9.9.jar/Users/burakyetistiren/.m2/repository/org/mapdb/mapdb/0.9.9/Users/burakyetistiren/.m2/repository/org/mapdb/mapdb/Users/burakyetistiren/.m2/repository/org/mapdb/Users/burakyetistiren/.m2/repository/org/Users/burakyetistiren/.m2/repository/org/mapdb/mapdb/0.9.9/mapdb-0.9.9.jar/org/Users/burakyetistiren/.m2/repository/org/mapdb/mapdb/0.9.9/mapdb-0.9.9.jar/org/mapdb/Users/burakyetistiren/.m2/repository/org/mapdb/mapdb/0.9.9/mapdb-0.9.9.jar/org/mapdb/DBMaker.classmakemake()fromHexafromHexa(java.lang.String)toHexatoHexa(byte[])extendStoreVolumeFactoryextendStoreVolumeFactory()extendStoreWALextendStoreWAL(org.mapdb.Volume.Factory)extendStoreDirectextendStoreDirect(org.mapdb.Volume.Factory)extendStoreAppendextendStoreAppend()extendWrapSnapshotEngineextendWrapSnapshotEngine(org.mapdb.Engine)extendWrapCacheextendWrapCache(org.mapdb.Engine)extendWrapStoreextendWrapStore(org.mapdb.Engine)extendArgumentCheckextendArgumentCheck()extendAsyncWriteEngineextendAsyncWriteEngine(org.mapdb.Engine)extendCacheHashTableextendCacheHashTable(org.mapdb.Engine)extendCacheHardRefextendCacheHardRef(org.mapdb.Engine)extendCacheSoftRefextendCacheSoftRef(org.mapdb.Engine)extendCacheWeakRefextendCacheWeakRef(org.mapdb.Engine)extendCacheLRUextendCacheLRU(org.mapdb.Engine)extendSnapshotEngineextendSnapshotEngine(org.mapdb.Engine)extendShutdownHookAfterextendShutdownHookAfter(org.mapdb.Engine)extendShutdownHookBeforeextendShutdownHookBefore(org.mapdb.Engine)propsGetRafModepropsGetRafMode()JVMSupportsLargeMappedFilesJVMSupportsLargeMappedFiles()propsGetXteaEncKeypropsGetXteaEncKey()propsGetBoolpropsGetBool(java.lang.String)propsGetLongpropsGetLong(java.lang.String,long)propsGetIntpropsGetInt(java.lang.String,int)makeEnginemakeEngine()makeTxMakermakeTxMaker()fullChunkAllocationEnablefullChunkAllocationEnable()doublesizeLimitsizeLimit(double)syncOnCommitDisablesyncOnCommitDisable()freeSpaceReclaimQfreeSpaceReclaimQ(int)readOnlyreadOnly()strictDBGetstrictDBGet()checksumEnablechecksumEnable()encryptionEnableencryptionEnable(byte[])encryptionEnable(java.lang.String)compressionEnablecompressionEnable()closeOnJvmShutdowncloseOnJvmShutdown()deleteFilesAfterClosedeleteFilesAfterClose()asyncWriteFlushDelayasyncWriteFlushDelay(int)asyncWriteEnableasyncWriteEnable()snapshotEnablesnapshotEnable()cacheSizecacheSize(int)mmapFileEnableIfSupportedmmapFileEnableIfSupported()mmapFileEnablePartialmmapFileEnablePartial()mmapFileEnablemmapFileEnable()cacheLRUEnablecacheLRUEnable()cacheSoftRefEnablecacheSoftRefEnable()cacheWeakRefEnablecacheWeakRefEnable()cacheHardRefEnablecacheHardRefEnable()cacheDisablecacheDisable()transactionDisabletransactionDisable()getThisgetThis()_newFileDB_newFileDB(java.io.File)newFileDBnewFileDB(java.io.File)newTempFileDBnewTempFileDB()newTempHashSetnewTempHashSet()NavigableSet<K>SortedSet<K>SequencedSet<K>SequencedCollection<K>newTempTreeSetnewTempTreeSet()HTreeMap<>/Users/burakyetistiren/.m2/repository/org/mapdb/mapdb/0.9.9/mapdb-0.9.9.jar/org/mapdb/HTreeMap.classConcurrentMap<>/modules/java.base/java/util/concurrent/ConcurrentMap.classMapWithModificationListener<>/Users/burakyetistiren/.m2/repository/org/mapdb/mapdb/0.9.9/mapdb-0.9.9.jar/org/mapdb/Bind$MapWithModificationListener.classHTreeMap<K,V>ConcurrentMap<K,V>MapWithModificationListener<K,V>newTempHashMapnewTempHashMap()BTreeMap<>/Users/burakyetistiren/.m2/repository/org/mapdb/mapdb/0.9.9/mapdb-0.9.9.jar/org/mapdb/BTreeMap.classConcurrentNavigableMap<>/modules/java.base/java/util/concurrent/ConcurrentNavigableMap.classNavigableMap<>/modules/java.base/java/util/NavigableMap.classSortedMap<>/modules/java.base/java/util/SortedMap.classSequencedMap<>/modules/java.base/java/util/SequencedMap.classBTreeMap<K,V>ConcurrentNavigableMap<K,V>NavigableMap<K,V>SortedMap<K,V>SequencedMap<K,V>newTempTreeMapnewTempTreeMap()_newAppendFileDB_newAppendFileDB(java.io.File)newAppendFileDBnewAppendFileDB(java.io.File)_newDirectMemoryDB_newDirectMemoryDB()newDirectMemoryDBnewDirectMemoryDB()_newMemoryDB_newMemoryDB()newMemoryDBnewMemoryDB()DBMakerDBMaker(java.io.File)DBMaker<>(java.io.File)DBMaker()DBMaker<>()TRUE"QueueSet " + " - "QueueSet  - "QueueSet population inserting "QueueSet population inserting " took " took force"Checkpoint not required"Checkpoint not required"Empty value "Empty value "Could not reinsert to queue, events which were taken but "
          + "not committed. Please report this issue."Could not reinsert to queue, events which were taken but not committed. Please report this issue."QueueSet is null, thus replayComplete"
          + " has been called which is illegal"QueueSet is null, thus replayComplete has been called which is illegalfileIDsTreeSet<Integer>AbstractSet<Integer>SortedSet<E>SequencedSet<E>TreeSet<Integer>(java.util.SortedSet)AbstractCollection<Integer>()AbstractSetAbstractSet()AbstractSet<Integer>()addLast(java.lang.Integer)addFirst(java.lang.Integer)last()first()tailSettailSet(java.lang.Object)tailSet(java.lang.Integer)headSetheadSet(java.lang.Object)headSet(java.lang.Integer)subSetsubSet(java.lang.Object,java.lang.Object)subSet(java.lang.Integer,java.lang.Integer)Comparator<>comparatorcomparator()Comparator<? super Integer>tailSet(java.lang.Object,boolean)tailSet(java.lang.Integer,boolean)headSet(java.lang.Object,boolean)headSet(java.lang.Integer,boolean)subSet(java.lang.Object,boolean,java.lang.Object,boolean)subSet(java.lang.Integer,boolean,java.lang.Integer,boolean)descendingSetdescendingSet()higherhigher(java.lang.Object)higher(java.lang.Integer)ceilingceiling(java.lang.Object)ceiling(java.lang.Integer)floorfloor(java.lang.Object)floor(java.lang.Integer)lowerlower(java.lang.Object)lower(java.lang.Integer)NavigableSet<E>TreeSetTreeSet(java.util.SortedSet)TreeSet(java.util.Collection)TreeSet<Integer>(java.util.Collection)TreeSet(java.util.Comparator)TreeSet<Integer>(java.util.Comparator)TreeSet()TreeSet<Integer>()NavigableMap<E,Object>SortedMap<E,Object>SequencedMap<E,Object>Map<E,Object>TreeSet(java.util.NavigableMap)NavigableMap<Integer,Object>SortedMap<Integer,Object>SequencedMap<Integer,Object>Map<Integer,Object>TreeSet<Integer>(java.util.NavigableMap)"index = "index = ", queueSize ", queueSize rightValueleftValue"Error closing db"Error closing db"Error closing backing store"Error closing backing store"Search Count = "Search Count = ", Search Time = ", Search Time = ", Copy Count = ", Copy Count = ", Copy Time = ", Copy Time = SetMultimap<Long,Integer>Multimap<Long,Integer>inflightFileIDs/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/FlumeEventQueue$InflightEventWrapper.classinflightEventsFiledigestfileChannelinflightEventsHashMultimap<Long,Long>HashMultimapGwtSerializationDependencies<Long,Long>AbstractSetMultimap<Long,Long>AbstractMapBasedMultimap<Long,Long>AbstractMultimap<Long,Long>HashMultimap<Long,Integer>HashMultimapGwtSerializationDependencies<Long,Integer>AbstractSetMultimap<Long,Integer>AbstractMapBasedMultimap<Long,Integer>AbstractMultimap<Long,Integer>"Could not"
            + "create inflight events file: "Could notcreate inflight events file: "MD5"MD5Map<Long,Collection<Integer>>BiConsumer<? super Long,? super Integer>Entry<Long,Integer>Collection<Entry<Long,Integer>>Iterable<Entry<Long,Integer>>Multimap<? extends Long,? extends Integer>put(java.lang.Long,java.lang.Integer)Set<Entry<Long,Integer>>expectedFileSize16bufferlongBufferchecksum"Expected File size of inflight events file does not match the "
                + "current file size. Checkpoint is incomplete."Expected File size of inflight events file does not match the current file size. Checkpoint is incomplete.pointerslong[]writtenLong[]"Number of events inserted into "
              + "inflights file: "Number of events inserted into inflights file: " file: " file: "Error while writing checkpoint to disk."Error while writing checkpoint to disk.inflightsfileChecksum"Checksum of inflights file differs"
            + " from the checksum expected."Checksum of inflights file differs from the checksum expected.numEventsval"Reached end of inflights buffer. Long buffer position ="Reached end of inflights buffer. Long buffer position =Queue of events in the channel. This queue stores only{@link FlumeEventPointer} objects which are representedas 8 byte longs internally. Additionally the queue itselfof longs is stored as a memory mapped file with a fixedheader and circular queue semantics. The header of the queuecontains the timestamp of last sync, the queue size andthe head position.max event capacity of queueNo need to check inflight puts, since that wouldcause elements.syncRequired() to return true.Retrieve and remove the head of the queue.FlumeEventPointer or null if queue is emptyAdd a FlumeEventPointer to the head of the queue.Called during rollbacks.to be addedtrue if space was available and pointer wasadded to the queueCalled only during rollback, so should not consider inflight takes' size,because normal puts through addTail method already account for theseevents since they are in the inflight takes. So puts will not happenin such a way that these takes cannot go back in. If this if returns true,there is a buuuuuuuug!Add a FlumeEventPointer to the tail of the queue.true if space was available and pointerwas added to the queueMust be called when a put happens to the log. This ensures that put commitsafter checkpoints will retrieve all events committed in that txn.Remove FlumeEventPointer from queue, willonly be used when recovering from a crash. It is notlegal to call this method after replayComplete has beencalled.to be removedtrue if the FlumeEventPointer was foundand removed remove() overloads should not be split, according to checkstyle. CHECKSTYLE:OFF CHECKSTYLE:ONa copy of the set of fileIDs which are currently on the queuewill be normally be used when deciding which data files canbe deletedJava implements clone pretty well. The main place this is usedin checkpointing and deleting old files, so bestto use a sorted set implementation. Shift left Sift rightMust be called when a transaction is being committed or rolled back.if txn id = 0, we are recovering from a crash. Move tail part to left Move head part to rightmax capacity of the queueCalled when ReplayHandler has completed and thus remove(FlumeEventPointer)will no longer be called.A representation of in flight events which have not yet been committed.None of the methods are thread safe, and should be called from threadsafe methods only. Both these are volatile for safe publication, they are never accessed by more than 1 thread at a time.Complete the transaction, and remove all events from inflight list.Add an event pointer to the inflights list.Serialize the set of in flights into a byte longBuffer.Returns the checksum of the buffer that is beingasynchronously written to disk.What is written out?Checksum - 16 bytesand then each key-value pair from the map:transactionid numberofeventsforthistxn listofeventpointersfor transactionIDs andevents per txn IDEvent pointersChecksumThere is no real need of filling the channel with 0s, since wewill write the exact number of bytes as expected file size.Read the inflights file and return a{@link com.google.common.collect.SetMultimap}of transactionIDs to events that were inflight.- map of inflight events per txnID.Needed for testing./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/Log.javaunlockunlock(java.io.File)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/Log.classtryLocktryLock(java.io.File)"resource"resourcelock(java.io.File)removeOldLogsremoveOldLogs(java.util.SortedSet)writeCheckpoint(java.lang.Boolean)rollroll(int,java.nio.ByteBuffer)roll(int)nextLogWriternextLogWriter(long)commit(long,short)setMaxFileSizesetMaxFileSize(long)setCheckpointIntervalsetCheckpointInterval(long)shutdownWorkershutdownWorker()lockExclusivelockExclusive()unlockSharedunlockShared()lockSharedlockShared()unlockExclusiveunlockExclusive()commitTakecommitTake(long)commitPutcommitPut(long)rollbackrollback(long)take(long,org.apache.flume.channel.file.FlumeEventPointer)put(long,org.apache.flume.Event)get(org.apache.flume.channel.file.FlumeEventPointer)getFlumeEventQueuegetFlumeEventQueue()getNextFileIDgetNextFileID()backupRestoredbackupRestored()doReplaydoReplay(org.apache.flume.channel.file.FlumeEventQueue,java.util.List,org.apache.flume.channel.file.encryption.KeyProvider,boolean)"deprecation"deprecationreplayreplay()LogLog(long,long,int,boolean,boolean,java.io.File,java.io.File,java.lang.String,boolean,boolean,long,org.apache.flume.channel.file.encryption.KeyProvider,java.lang.String,java.lang.String,long,boolean,int,boolean,org.apache.flume.channel.file.instrumentation.FileChannelCounter,java.io.File[])pendingDeletesrollbackCountcommittedCounttakeCountputCountreadCountusableSpaceRefreshIntervalencryptionKeyencryptionKeyAliascheckpointWriterLockcheckpointReadLockcheckpointLockMap<String,FileLock>locksworkerExecutorjava.util.concurrent.atomicAtomicReferenceArray<Writer>/modules/java.base/java/util/concurrent/atomic/AtomicReferenceArray.class/modules/java.base/java/util/concurrent/atomicqueueCapacitynextFileIDMap<Integer,RandomReader>idLogFileMapMIN_NUM_LOGSLOGGERHashMap<Integer,RandomReader>AbstractMap<Integer,RandomReader>HashMap<Integer,RandomReader>()? super RandomReader? extends RandomReaderBiFunction<? super RandomReader,? super RandomReader,? extends RandomReader>merge(java.lang.Integer,org.apache.flume.channel.file.LogFile.RandomReader,java.util.function.BiFunction)BiFunction<? super Integer,? super RandomReader,? extends RandomReader>Function<? super Integer,? extends RandomReader>replace(java.lang.Integer,org.apache.flume.channel.file.LogFile.RandomReader)replace(java.lang.Integer,org.apache.flume.channel.file.LogFile.RandomReader,org.apache.flume.channel.file.LogFile.RandomReader)putIfAbsent(java.lang.Integer,org.apache.flume.channel.file.LogFile.RandomReader)BiConsumer<? super Integer,? super RandomReader>getOrDefault(java.lang.Object,org.apache.flume.channel.file.LogFile.RandomReader)Entry<Integer,RandomReader>Set<Entry<Integer,RandomReader>>Collection<Entry<Integer,RandomReader>>Iterable<Entry<Integer,RandomReader>>Collection<RandomReader>Iterable<RandomReader>Map<? extends Integer,? extends RandomReader>put(java.lang.Integer,org.apache.flume.channel.file.LogFile.RandomReader)AbstractMap<Integer,RandomReader>()Node<Integer,RandomReader>TreeNode<Integer,RandomReader>newTreeNode(int,java.lang.Integer,org.apache.flume.channel.file.LogFile.RandomReader,java.util.HashMap.Node)newNode(int,java.lang.Integer,org.apache.flume.channel.file.LogFile.RandomReader,java.util.HashMap.Node)Node<Integer,RandomReader>[]putVal(int,java.lang.Integer,org.apache.flume.channel.file.LogFile.RandomReader,boolean,boolean)HashMap<Integer,RandomReader>(java.util.Map)HashMap<Integer,RandomReader>(int)HashMap<Integer,RandomReader>(int,float)"log-"log-Class<Log>"in_use.lock"in_use.lock"queueset"queuesetHashSet<String>AbstractSet<String>AbstractCollection<String>build()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/Log$Builder.classsetChannelCountersetChannelCounter(org.apache.flume.channel.file.instrumentation.FileChannelCounter)setCheckpointOnClosesetCheckpointOnClose(boolean)setBackupCheckpointDirsetBackupCheckpointDir(java.io.File)setCompressBackupCheckpointsetCompressBackupCheckpoint(boolean)setUseDualCheckpointssetUseDualCheckpoints(boolean)setEncryptionCipherProvidersetEncryptionCipherProvider(java.lang.String)setEncryptionKeyAliassetEncryptionKeyAlias(java.lang.String)setEncryptionKeyProvidersetEncryptionKeyProvider(org.apache.flume.channel.file.encryption.KeyProvider)setUseFastReplaysetUseFastReplay(boolean)setUseLogReplayV1setUseLogReplayV1(boolean)setMinimumRequiredSpacesetMinimumRequiredSpace(long)setChannelNamesetChannelName(java.lang.String)setLogDirssetLogDirs(java.io.File[])setCheckpointDirsetCheckpointDir(java.io.File)setQueueSizesetQueueSize(int)setUsableSpaceRefreshIntervalsetUsableSpaceRefreshInterval(long)setFsyncIntervalsetFsyncInterval(int)getFsyncIntervalgetFsyncInterval()setFsyncPerTransactionsetFsyncPerTransaction(boolean)isFsyncPerTransactionisFsyncPerTransaction()bBackupCheckpointDirbCompressBackupCheckpointbUseDualCheckpointsbUsableSpaceRefreshIntervalbEncryptionCipherProviderbEncryptionKeyAliasbEncryptionKeyProviderbNamebLogDirsbCheckpointDirbQueueCapacitybMaxFileSizebMinimumRequiredSpacebCheckpointIntervalBuilderBuilder()15L15intervalmaxSizecpDirdirsUseDualCheckpointsenableCheckpointOnClose"checkpointInterval <= 0"checkpointInterval <= 0"queueCapacity <= 0"queueCapacity <= 0"maxFileSize <= 0"maxFileSize <= 0"usableSpaceRefreshInterval <= 0"usableSpaceRefreshInterval <= 0"CheckpointDir "CheckpointDir " could not be created" could not be created"backupCheckpointDir is" +
          " null while dual checkpointing is enabled."backupCheckpointDir is null while dual checkpointing is enabled."Backup CheckpointDir "Backup CheckpointDir "logDirs""logDirs empty"logDirs empty"channel name should be specified"channel name should be specified"ChannelCounter must be not null"ChannelCounter must be not null"LogDir "LogDir HashMap<String,FileLock>AbstractMap<String,FileLock>"Encryption is enabled with encryptionKeyProvider = "Encryption is enabled with encryptionKeyProvider = ", encryptionKeyAlias = ", encryptionKeyAlias = ", encryptionCipherProvider = ", encryptionCipherProvider = "Encryption is not enabled"Encryption is not enabled"Encryption configuration must all " +
          "null or all not null: encryptionKeyProvider = "Encryption configuration must all null or all not null: encryptionKeyProvider = AtomicReferenceArray<Writer>(int)weakCompareAndSetReleaseweakCompareAndSetRelease(int,java.lang.Object,java.lang.Object)weakCompareAndSetRelease(int,org.apache.flume.channel.file.LogFile.Writer,org.apache.flume.channel.file.LogFile.Writer)weakCompareAndSetAcquireweakCompareAndSetAcquire(int,java.lang.Object,java.lang.Object)weakCompareAndSetAcquire(int,org.apache.flume.channel.file.LogFile.Writer,org.apache.flume.channel.file.LogFile.Writer)weakCompareAndSetVolatileweakCompareAndSetVolatile(int,java.lang.Object,java.lang.Object)weakCompareAndSetVolatile(int,org.apache.flume.channel.file.LogFile.Writer,org.apache.flume.channel.file.LogFile.Writer)compareAndExchangeReleasecompareAndExchangeRelease(int,java.lang.Object,java.lang.Object)compareAndExchangeRelease(int,org.apache.flume.channel.file.LogFile.Writer,org.apache.flume.channel.file.LogFile.Writer)compareAndExchangeAcquirecompareAndExchangeAcquire(int,java.lang.Object,java.lang.Object)compareAndExchangeAcquire(int,org.apache.flume.channel.file.LogFile.Writer,org.apache.flume.channel.file.LogFile.Writer)compareAndExchangecompareAndExchange(int,java.lang.Object,java.lang.Object)compareAndExchange(int,org.apache.flume.channel.file.LogFile.Writer,org.apache.flume.channel.file.LogFile.Writer)setReleasesetRelease(int,java.lang.Object)setRelease(int,org.apache.flume.channel.file.LogFile.Writer)getAcquiregetAcquire(int)setOpaquesetOpaque(int,java.lang.Object)setOpaque(int,org.apache.flume.channel.file.LogFile.Writer)getOpaquegetOpaque(int)setPlainsetPlain(int,java.lang.Object)setPlain(int,org.apache.flume.channel.file.LogFile.Writer)getPlaingetPlain(int)BinaryOperator<E>/modules/java.base/java/util/function/BinaryOperator.classBiFunction<E,E,E>accumulateAndGetaccumulateAndGet(int,java.lang.Object,java.util.function.BinaryOperator)BinaryOperator<Writer>BiFunction<Writer,Writer,Writer>accumulateAndGet(int,org.apache.flume.channel.file.LogFile.Writer,java.util.function.BinaryOperator)getAndAccumulategetAndAccumulate(int,java.lang.Object,java.util.function.BinaryOperator)getAndAccumulate(int,org.apache.flume.channel.file.LogFile.Writer,java.util.function.BinaryOperator)updateAndGetupdateAndGet(int,java.util.function.UnaryOperator)UnaryOperator<Writer>Function<Writer,Writer>getAndUpdategetAndUpdate(int,java.util.function.UnaryOperator)weakCompareAndSetPlainweakCompareAndSetPlain(int,java.lang.Object,java.lang.Object)weakCompareAndSetPlain(int,org.apache.flume.channel.file.LogFile.Writer,org.apache.flume.channel.file.LogFile.Writer)weakCompareAndSetweakCompareAndSet(int,java.lang.Object,java.lang.Object)weakCompareAndSet(int,org.apache.flume.channel.file.LogFile.Writer,org.apache.flume.channel.file.LogFile.Writer)compareAndSetcompareAndSet(int,java.lang.Object,java.lang.Object)compareAndSet(int,org.apache.flume.channel.file.LogFile.Writer,org.apache.flume.channel.file.LogFile.Writer)getAndSetgetAndSet(int,java.lang.Object)getAndSet(int,org.apache.flume.channel.file.LogFile.Writer)lazySetlazySet(int,java.lang.Object)lazySet(int,org.apache.flume.channel.file.LogFile.Writer)set(int,org.apache.flume.channel.file.LogFile.Writer)length()AtomicReferenceArrayAtomicReferenceArray(java.lang.Object[])Writer[]AtomicReferenceArray<Writer>(org.apache.flume.channel.file.LogFile.Writer[])AtomicReferenceArray(int)"Log-BackgroundWorker-"Log-BackgroundWorker-"Cannot replay after Log has been opened"Cannot replay after Log has been openeddataFilesshouldFastReplay"Replay started"Replay started"Found NextFileID "Found NextFileID ", from ", from "Disabling fast full replay because checkpoint " +
              "exists: "Disabling fast full replay because checkpoint exists: "Not disabling fast full replay because checkpoint " +
              " does not exist: "Not disabling fast full replay because checkpoint  does not exist: "Last Checkpoint "Last Checkpoint ", queue depth = ", queue depth = "Checkpoint may not have completed successfully. "
              + "Restoring checkpoint and starting up."Checkpoint may not have completed successfully. Restoring checkpoint and starting up."Checkpoint may not have completed successfully. "
              + "Forcing full replay, this may take a while."Checkpoint may not have completed successfully. Forcing full replay, this may take a while."Could not delete files in checkpoint " +
                "directory to recover from a corrupt or incomplete checkpoint"Could not delete files in checkpoint directory to recover from a corrupt or incomplete checkpoint"Rolling "Rolling "Failed to initialize Log on "Failed to initialize Log on "Fast replay successful."Fast replay successful.replayHandler"Replaying logs with v1 replay logic"Replaying logs with v1 replay logic"Replaying logs with v2 replay logic"Replaying logs with v2 replay logic"Log is closed"Log is closed"LogFile is null for id "LogFile is null for id "Corrupt event found. Please run File Channel " +
            "Integrity tool."Corrupt event found. Please run File Channel Integrity tool.flumeEventlogFileIndexusableSpacerequiredSpace"Usable space exhausted, only "Usable space exhausted, only " bytes remaining, required " bytes remaining, required " bytes" bytes"Rolling back "Rolling back err"Failed creating checkpoint on close of channel "Failed creating checkpoint on close of channel "Replay will take longer next time channel is started."Replay will take longer next time channel is started.writerlogId"Error unlocking "Error unlocking "Attempting to shutdown background worker."Attempting to shutdown background worker."Interrupted while waiting for worker to die."Interrupted while waiting for worker to die.logFileWriteroldLogFile"Roll start "Roll start "Roll end"Roll endcheckpointCompletedlogFileRefCountsAlllogFileRefCountsActivenumFilesidIteratorlogWriter"Updated checkpoint for file: "Updated checkpoint for file: " position: " position: " logWriteOrderID: " logWriteOrderID: "logWriteOrderID "logWriteOrderID "Could not update all data file timestamps: "Could not update all data file timestamps: minFileIDfileToDelete"Removing old file: "Removing old file: "Files currently in use: "Files currently in use: logsdirsecondLock"Cannot lock "Cannot lock ". The directory is already locked. ". The directory is already locked. "Directory "Directory " does not support locking" does not support lockingput(java.lang.String,java.nio.channels.FileLock)? super FileLock? extends FileLockBiFunction<? super FileLock,? super FileLock,? extends FileLock>merge(java.lang.String,java.nio.channels.FileLock,java.util.function.BiFunction)BiFunction<? super String,? super FileLock,? extends FileLock>Function<? super String,? extends FileLock>replace(java.lang.String,java.nio.channels.FileLock)replace(java.lang.String,java.nio.channels.FileLock,java.nio.channels.FileLock)putIfAbsent(java.lang.String,java.nio.channels.FileLock)BiConsumer<? super String,? super FileLock>getOrDefault(java.lang.Object,java.nio.channels.FileLock)Entry<String,FileLock>Set<Entry<String,FileLock>>Collection<Entry<String,FileLock>>Iterable<Entry<String,FileLock>>Collection<FileLock>Iterable<FileLock>Map<? extends String,? extends FileLock>lockF"rws"rwsresoe"Cannot create lock on "Cannot create lock on /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/Log$BackgroundWorker.classClass<BackgroundWorker>"Error doing checkpoint"Error doing checkpoint"General error in checkpoint worker"General error in checkpoint workerStores FlumeEvents on disk and pointers to the events in a in memory queue.Once a log object is created the replay method should be called to reconcilethe on disk write ahead log with the last checkpoint of the queue.Before calling any of commitPut/commitTake/get/put/rollback/take{@linkplain org.apache.flume.channel.file.Log#lockShared()}should be called. Afterthe operation and any additional modifications of theFlumeEventQueue, the Log.unlockShared method should be called. for readerSet of files that should be excluded from backup and restores.Shared lockExclusive lockRead checkpoint and data files from disk replaying them to the statedirectly before the shutdown or crash.First we are going to look through the data directoriesand find all log files. We will store the highest file id(at the end of the filename) we find and use that when wecreate additional log files.Also store up the list of files so we can replay them later.sort the data files by file id so we can replay them by file idwhich should approximately give us sequential eventsRead the checkpoint (in memory queue) from one of two alternatinglocations. We will read the last one written to disk.We now have everything we need to actually replay the log filesthe queue, the timestamp the queue was written to disk, andthe list of data files.This will throw if and only if checkpoint file was fine,but the inflights were not. If the checkpoint was bad, the backingstore factory would have thrown. If the checkpoint was deleted due to BadCheckpointException, then trigger fast replay if the channel is configured to.Now that we have replayed, write the current queue to diskWas a checkpoint backup used to replay?true if a checkpoint backup was used to replay.Return the FlumeEvent for an event pointer. This method isnon-transactional. It is assumed the client has obtained thisFlumeEventPointer via FlumeEventQueue.InterruptedExceptionLog a put of an eventSynchronization not required as this method is atomicLog a take of an event, pointer points at the corresponding putLog a rollback of a transactionLog commit of put, we need to know which type of commitso we know if the pointers corresponding to the eventsshould be added or removed from the flume queue. Wecould infer but it's best to be explicit.Log commit of take, we need to know which type of commitSynchronization not required since this method gets the write lock,so checkpoint and this method cannot run at the same time. do this before acquiring exclusive lock If multiple transactions are committing at the same time, this ensures that the number of actual fsyncs is small and a number of them are grouped together into one.Atomic so not synchronization required.Unconditionally rollSynchronization done internallyRoll a log if needed. Roll always occurs if the log at the indexdoes not exist (typically on startup), or buffer is null. OtherwiseLogFile.Writer.isRollRequired is checked again to ensure we don'thave threads pile up on this log resulting in multiple successiverollsSynchronization required since both synchronized and unsynchronizedmethods call this method, and this method acquires only aread lock. The synchronization guarantees that multiple threads don'troll at the same time. check to make sure a roll is actually required due to the possibility of multiple writes waiting on lock writer from this point on will get new reference close out old logWrite the current checkpoint object and then swap objects so thatthe next checkpoint occurs on the other checkpoint directory.Synchronization is not required because this method acquires awrite lock. So this method gets exclusive access to all thedata structures this method accesses.a flag to force the writing of checkpointif we are unable to write the checkpoint out to diskSince the active files might also be in the queue's fileIDs,we need to either move each one to a new set or remove each oneas we do here. Otherwise we cannot make sure every element infileID set from the queue have been updated.Since clone is smarter than insert, better to makea copy of the set first so that we can use it later. Update any inactive data files as wellAdd files from all log directoriesDo the deletes outside the checkpointWriterLockDelete logic is expensive.Since the exception is not caught, this will not be returned ifan exception is thrown from the try. To maintain a single code path for deletes, if backup of checkpoint is enabled or not, we will track the files which can be deleted after the current checkpoint (since the one which just got backed up still needs these files) and delete them only after the next (since the current checkpoint will become the backup at that time, and thus these files are no longer needed). we will find the smallest fileID currently in use and won't delete any files with an id larger than the min sort oldset to newest ensure we always keep two logs per dirLock storage to provide exclusive access.<p> Locking is not supported by all file systems.E.g., NFS does not consistently support exclusive locks.<p> If locking is supported we guarantee exclusive access to thestorage directory. Otherwise, no guarantee is given.if locking failsAttempts to acquire an exclusive lock on the directory.A lock object representing the newly-acquired lock or<code>null</code> if directory is already locked.if locking fails.Unlock directory./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/LogFile.javaFILL/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFile.classClass<LogFile>fileHandle/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFile$MetaDataWriter.classgetVersiongetVersion()markCheckpointmarkCheckpoint(long,long)markCheckpoint(long)lastCheckpointWriteOrderIDlastCheckpointOffsetwriteFileHandlecurrentPositiongetUsableSpacegetUsableSpace()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFile$CachedFSUsableSpace.classdecrementdecrement(long)CachedFSUsableSpaceCachedFSUsableSpace(java.io.File,long)lastRefreshfsnumBytes"numBytes less than zero"numBytes less than zero/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFile$Writer.classsyncsync()isRollRequiredisRollRequired(java.nio.ByteBuffer)writewrite(java.nio.ByteBuffer)commit(java.nio.ByteBuffer)rollback(java.nio.ByteBuffer)take(java.nio.ByteBuffer)put(java.nio.ByteBuffer)positionposition()getSyncCountgetSyncCount()getLastSyncPositiongetLastSyncPosition()getLastCommitPositiongetLastCommitPosition()getMaxSizegetMaxSize()getParentgetParent()getFilegetFile()getLogFileIDgetLogFileID()WriterWriter(java.io.File,int,long,org.apache.flume.channel.file.encryption.CipherProvider.Encryptor,long,boolean,int)syncCountdirtysyncExecutorlastSyncPositionlastCommitPositionencryptorwriteFileChannel"Sync interval = "Sync interval = /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFile$Writer$1.class"Data file, "Data file, " could not " +
                  "be synced to disk due to an error." could not be synced to disk due to an error."Opened "Opened pairexpectedLengthrecordLengthtoWritewrote"File closed "File closed " > " > "No events written to file, "No events written to file, " in last " in last " or since last commit." or since last commit."Closing "Closing "Unable to flush to disk "Unable to flush to disk "Preallocating at position "Preallocating at position /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFile$OperationRecordUpdater.class"File to update, "File to update, " does not exist." does not exist.byteRead"Expected to read a record but the byte read indicates EOF"Expected to read a record but the byte read indicates EOF"Marking event as "Marking event as " at " at " for file " for file "Could not close file handle to file "Could not close file handle to file close(java.io.RandomAccessFile,java.io.File)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFile$RandomReader.classcheckOutcheckOut()checkIncheckIn(java.io.RandomAccessFile)open()BlockingQueue<RandomAccessFile>Queue<RandomAccessFile>Collection<RandomAccessFile>Iterable<RandomAccessFile>readFileHandlesArrayBlockingQueue<RandomAccessFile>/modules/java.base/java/util/concurrent/ArrayBlockingQueue.classAbstractQueue<RandomAccessFile>AbstractCollection<RandomAccessFile>ArrayBlockingQueue<RandomAccessFile>(int,boolean)Spliterator<RandomAccessFile>? super RandomAccessFileConsumer<? super RandomAccessFile>Iterator<RandomAccessFile>Stream<RandomAccessFile>BaseStream<RandomAccessFile,Stream<RandomAccessFile>>Predicate<? super RandomAccessFile>? extends RandomAccessFileCollection<? extends RandomAccessFile>Iterable<? extends RandomAccessFile>add(java.io.RandomAccessFile)AbstractCollection<RandomAccessFile>()offer(java.io.RandomAccessFile)AbstractQueue<RandomAccessFile>()Collection<? super RandomAccessFile>Iterable<? super RandomAccessFile>offer(java.io.RandomAccessFile,long,java.util.concurrent.TimeUnit)put(java.io.RandomAccessFile)ArrayBlockingQueueArrayBlockingQueue(int,boolean,java.util.Collection)ArrayBlockingQueue<RandomAccessFile>(int,boolean,java.util.Collection)ArrayBlockingQueue(int,boolean)ArrayBlockingQueue(int)ArrayBlockingQueue<RandomAccessFile>(int)removeAtremoveAt(int)itemAtitemAt(java.lang.Object[],int)itemAt(int)decdec(int,int)incinc(int,int)itrsputIndextakeIndexitems50"File closed"File closedoperation"No op record found. Corrupt record " +
              "may have been repaired by File Channel Integrity tool"No op record found. Corrupt record may have been repaired by File Channel Integrity tool"Operation code is invalid. File " +
                  "is corrupt. Please run File Channel Integrity tool."Operation code is invalid. File is corrupt. Please run File Channel Integrity tool."Record is "Record is ? extends TransactionEventRecordClass<? extends TransactionEventRecord>Map<String,? extends TransactionEventRecord>TransactionEventRecord[]Constructor<? extends TransactionEventRecord>TypeVariable<Class<? extends TransactionEventRecord>>TypeVariable<Class<? extends TransactionEventRecord>>[]List<RandomAccessFile>SequencedCollection<RandomAccessFile>fileHandlesArrayList<RandomAccessFile>AbstractList<RandomAccessFile>"Closing RandomReader "Closing RandomReader "Unable to close fileHandle for "Unable to close fileHandle for addLast(java.io.RandomAccessFile)addFirst(java.io.RandomAccessFile)ListIterator<RandomAccessFile>add(int,java.io.RandomAccessFile)set(int,java.io.RandomAccessFile)Comparator<? super RandomAccessFile>UnaryOperator<RandomAccessFile>Function<RandomAccessFile,RandomAccessFile>5Lremaining"Opening "Opening " for read, remaining number of file " +
            "handles available for reads of this file is " for read, remaining number of file handles available for reads of this file is skipToLastCheckpointPositionskipToLastCheckpointPosition(long)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFile$SequentialReader.classdoNextdoNext(int)SequentialReaderSequentialReader(java.io.File,org.apache.flume.channel.file.encryption.KeyProvider)backupCheckpointWriteOrderIDbackupCheckpointPositionlastCheckpointPosition"LogFileID is not positive: "LogFileID is not positive: checkpointWriteOrderID"fast-forward to checkpoint position: "fast-forward to checkpoint position: "Checkpoint for file("Checkpoint for file(") "
            + "is: ") is: ", which is beyond the "
            + "requested checkpoint time: ", which is beyond the requested checkpoint time: " and position " and position "File position exceeds the threshold: "File position exceeds the threshold: ", position: ", position: "Encountered EOF at "Encountered EOF at "No op event found in file: "No op event found in file: ". Skipping event.". Skipping event."Encountered non op-record at "Encountered non op-record at "Unable to read next Transaction from log file "Unable to read next Transaction from log file " at offset " at offset output"Length of event is: "Length of event is: ". Event must have length >= 0. Possible corruption of data or partial fsync.". Event must have length >= 0. Possible corruption of data or partial fsync."Remaining data in file less than " +
                                      "expected size of event."Remaining data in file less than expected size of event.commitCountts"Unknown record type: "Unknown record type: ", ", "Replayed "Replayed " from " from " read: " read: ", put: ", put: ", take: ", take: ", rollback: ", rollback: ", commit: ", commit: "Hit EOF on "Hit EOF on This class preallocates the data files 1MB at time to avoidthe updating of the inode on each write and to avoid the diskfilling up during a write. It's also faster, so there. To ensure we can count the number of fsyncs. encrypt and write methods may not be thread safe in the following methods, so all methods need to be synchronized. OP_RECORD + size + bufferSync the underlying log file to disk. Expensive call,should be used only on commits. If a sync has already happened afterthe last commit, this method is a no-opLogFileRetryableIOException- if this log file is closed. Shutdown the executor before attempting to close. No need to wait for it to shutdown.This is an class meant to be an internal Flume API,and can change at any time. Intended to be used only from File Channel  Integritytest tool. Not to be used for any other purpose. First ensure that the offset actually is an OP_RECORD. There is a small possibility that it still is OP_RECORD, but is not actually the beginning of a record. Is there anything we can do about it? this is uninterruptableConstruct a Sequential Log Reader objectif an I/O error occursEOFExceptionif the file is empty for puts the fileId is the fileID of the file they exist in for takes the fileId and offset are pointers to a put/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/LogFileFactory.javagetSequentialReadergetSequentialReader(java.io.File,org.apache.flume.channel.file.encryption.KeyProvider,boolean)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFileFactory.classgetRandomReadergetRandomReader(java.io.File,org.apache.flume.channel.file.encryption.KeyProvider,boolean)getWritergetWriter(java.io.File,int,long,java.security.Key,java.lang.String,java.lang.String,long,boolean,int)getMetaDataWritergetMetaDataWriter(java.io.File,int)LogFileFactoryLogFileFactory()Class<LogFileFactory>" has bad version " has bad version "File already exists "File already exists "File could not be created "File could not be created oldMetadataFiletempMetadataFilehasMeta"Renaming of "Renaming of " failed" failed"MetaData file %s is empty, but log %s" +
                " is of size %d"MetaData file %s is empty, but log %s is of size %d"MetaData file %s is empty"MetaData file %s is empty either this is a rr for a just created file or the metadata file exists and as such it's V3 FLUME-1699: If the platform does not support atomic rename, then we renamed log.meta -> log.meta.old followed by log.meta.tmp -> log.meta I am not sure if all platforms maintain file integrity during file metadata update operations. So: 1. check if meta file exists 2. If 1 returns false, check if temp exists 3. if 2 is also false (maybe the machine died during temp->meta,    then check if old exists. In the above, we assume that if a file exists, it's integrity is ok. Now the metadata file has been found, delete old or temp files so it does not interfere with normal operation./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/LogFileRetryableIOException.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFileRetryableIOException.class2747112999806160431L2747112999806160431/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/LogFileV2.javaLogFileV2LogFileV2()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFileV2.classOFFSET_CHECKPOINTClass<LogFileV2>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFileV2$MetaDataWriter.classfid"The version of log file: "The version of log file: " is different from expected "
              + " version: expected = " is different from expected  version: expected = ", found = ", found = "The file id of log file: "The file id of log file: " is different from expected "
              + " id: expected = " is different from expected  id: expected = "File: "File: " was last checkpointed "
            + "at position: " was last checkpointed at position: ", logWriteOrderID: ", logWriteOrderID: "Noted checkpoint for file: "Noted checkpoint for file: ", id: ", id: ", checkpoint position: ", checkpoint position: /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFileV2$Writer.classWriter(java.io.File,int,long,long)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFileV2$RandomReader.classRandomReaderRandomReader(java.io.File)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFileV2$SequentialReader.classSequentialReader(java.io.File)"Version is "Version is " expected " expected Represents a single data file on disk. Has methods to write,read sequentially (replay), and read randomly (channel takes). checkpoint marker timestamp placeholder/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/LogFileV3.javaLogFileV3LogFileV3()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFileV3.classClass<LogFileV3>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFileV3$MetaDataWriter.classlogFileMetaDatametaDataReadermetaDataBuilderreadread()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFileV3$MetaDataReader.classmetaData"Metadata cannot be null"Metadata cannot be nulltmpclosedoldFile"Unable to rename "Unable to rename " over " over /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFileV3$Writer.classWriter(java.io.File,int,long,java.security.Key,java.lang.String,java.lang.String,long,boolean,int)logFileEncryptionBuilder"encryptionKeyAlias""encryptionCipherProvider"/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFileV3$RandomReader.classgetDecryptorgetDecryptor()initializeinitialize()RandomReader(java.io.File,org.apache.flume.channel.file.encryption.KeyProvider,boolean)BlockingQueue<Decryptor>Queue<Decryptor>Collection<Decryptor>Iterable<Decryptor>decryptorsparameterscipherProviderencryptionEnabledinitializedLinkedBlockingDeque<Decryptor>AbstractQueue<Decryptor>AbstractCollection<Decryptor>BlockingDeque<Decryptor>Deque<Decryptor>SequencedCollection<Decryptor>LinkedBlockingDeque<Decryptor>()Spliterator<Decryptor>? super DecryptorConsumer<? super Decryptor>Iterator<Decryptor>Stream<Decryptor>BaseStream<Decryptor,Stream<Decryptor>>Predicate<? super Decryptor>? extends DecryptorCollection<? extends Decryptor>Iterable<? extends Decryptor>add(org.apache.flume.channel.file.encryption.CipherProvider.Decryptor)AbstractCollection<Decryptor>()offer(org.apache.flume.channel.file.encryption.CipherProvider.Decryptor)AbstractQueue<Decryptor>()Collection<? super Decryptor>Iterable<? super Decryptor>offer(org.apache.flume.channel.file.encryption.CipherProvider.Decryptor,long,java.util.concurrent.TimeUnit)put(org.apache.flume.channel.file.encryption.CipherProvider.Decryptor)addLast(org.apache.flume.channel.file.encryption.CipherProvider.Decryptor)addFirst(org.apache.flume.channel.file.encryption.CipherProvider.Decryptor)push(org.apache.flume.channel.file.encryption.CipherProvider.Decryptor)offerLast(org.apache.flume.channel.file.encryption.CipherProvider.Decryptor)offerFirst(org.apache.flume.channel.file.encryption.CipherProvider.Decryptor)offerLast(org.apache.flume.channel.file.encryption.CipherProvider.Decryptor,long,java.util.concurrent.TimeUnit)offerFirst(org.apache.flume.channel.file.encryption.CipherProvider.Decryptor,long,java.util.concurrent.TimeUnit)putLast(org.apache.flume.channel.file.encryption.CipherProvider.Decryptor)putFirst(org.apache.flume.channel.file.encryption.CipherProvider.Decryptor)Node<Decryptor>LinkedBlockingDeque<Decryptor>(java.util.Collection)LinkedBlockingDeque<Decryptor>(int)"MetaData cannot be null"MetaData cannot be nullencryption"Data file is encrypted but no " +
                " provider was specified"Data file is encrypted but no  provider was specifieddecryptor"Error decrypting event"Error decrypting event/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogFileV3$SequentialReader.class"Corrupt file found. File id: log-"Corrupt file found. File id: log-"Could not decrypt even read from channel. Skipping " +
              "event."Could not decrypt even read from channel. Skipping event.Set the previous checkpoint position and write order id so that itwould be possible to recover from a backup.Writes a GeneratedMessage to a temp file, synchronizes it to diskand then renames the file over file.GeneratedMessage to write to the filedestination fileif a write error occurs or the File.renameTomethod returns false meaning the file could not be overwritten.Some platforms don't support moving over an existing file.So:log.meta -> log.meta.oldlog.meta.tmp -> log.metadelete log.meta.old readers are opened right when the file is created and thus empty. As such we wait to initialize until there is some data before we we initialize Return null so that replay handler thinks all events in this file have been taken./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/LogRecord.javashort[]replaySortOrder/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogRecord.classComparable<LogRecord>thisIndexthatIndex oops we have hit a flume-1.2 bug. let's try and use the txid to replay the events events are within the same transaction. Basically we want commit and rollback to come after take and put/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/LogUtils.javagetLogsgetLogs(java.io.File)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogUtils.classgetIDForFilegetIDForFile(java.io.File)sort(java.util.List)"^"^"\\d+$"\d+$/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/LogUtils$1.classComparator<File>ToDoubleFunction<>/modules/java.base/java/util/function/ToDoubleFunction.classComparator<T>comparingDoublecomparingDouble(java.util.function.ToDoubleFunction)ToDoubleFunction<? super T>ToLongFunction<>/modules/java.base/java/util/function/ToLongFunction.classcomparingLongcomparingLong(java.util.function.ToLongFunction)ToLongFunction<? super T>ToIntFunction<>/modules/java.base/java/util/function/ToIntFunction.classcomparingIntcomparingInt(java.util.function.ToIntFunction)ToIntFunction<? super T>Function<>? super UComparable<? super U>comparingcomparing(java.util.function.Function)Function<? super T,? extends U>comparing(java.util.function.Function,java.util.Comparator)Comparator<? super U>nullsLastnullsLast(java.util.Comparator)Comparator<? super T>nullsFirstnullsFirst(java.util.Comparator)Comparable<? super T>naturalOrdernaturalOrder()reverseOrderreverseOrder()thenComparingDoublethenComparingDouble(java.util.function.ToDoubleFunction)ToDoubleFunction<? super File>thenComparingLongthenComparingLong(java.util.function.ToLongFunction)ToLongFunction<? super File>thenComparingIntthenComparingInt(java.util.function.ToIntFunction)ToIntFunction<? super File>thenComparingthenComparing(java.util.function.Function)Function<? super File,? extends U>thenComparing(java.util.function.Function,java.util.Comparator)thenComparing(java.util.Comparator)comparecompare(java.lang.Object,java.lang.Object)compare(java.io.File,java.io.File)file1file2id1id2".listFiles() returned null: ".listFiles() returned null: "File = "File = "Exists = "Exists = "Writable = "Writable = Sort a list of files by the number after Log.PREFIX.Get the id after the Log.PREFIXFind all log files within a directorydirectory to searchList of data files within logDir/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/NoopRecordException.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/NoopRecordException.class7394180633208889738L7394180633208889738/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/Pair.javarightleftPair<L,R>(java.lang.Object,java.lang.Object)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/Put.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/Put.classgetEventgetEvent()PutPut(java.lang.Long,java.lang.Long,org.apache.flume.channel.file.FlumeEvent)Put(java.lang.Long,java.lang.Long)putBuildereventBuilderheaderBuilderprotoEvent"Put cannot be null"Put cannot be nullprotosEventeventBodyheaderList<FlumeEventHeader>SequencedCollection<FlumeEventHeader>Collection<FlumeEventHeader>Iterable<FlumeEventHeader>eventBodyChecksum"Expected checksum for event was "Expected checksum for event was " but the checksum of the event is " but the checksum of the event is "Put [event="Put [event=Represents a Put on disk Should we move this to a higher level to not make multiple instances? Doing that might cause performance issues, since access to this would need to be synchronized (the whole reset-update-getValue cycle would need to be). TODO when we remove v2, remove FlumeEvent and use EventBuilder here/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/ReplayHandler.javaprocessCommitprocessCommit(short,java.util.Collection)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/ReplayHandler.classreplayLogreplayLog(java.util.List)replayLogv1replayLogv1(java.util.List)ReplayHandlerReplayHandler(org.apache.flume.channel.file.FlumeEventQueue,org.apache.flume.channel.file.encryption.KeyProvider,boolean)skipCountPriorityQueue<LogRecord>/modules/java.base/java/util/PriorityQueue.classAbstractQueue<LogRecord>AbstractCollection<LogRecord>Collection<LogRecord>Iterable<LogRecord>Queue<LogRecord>logRecordBufferMap<Integer,SequentialReader>readerslastCheckpointClass<ReplayHandler>HashMap<Integer,SequentialReader>AbstractMap<Integer,SequentialReader>PriorityQueue<LogRecord>()Spliterator<LogRecord>? super LogRecordConsumer<? super LogRecord>Iterator<LogRecord>Stream<LogRecord>BaseStream<LogRecord,Stream<LogRecord>>Predicate<? super LogRecord>? extends LogRecordCollection<? extends LogRecord>Iterable<? extends LogRecord>add(org.apache.flume.channel.file.LogRecord)AbstractCollection<LogRecord>()offer(org.apache.flume.channel.file.LogRecord)AbstractQueue<LogRecord>()Comparator<? super LogRecord>removeEqremoveEq(java.lang.Object)SortedSet<? extends E>Set<? extends E>SequencedSet<? extends E>SequencedCollection<? extends E>PriorityQueuePriorityQueue(java.util.SortedSet)SortedSet<? extends LogRecord>Set<? extends LogRecord>SequencedSet<? extends LogRecord>SequencedCollection<? extends LogRecord>PriorityQueue<LogRecord>(java.util.SortedSet)PriorityQueue<? extends E>AbstractQueue<? extends E>AbstractCollection<? extends E>Queue<? extends E>PriorityQueue(java.util.PriorityQueue)PriorityQueue<? extends LogRecord>AbstractQueue<? extends LogRecord>AbstractCollection<? extends LogRecord>Queue<? extends LogRecord>PriorityQueue<LogRecord>(java.util.PriorityQueue)PriorityQueue(java.util.Collection)PriorityQueue<LogRecord>(java.util.Collection)PriorityQueue(int,java.util.Comparator)PriorityQueue<LogRecord>(int,java.util.Comparator)PriorityQueue(java.util.Comparator)PriorityQueue<LogRecord>(java.util.Comparator)PriorityQueue(int)PriorityQueue<LogRecord>(int)PriorityQueue()totaltransactionMappendingTakesSizeeventPointerseventPointer"Starting replay of "Starting replay of "Replaying "Replaying "unchecked"uncheckedHashSet<FlumeEventPointer>AbstractSet<FlumeEventPointer>Set<FlumeEventPointer>"read: "read: ", skipp: ", skipp: inflightTxnIdinflightUncommittedTakesinflightUncommittedTake"Pending takes "Pending takes " exist after the end of replay" exist after the end of replay"Pending take "Pending take ". Duplicate messages will exist in destination.". Duplicate messages will exist in destination.logRecordBiFunction<? super SequentialReader,? super SequentialReader,? extends SequentialReader>merge(java.lang.Integer,org.apache.flume.channel.file.LogFile.SequentialReader,java.util.function.BiFunction)BiFunction<? super Integer,? super SequentialReader,? extends SequentialReader>Function<? super Integer,? extends SequentialReader>replace(java.lang.Integer,org.apache.flume.channel.file.LogFile.SequentialReader)replace(java.lang.Integer,org.apache.flume.channel.file.LogFile.SequentialReader,org.apache.flume.channel.file.LogFile.SequentialReader)putIfAbsent(java.lang.Integer,org.apache.flume.channel.file.LogFile.SequentialReader)BiConsumer<? super Integer,? super SequentialReader>getOrDefault(java.lang.Object,org.apache.flume.channel.file.LogFile.SequentialReader)Entry<Integer,SequentialReader>Set<Entry<Integer,SequentialReader>>Collection<Entry<Integer,SequentialReader>>Iterable<Entry<Integer,SequentialReader>>Map<? extends Integer,? extends SequentialReader>put(java.lang.Integer,org.apache.flume.channel.file.LogFile.SequentialReader)"Readers "Readers " already contains " already contains ", skip: ", skip: ", eventCount:", eventCount:" exist after the" +
          " end of replay. Duplicate messages will exist in" +
          " destination." exist after the end of replay. Duplicate messages will exist in destination.resultLogRecordnextLogRecord"Unable to add "Unable to add ". Queue depth = ". Queue depth = ", Capacity = ", Capacity = "Take was pending and pointer was successfully added to the"
                  + " queue but could not be removed: "Take was pending and pointer was successfully added to the queue but could not be removed: removedProcesses a set of data logs, replaying said logs into the queue.This data structure stores takes for which we found a commit in the logfiles before we found a commit for the put. This can happen if the channelis configured for multiple directories.Consider the following:logdir1, logdir2Put goes to logdir2 Commit of Put goes to logdir2 Take goes to logdir1Commit of Take goes to logdir1When replaying we will start with log1 and find the take and commit beforefinding the put and commit in logdir2.Replay logic from Flume1.2 which can be activated if the v2 logicis failing on ol logs for some reason.Read inflight puts to see if they were committedre-insert the events in the take map,since the takes were not committed.Replay logs in order records were written seed both with the highest known sequence of either the tnxid or woidLoad the inflight puts into the transaction map to see if they werecommitted in one of the logs. there is more log records to read/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/Rollback.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/Rollback.classRollbackRollback(java.lang.Long,java.lang.Long)rollbackBuilder"unused"unused"Rollback cannot be null"Rollback cannot be null"Rollback [getLogWriteOrderID()="Rollback [getLogWriteOrderID()=Represents a Rollback on disk/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/Serialization.javadeleteAllFilesdeleteAllFiles(java.io.File,java.util.Set)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/Serialization.classgetOldMetaDataFilegetOldMetaDataFile(java.io.File)getMetaDataFile(java.io.File)getMetaDataTempFilegetMetaDataTempFile(java.io.File)FILE_BUFFER_SIZEVERSION_3VERSION_2SIZE_OF_LONGSIZE_OF_INTSerializationSerialization()".meta".meta".tmp".tmp".old".oldClass<Serialization>metaDataFileNameoldMetaDataFileNameexcludes"Deleted the following files: "Deleted the following files: "Skipping "Skipping " because it is in excludes " +
            "set" because it is in excludes set"Error while attempting to delete: "Error while attempting to delete: to"Source file is null, file copy failed."Source file is null, file copy failed."Destination file is null, " +
        "file copy failed."Destination file is null, file copy failed."Source file: "Source file: "Destination file: "Destination file: " unexpectedly exists." unexpectedly exists.buf"The size of the origin file and destination file are not equal."The size of the origin file and destination file are not equal."Error while attempting to copy "Error while attempting to copy "Error while closing input file."Error while closing input file."Error while closing output file."Error while closing output file."Copying file: "Copying file: " to: " to: " may have failed." may have failed.uncompressedcompressedsnappyOut"Source file is null, compression failed."Source file is null, compression failed."Destination file is null, compression failed."Destination file is null, compression failed."Compressed file: "Compressed file: " unexpectedly " + "exists.""Error while attempting to compress "Error while attempting to compress decompressedsnappyIn"Source file is null, decompression failed."Source file is null, decompression failed."Destination file is " +
        "null, decompression failed."Destination file is null, decompression failed."Decompressed file: "Decompressed file: "Decompressing file: "Decompressing file:  64 K buffer to copy and compress files. Support platforms that cannot do atomic renames - FLUME-1699Deletes all files in given directory.- The directory whose files are to be deleted- Names of files which should not be deleted from thisdirectory.- true if all files were successfully deleted, false otherwise.Copy a file using a 64K size buffer. This method will copy the file andthen fsync to diskFile to copy - this file should existDestination file - this file should not existtrue if the copy was successfuluse a RandomAccessFile for easy fsync Should never reach here.Compress file using SnappyFile to compress - this file should existCompressed file - this file should not existtrue if compression was successfulDecompress file using Snappytrue if decompression was successful/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/Take.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/Take.classTakeTake(java.lang.Long,java.lang.Long,int,int)Take(java.lang.Long,java.lang.Long)takeBuilder"Take cannot be null"Take cannot be null"Take [offset="Take [offset=", fileID=", fileID=Represents a Take on disk/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/TransactionEventRecord.javanewRecordForTypenewRecordForType(short,long,long)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/TransactionEventRecord.classgetName(short)fromByteArrayfromByteArray(byte[])toByteBuffertoByteBuffer(org.apache.flume.channel.file.TransactionEventRecord)fromDataInputV2fromDataInputV2(java.io.DataInput)toByteBufferV2toByteBufferV2(org.apache.flume.channel.file.TransactionEventRecord)ImmutableMap<Short,Constructor<? extends TransactionEventRecord>>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/ImmutableMap.classMap<Short,Constructor<? extends TransactionEventRecord>>"Use ImmutableMap.of or another implementation"Use ImmutableMap.of or another implementationTYPESMAGIC_HEADERgetTransactionIDgetTransactionID()Class<TransactionEventRecord>0xdeadbeef-559038737Builder<Short,Constructor<? extends TransactionEventRecord>>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/ImmutableMap$Builder.class"Create a real instance instead"Create a real instance insteadImmutableMap<>Builder<>put(java.lang.Short,java.lang.reflect.Constructor)ImmutableMap<K,V>buildJdkBackedbuildJdkBacked()combinecombine(com.google.common.collect.ImmutableMap.Builder)Comparator<? super V>orderEntriesByValueorderEntriesByValue(java.util.Comparator)? super Constructor<? extends TransactionEventRecord>Comparator<? super Constructor<? extends TransactionEventRecord>>? extends Entry<? extends K,? extends V>Iterable<? extends Entry<? extends K,? extends V>>putAll(java.lang.Iterable)? extends Short? extends Constructor<? extends TransactionEventRecord>Entry<? extends Short,? extends Constructor<? extends TransactionEventRecord>>? extends Entry<? extends Short,? extends Constructor<? extends TransactionEventRecord>>Iterable<? extends Entry<? extends Short,? extends Constructor<? extends TransactionEventRecord>>>Map<? extends Short,? extends Constructor<? extends TransactionEventRecord>>put(java.util.Map.Entry)Builder(int)Builder<Short,Constructor<? extends TransactionEventRecord>>(int)Builder<Short,Constructor<? extends TransactionEventRecord>>()entriesUsedEntry<K,V>[]valueComparatorConstructor<Put>Class<Put>Map<String,Put>Put[]? super PutClass<? super Put>TypeVariable<Class<Put>>TypeVariable<Class<Put>>[]Class<Long>Constructor<Take>Class<Take>Map<String,Take>Take[]? super TakeClass<? super Take>TypeVariable<Class<Take>>TypeVariable<Class<Take>>[]Constructor<Rollback>Class<Rollback>Map<String,Rollback>Rollback[]? super RollbackClass<? super Rollback>TypeVariable<Class<Rollback>>TypeVariable<Class<Rollback>>[]Constructor<Commit>Class<Commit>Map<String,Commit>Commit[]? super CommitClass<? super Commit>TypeVariable<Class<Commit>>TypeVariable<Class<Commit>>[]TypeType(short)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/TransactionEventRecord$Type.classbyteOutput512dataOutput"Error closing byte array output stream"Error closing byte array output stream"Header "Header " is not the required value: " is not the required value: footer"Header cannot be null"Header cannot be nulltransactionEvent"Footer cannot be null"Footer cannot be null"Could not parse event from data file."Could not parse event from data file."Error closing byte array input stream"Error closing byte array input streamconstructorBiFunction<? super Constructor<? extends TransactionEventRecord>,? super Constructor<? extends TransactionEventRecord>,? extends Constructor<? extends TransactionEventRecord>>merge(java.lang.Short,java.lang.reflect.Constructor,java.util.function.BiFunction)? super ShortBiFunction<? super Short,? super Constructor<? extends TransactionEventRecord>,? extends Constructor<? extends TransactionEventRecord>>compute(java.lang.Short,java.util.function.BiFunction)computeIfPresent(java.lang.Short,java.util.function.BiFunction)Function<? super Short,? extends Constructor<? extends TransactionEventRecord>>computeIfAbsent(java.lang.Short,java.util.function.Function)replace(java.lang.Short,java.lang.reflect.Constructor)replace(java.lang.Short,java.lang.reflect.Constructor,java.lang.reflect.Constructor)putIfAbsent(java.lang.Short,java.lang.reflect.Constructor)BiConsumer<? super Short,? super Constructor<? extends TransactionEventRecord>>getOrDefault(java.lang.Object,java.lang.reflect.Constructor)Entry<Short,Constructor<? extends TransactionEventRecord>>Set<Entry<Short,Constructor<? extends TransactionEventRecord>>>Collection<Entry<Short,Constructor<? extends TransactionEventRecord>>>Iterable<Entry<Short,Constructor<? extends TransactionEventRecord>>>Collection<Constructor<? extends TransactionEventRecord>>Iterable<Constructor<? extends TransactionEventRecord>>Set<Short>Collection<Short>Iterable<Short>writeReplacewriteReplace()isHashCodeFastisHashCodeFast()isPartialViewisPartialView()ImmutableSetMultimap<>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/ImmutableSetMultimap.classImmutableMultimap<>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/ImmutableMultimap.classBaseImmutableMultimap<>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/BaseImmutableMultimap.classImmutableSetMultimap<K,V>ImmutableMultimap<K,V>BaseImmutableMultimap<K,V>AbstractMultimap<K,V>Multimap<K,V>SetMultimap<K,V>asMultimapasMultimap()ImmutableSetMultimap<Short,Constructor<? extends TransactionEventRecord>>ImmutableMultimap<Short,Constructor<? extends TransactionEventRecord>>BaseImmutableMultimap<Short,Constructor<? extends TransactionEventRecord>>AbstractMultimap<Short,Constructor<? extends TransactionEventRecord>>Multimap<Short,Constructor<? extends TransactionEventRecord>>SetMultimap<Short,Constructor<? extends TransactionEventRecord>>ImmutableCollection<V>AbstractCollection<V>createValuescreateValues()ImmutableCollection<Constructor<? extends TransactionEventRecord>>AbstractCollection<Constructor<? extends TransactionEventRecord>>Spliterator<K>keySpliteratorkeySpliterator()Spliterator<Short>UnmodifiableIterator<>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/UnmodifiableIterator.classUnmodifiableIterator<K>Iterator<K>keyIteratorkeyIterator()UnmodifiableIterator<Short>Iterator<Short>ImmutableSet<K>ImmutableCollection<K>AbstractCollection<K>createKeySetcreateKeySet()ImmutableSet<Short>ImmutableCollection<Short>AbstractCollection<Short>ImmutableSet<Entry<K,V>>ImmutableCollection<Entry<K,V>>AbstractCollection<Entry<K,V>>createEntrySetcreateEntrySet()ImmutableSet<Entry<Short,Constructor<? extends TransactionEventRecord>>>ImmutableCollection<Entry<Short,Constructor<? extends TransactionEventRecord>>>AbstractCollection<Entry<Short,Constructor<? extends TransactionEventRecord>>>ImmutableMapImmutableMap()ImmutableMap<Short,Constructor<? extends TransactionEventRecord>>()copyOf(java.lang.Iterable)conflictExceptionconflictException(java.lang.String,java.lang.Object,java.lang.Object)Entry<?,?>checkNoConflictcheckNoConflict(boolean,java.lang.String,java.util.Map.Entry,java.util.Map.Entry)Builder<K,V>builderWithExpectedSizebuilderWithExpectedSize(int)builder()entryOfentryOf(java.lang.Object,java.lang.Object)BinaryOperator<>BiFunction<>Collector<>/modules/java.base/java/util/stream/Collector.classCollector<T,?,ImmutableMap<K,V>>toImmutableMaptoImmutableMap(java.util.function.Function,java.util.function.Function,java.util.function.BinaryOperator)Function<? super T,? extends K>Function<? super T,? extends V>BinaryOperator<V>BiFunction<V,V,V>toImmutableMap(java.util.function.Function,java.util.function.Function)Entry<?,?>[]EMPTY_ENTRY_ARRAY"Unknown action "Unknown action getAnnotatedReceiverTypegetAnnotatedReceiverType()getAnnotatedReturnTypegetAnnotatedReturnType()handleParameterNumberMismatchhandleParameterNumberMismatch(int,java.lang.Class[])Annotation[][]getParameterAnnotationsgetParameterAnnotations()getRawParameterAnnotationsgetRawParameterAnnotations()getSignaturegetSignature()getSlotgetSlot()setConstructorAccessorsetConstructorAccessor(jdk.internal.reflect.ConstructorAccessor)getConstructorAccessorgetConstructorAccessor()isVarArgsisVarArgs()newInstanceWithCallernewInstanceWithCaller(java.lang.Object[],boolean,java.lang.Class)newInstance(java.lang.Object[])specificToGenericStringHeaderspecificToGenericStringHeader(java.lang.StringBuilder)toShortStringtoShortString()specificToStringHeaderspecificToStringHeader(java.lang.StringBuilder)getGenericExceptionTypesgetGenericExceptionTypes()getExceptionTypesgetExceptionTypes()getGenericParameterTypesgetGenericParameterTypes()getParameterCountgetParameterCount()getParameterTypesgetParameterTypes()getSharedExceptionTypesgetSharedExceptionTypes()getSharedParameterTypesgetSharedParameterTypes()TypeVariable<Constructor>TypeVariable<Constructor>[]TypeVariable<Constructor<? extends TransactionEventRecord>>TypeVariable<Constructor<? extends TransactionEventRecord>>[]getAnnotationBytesgetAnnotationBytes()hasGenericInformationhasGenericInformation()checkCanSetAccessiblecheckCanSetAccessible(java.lang.Class)setAccessiblesetAccessible(boolean)copycopy()ConstructorConstructor(java.lang.Class,java.lang.Class[],java.lang.Class[],int,int,java.lang.String,byte[],byte[])Constructor<? extends TransactionEventRecord>(java.lang.Class,java.lang.Class[],java.lang.Class[],int,int,java.lang.String,byte[],byte[])getRootgetRoot()getGenericInfogetGenericInfo()Base class for records in data file: Put, Take, Rollback, CommitProvides a minimum guarantee we are not reading complete junk TODO toByteArray does an unneeded copy near impossible/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/TransactionIDOracle.javaTRANSACTION_ID/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/TransactionIDOracle.classTransactionIDOracleTransactionIDOracle()highestprevious/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/Writable.javaDefines methods for reading from or writing to streams <p>Based on org.apache.hadoop.io.WritableSerialize the fields of this object to <code>out</code><code>DataOutput</code> to serialize this object into.Deserialize the fields of this object from <code>in</code><code>DataInput</code> to deserialize this object from./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/WritableUtils.javaWritableUtilsWritableUtils()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/WritableUtils.classlen112127idxshiftbitsmask0xFFL255firstByteb0xFFn"value too long to fit in integer"value too long to fit in integer119111Util methods copied from org.apache.hadoop.io.WritableUtils.Serializes an integer to a binary stream with zero-compressed encoding.For -120 <= i <= 127, only one byte is used with the actual value.For other values of i, the first byte value indicates whether theinteger is positive or negative, and the number of bytes that follow.If the first byte value v is between -121 and -124, the following integeris positive, with number of bytes that follow are -(v+120).If the first byte value v is between -125 and -128, the following integeris negative, with number of bytes that follow are -(v+124). Bytes arestored in the high-non-zero-byte-first order.Binary output streamInteger to be serializedjava.io.IOExceptionSerializes a long to a binary stream with zero-compressed encoding.For -112 <= i <= 127, only one byte is used with the actual value.long is positive or negative, and the number of bytes that follow.If the first byte value v is between -113 and -120, the following longis positive, with number of bytes that follow are -(v+112).If the first byte value v is between -121 and -128, the following longis negative, with number of bytes that follow are -(v+120). Bytes areLong to be serialized take one's complement'Reads a zero-compressed encoded long from input stream and returns it.Binary input streamdeserialized long from stream.Reads a zero-compressed encoded integer from input stream and returns it.deserialized integer from stream.Given the first byte of a vint/vlong, determine the signthe first byteis the value negativeParse the first byte of a vint/vlong to determine the number of bytesthe first byte of the vint/vlongthe total number of bytes (1 to 9)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/WriteOrderOracle.javaWRITER_ORDERER/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/WriteOrderOracle.classWriteOrderOracleWriteOrderOracle()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/encryption/AESCTRNoPaddingProvider.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/encryptionorg.apache.flume.channel.file.encryptiongetCiphergetCipher(java.security.Key,int,byte[])/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/encryption/AESCTRNoPaddingProvider.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/encryptiondoFinaldoFinal(javax.crypto.Cipher,byte[])TYPEClass<AESCTRNoPaddingProvider>"AES/CTR/NoPadding"AES/CTR/NoPadding/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/encryption/CipherProvider$Encryptor$Builder.classBuilder<AESCTRNoPaddingEncryptor>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/encryption/CipherProvider$Decryptor$Builder.classBuilder<AESCTRNoPaddingDecryptor>seed12randomAESCTRNoPaddingEncryptorAESCTRNoPaddingEncryptor(java.security.Key,byte[])/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/encryption/AESCTRNoPaddingProvider$AESCTRNoPaddingEncryptor.classcipherclearTextAESCTRNoPaddingDecryptorAESCTRNoPaddingDecryptor(java.security.Key,byte[])/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/encryption/AESCTRNoPaddingProvider$AESCTRNoPaddingDecryptor.classcipherTextinput"Unable to encrypt or decrypt data "Unable to encrypt or decrypt data " input.length " input.length mode"Unable to load key using transformation: "Unable to load key using transformation: maxAllowedLen256"; Warning: Maximum allowed key length = "; Warning: Maximum allowed key length = " with the available JCE security policy files. Have you"
                + " installed the JCE unlimited strength jurisdiction policy"
                + " files?" with the available JCE security policy files. Have you installed the JCE unlimited strength jurisdiction policy files?"; Unable to find specified algorithm?"; Unable to find specified algorithm?/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/encryption/CipherProvider.javaBuilder<?>"key cannot be null"key cannot be nullBuilder implementations MUST have a no-arg constructor/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/encryption/CipherProviderFactory.javagetProvidergetProvider(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/encryption/CipherProviderFactory.class"rawtypes"rawtypesloggerClass<CipherProviderFactory>cipherProviderTypeprovidersetKeysetKey(java.security.Key)Builder<?>()setParameterssetParameters(byte[])? extends CipherProviderClass<? extends CipherProvider>providerClass"cipher provider type must not be null"cipher provider type must not be null"Not in enum, loading provider class: {}"Not in enum, loading provider class: {}Class<CipherProvider>Map<String,CipherProvider>CipherProvider[]Constructor<CipherProvider>? super CipherProviderClass<? super CipherProvider>TypeVariable<Class<CipherProvider>>TypeVariable<Class<CipherProvider>>[]errMessage"Unable to instantiate provider from "Unable to instantiate provider from "Class not found: "Class not found: Map<String,? extends CipherProvider>Constructor<? extends CipherProvider>TypeVariable<Class<? extends CipherProvider>>TypeVariable<Class<? extends CipherProvider>>[]"Cannot instantiate provider: "Cannot instantiate provider:  try to find builder class in enum of known providers handle the case where they have specified their own builder in the config/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/encryption/CipherProviderType.javaCipherProviderTypeCipherProviderType(java.lang.Class)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/encryption/CipherProviderType.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/encryption/DecryptionFailureException.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/encryption/DecryptionFailureException.class6646810195384793646L6646810195384793646Exception that is thrown when the channel is unable to decrypt an evenread from the channel./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/encryption/EncryptionConfiguration.javaEncryptionConfigurationEncryptionConfiguration()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/encryption/EncryptionConfiguration.class"encryption""keyProvider"keyProvider"activeKey"activeKey"cipherProvider""keys""passwordFile"passwordFile"keyStoreFile"keyStoreFile"keyStorePasswordFile"keyStorePasswordFile prefix before all encryption optionsEncryption key provider, default is null.Encryption key alias, default is null.Encryption cipher provider, default is null.Space separated list of keys which are needed for the current set of logsplus the one specified in keyAliasPath to key password file is:keys.aliasName.passwordFilePath to a jceks key storePath to a jceks key store password file/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/encryption/JCEFileKeyProvider.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/encryption/JCEFileKeyProvider.classkeyStorePasswordksMap<String,File>aliasPasswordFileMapClass<JCEFileKeyProvider>"jceks"jceksaliaskeyPasswordBiFunction<? super File,? super File,? extends File>merge(java.lang.String,java.io.File,java.util.function.BiFunction)BiFunction<? super String,? super File,? extends File>Function<? super String,? extends File>replace(java.lang.String,java.io.File)replace(java.lang.String,java.io.File,java.io.File)putIfAbsent(java.lang.String,java.io.File)BiConsumer<? super String,? super File>getOrDefault(java.lang.Object,java.io.File)Entry<String,File>Set<Entry<String,File>>Collection<Entry<String,File>>Iterable<Entry<String,File>>Map<? extends String,? extends File>put(java.lang.String,java.io.File)keyPasswordFile"KeyStore returned null for "KeyStore returned null for ? extends ExceptionClass<? extends Exception>Map<String,? extends Exception>Exception[]Constructor<? extends Exception>TypeVariable<Class<? extends Exception>>TypeVariable<Class<? extends Exception>>[]". " +
          "Key = ". Key = ", passwordFile = ", passwordFile = keyStoreFileNamekeyStorePasswordFileNameHashMap<String,File>AbstractMap<String,File>passwordProtectedKeys"KeyStore file not specified"KeyStore file not specified"KeyStore password  file not specified"KeyStore password  file not specified"Keys available to KeyStore was not specified or empty"Keys available to KeyStore was not specified or emptypasswordName"\\s+"\s+propertyNamepasswordFileName"Password file for alias "Password file for alias /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/encryption/KeyProvider.javaReturns a non-null Key/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/encryption/KeyProviderFactory.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/encryption/KeyProviderFactory.classClass<KeyProviderFactory>keyProviderType? extends BuilderClass<? extends Builder>"key provider type must not be null"key provider type must not be nullClass<Builder>Map<String,Builder>Builder[]Constructor<Builder>? super BuilderClass<? super Builder>TypeVariable<Class<Builder>>TypeVariable<Class<Builder>>[]"Unable to instantiate Builder from "Unable to instantiate Builder from Map<String,? extends Builder>Constructor<? extends Builder>TypeVariable<Class<? extends Builder>>TypeVariable<Class<? extends Builder>>[]"Cannot instantiate builder: "Cannot instantiate builder:  build the builder/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/encryption/KeyProviderType.javaKeyProviderTypeKeyProviderType(java.lang.Class)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/encryption/KeyProviderType.classkeyProviderClasskeyStoreProviderClass/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/instrumentation/FileChannelCounter.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/instrumentationorg.apache.flume.channel.file.instrumentationCHECKPOINT_BACKUP_WRITE_ERROR_COUNT/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/instrumentation/FileChannelCounter.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/instrumentationCHECKPOINT_WRITE_ERROR_COUNTEVENT_TAKE_ERROR_COUNTEVENT_PUT_ERROR_COUNTunhealthy"channel.file.event.put.error"channel.file.event.put.error"channel.file.event.take.error"channel.file.event.take.error"channel.file.checkpoint.write.error"channel.file.checkpoint.write.error"channel.file.checkpoint.backup.write.error"channel.file.checkpoint.backup.write.error/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/instrumentation/FileChannelCounterMBean.javaThe numeric representation (0/1) of the negated value of the open flag.A value of 0 represents that the channel is in a healthy state: it is either startingup (i.e. the replay is running) or already started up successfully.A value of 1 represents that the channel is in a permanently failed state, which means thatthe startup was unsuccessful due to an exception during the replay.Once the channel started up successfully the *ErrorCount (or the ratio of the *AttemptCountand *SuccessCount) counters should be used to check whether it is functioning properly.Note: this flag doesn't report the channel as unhealthy if the configuration failed because theChannelCounter might not have been instantiated/started yet.A count of the number of IOExceptions encountered while trying to put() onto the channel.@seeorg.apache.flume.channel.file.FileChannel.FileBackedTransaction#doPut(org.apache.flume.Event)A count of the number of errors encountered while trying to take() from the channel,including IOExceptions and corruption-related errors.org.apache.flume.channel.file.FileChannel.FileBackedTransaction#doTake()A count of the number of errors encountered while trying to write the checkpoints. Thisincludes any Throwables.org.apache.flume.channel.file.Log.BackgroundWorker#run()A count of the number of errors encountered while trying to write the backup checkpoints. Thisorg.apache.flume.channel.file.EventQueueBackingStoreFile#startBackupThread()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/test/resources/log4j2.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/test/resources/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/src/testOFFSYSTEM_OUT/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/generated-sources/protobuf/java/org/apache/flume/channel/file/proto/ProtosFactory.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/generated-sources/protobuf/java/org/apache/flume/channel/file/proto/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/generated-sources/protobuf/java/org/apache/flume/channel/file/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/generated-sources/protobuf/java/org/apache/flume/channel/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/generated-sources/protobuf/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/generated-sources/protobuf/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/generated-sources/protobuf/java/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/generated-sources/protobuf/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/generated-sources/protobuf/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/generated-sourcesorg.apache.flume.channel.file.protodescriptor/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/protointernal_static_FlumeEventHeader_fieldAccessorTableinternal_static_FlumeEventHeader_descriptorinternal_static_FlumeEvent_fieldAccessorTableinternal_static_FlumeEvent_descriptorinternal_static_TransactionEventFooter_fieldAccessorTableinternal_static_TransactionEventFooter_descriptorinternal_static_Commit_fieldAccessorTableinternal_static_Commit_descriptorinternal_static_Rollback_fieldAccessorTableinternal_static_Rollback_descriptorinternal_static_Take_fieldAccessorTableinternal_static_Take_descriptorinternal_static_Put_fieldAccessorTableinternal_static_Put_descriptorinternal_static_TransactionEventHeader_fieldAccessorTableinternal_static_TransactionEventHeader_descriptorinternal_static_LogFileEncryption_fieldAccessorTableinternal_static_LogFileEncryption_descriptorinternal_static_LogFileMetaData_fieldAccessorTableinternal_static_LogFileMetaData_descriptorinternal_static_ActiveLog_fieldAccessorTableinternal_static_ActiveLog_descriptorinternal_static_Checkpoint_fieldAccessorTableinternal_static_Checkpoint_descriptorProtosFactoryProtosFactory()descriptorData"\n\021filechannel.proto\"y\n\nCheckpoint\022\017\n\007ver" +
      "sion\030\001 \002(\017\022\024\n\014writeOrderID\030\002 \002(\020\022\021\n\tqueu" +
      "eSize\030\003 \002(\017\022\021\n\tqueueHead\030\004 \002(\017\022\036\n\nactive" +
      "Logs\030\005 \003(\0132\n.ActiveLog\"-\n\tActiveLog\022\021\n\tl" +
      "ogFileID\030\001 \002(\017\022\r\n\005count\030\002 \002(\017\"\341\001\n\017LogFil" +
      "eMetaData\022\017\n\007version\030\001 \002(\017\022\021\n\tlogFileID\030" +
      "\002 \002(\017\022\032\n\022checkpointPosition\030\003 \002(\020\022\036\n\026che" +
      "ckpointWriteOrderID\030\004 \002(\020\022&\n\nencryption\030" +
      "\005 \001(\0132\022.LogFileEncryption\022 \n\030backupCheck" +
      "pointPosition\030\006 \001(\020\022$\n\034backupCheckpointW"
filechannel.proto"y

Checkpoint
version (
writeOrderID (
	queueSize (
	queueHead (

activeLogs (2
.ActiveLog"-
	ActiveLog
	logFileID (
count ("
LogFileMetaData
version (
	logFileID (
checkpointPosition (
checkpointWriteOrderID (&

encryption (2.LogFileEncryption 
backupCheckpointPosition ($
backupCheckpointW"riteOrderID\030\007 \001(\020\"Q\n\021LogFileEncryption\022\026" +
      "\n\016cipherProvider\030\001 \002(\t\022\020\n\010keyAlias\030\002 \002(\t" +
      "\022\022\n\nparameters\030\003 \001(\014\"S\n\026TransactionEvent" +
      "Header\022\014\n\004type\030\001 \002(\017\022\025\n\rtransactionID\030\002 " +
      "\002(\020\022\024\n\014writeOrderID\030\003 \002(\020\"3\n\003Put\022\032\n\005even" +
      "t\030\001 \002(\0132\013.FlumeEvent\022\020\n\010checksum\030\002 \001(\020\"&" +
      "\n\004Take\022\016\n\006fileID\030\001 \002(\017\022\016\n\006offset\030\002 \002(\017\"\n" +
      "\n\010Rollback\"\026\n\006Commit\022\014\n\004type\030\001 \002(\017\"\030\n\026Tr" +
      "ansactionEventFooter\">\n\nFlumeEvent\022\"\n\007he" +
      "aders\030\001 \003(\0132\021.FlumeEventHeader\022\014\n\004body\030\002"riteOrderID ("Q
LogFileEncryption
cipherProvider (	
keyAlias (	

parameters ("S
TransactionEventHeader
type (
transactionID (
writeOrderID ("3
Put
event (2.FlumeEvent
checksum ("&
Take
fileID (
offset ("

Rollback"
Commit
type ("
TransactionEventFooter">

FlumeEvent"
headers (2.FlumeEventHeader
body" \002(\014\".\n\020FlumeEventHeader\022\013\n\003key\030\001 \002(\t\022\r\n" +
      "\005value\030\002 \001(\tB4\n#org.apache.flume.channel" +
      ".file.protoB\rProtosFactory" (".
FlumeEventHeader
key (	
value (	B4
#org.apache.flume.channel.file.protoBProtosFactoryassigner/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$1.classassignDescriptorsassignDescriptors(com.google.protobuf.Descriptors.FileDescriptor)rootList<Descriptor>SequencedCollection<Descriptor>Collection<Descriptor>Iterable<Descriptor>Spliterator<Descriptor>? super DescriptorConsumer<? super Descriptor>Iterator<Descriptor>Stream<Descriptor>BaseStream<Descriptor,Stream<Descriptor>>Predicate<? super Descriptor>? extends DescriptorCollection<? extends Descriptor>Iterable<? extends Descriptor>add(com.google.protobuf.Descriptors.Descriptor)addLast(com.google.protobuf.Descriptors.Descriptor)addFirst(com.google.protobuf.Descriptors.Descriptor)ListIterator<Descriptor>add(int,com.google.protobuf.Descriptors.Descriptor)set(int,com.google.protobuf.Descriptors.Descriptor)Comparator<? super Descriptor>UnaryOperator<Descriptor>Function<Descriptor,Descriptor>"Version"Version"WriteOrderID"WriteOrderID"QueueSize"QueueSize"QueueHead"QueueHead"ActiveLogs"ActiveLogs"LogFileID"LogFileID"Count"Count"CheckpointPosition"CheckpointPosition"CheckpointWriteOrderID"CheckpointWriteOrderID"Encryption"Encryption"BackupCheckpointPosition"BackupCheckpointPosition"BackupCheckpointWriteOrderID"BackupCheckpointWriteOrderID"CipherProvider"CipherProvider"KeyAlias"KeyAlias"Parameters"Parameters"Type""TransactionID"TransactionID"Event"Event"Checksum"6"FileID"FileID"Offset"Offset9"Headers"Headers"Body"Body11"Key"Key"Value"ValueFileDescriptor[]registry? extends ActiveLogOrBuilderList<? extends ActiveLogOrBuilder>SequencedCollection<? extends ActiveLogOrBuilder>Collection<? extends ActiveLogOrBuilder>Iterable<? extends ActiveLogOrBuilder>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$Checkpoint.classmemoizedSerializedSizememoizedIsInitializedinitFieldsinitFields()activeLogs_queueHead_queueSize_writeOrderID_version_bitField0_CheckpointCheckpoint(com.google.protobuf.CodedInputStream,com.google.protobuf.ExtensionRegistryLite)unknownFieldsdefaultInstanceCheckpoint(boolean)/Users/burakyetistiren/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar/Users/burakyetistiren/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/Users/burakyetistiren/.m2/repository/com/google/protobuf/protobuf-java/Users/burakyetistiren/.m2/repository/com/google/protobuf/Users/burakyetistiren/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar/com/Users/burakyetistiren/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar/com/google/Users/burakyetistiren/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar/com/google/protobuf/Users/burakyetistiren/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar/com/google/protobuf/GeneratedMessage$Builder.class/Users/burakyetistiren/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar/com/google/protobuf/AbstractMessage$Builder.class/Users/burakyetistiren/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar/com/google/protobuf/AbstractMessageLite$Builder.classCheckpoint(com.google.protobuf.GeneratedMessage.Builder)Parser<Checkpoint>/Users/burakyetistiren/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar/com/google/protobuf/Parser.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$Checkpoint$1.classAbstractParser<Checkpoint>/Users/burakyetistiren/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar/com/google/protobuf/AbstractParser.classparsePartialDelimitedFromparsePartialDelimitedFrom(java.io.InputStream,com.google.protobuf.ExtensionRegistryLite)parsePartialDelimitedFrom(java.io.InputStream)parseDelimitedFromparseDelimitedFrom(java.io.InputStream,com.google.protobuf.ExtensionRegistryLite)parseDelimitedFrom(java.io.InputStream)parsePartialFromparsePartialFrom(java.io.InputStream,com.google.protobuf.ExtensionRegistryLite)parsePartialFrom(java.io.InputStream)parseFromparseFrom(java.io.InputStream,com.google.protobuf.ExtensionRegistryLite)parseFrom(java.io.InputStream)parsePartialFrom(byte[],com.google.protobuf.ExtensionRegistryLite)parsePartialFrom(byte[])parsePartialFrom(byte[],int,int,com.google.protobuf.ExtensionRegistryLite)parsePartialFrom(byte[],int,int)parseFrom(byte[],com.google.protobuf.ExtensionRegistryLite)parseFrom(byte[])parseFrom(byte[],int,int,com.google.protobuf.ExtensionRegistryLite)parseFrom(byte[],int,int)parsePartialFrom(com.google.protobuf.ByteString,com.google.protobuf.ExtensionRegistryLite)parsePartialFrom(com.google.protobuf.ByteString)parseFrom(com.google.protobuf.ByteString,com.google.protobuf.ExtensionRegistryLite)parseFrom(com.google.protobuf.ByteString)parsePartialFrom(com.google.protobuf.CodedInputStream,com.google.protobuf.ExtensionRegistryLite)parsePartialFrom(com.google.protobuf.CodedInputStream)parseFrom(com.google.protobuf.CodedInputStream,com.google.protobuf.ExtensionRegistryLite)parseFrom(com.google.protobuf.CodedInputStream)AbstractParserAbstractParser()AbstractParser<Checkpoint>()extensionRegistrygetUnknownFieldsgetUnknownFields()addAll(java.lang.Iterable,java.util.Collection)Iterable<T>Collection<? super T>Iterable<? super T>newUninitializedMessageExceptionnewUninitializedMessageException(com.google.protobuf.MessageLite)mergeDelimitedFrommergeDelimitedFrom(java.io.InputStream)mergeDelimitedFrom(java.io.InputStream,com.google.protobuf.ExtensionRegistryLite)mergeFrommergeFrom(java.io.InputStream,com.google.protobuf.ExtensionRegistryLite)mergeFrom(java.io.InputStream)mergeFrom(byte[],int,int,com.google.protobuf.ExtensionRegistryLite)mergeFrom(byte[],com.google.protobuf.ExtensionRegistryLite)mergeFrom(byte[],int,int)mergeFrom(byte[])mergeFrom(com.google.protobuf.ByteString,com.google.protobuf.ExtensionRegistryLite)mergeFrom(com.google.protobuf.ByteString)mergeFrom(com.google.protobuf.CodedInputStream,com.google.protobuf.ExtensionRegistryLite)mergeFrom(com.google.protobuf.CodedInputStream)newUninitializedMessageException(com.google.protobuf.Message)getFieldBuildergetFieldBuilder(com.google.protobuf.Descriptors.FieldDescriptor)mergeUnknownFieldsmergeUnknownFields(com.google.protobuf.UnknownFieldSet)FieldSet<FieldDescriptor>/Users/burakyetistiren/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar/com/google/protobuf/FieldSet.classmergeFieldFrommergeFieldFrom(com.google.protobuf.CodedInputStream,com.google.protobuf.UnknownFieldSet.Builder,com.google.protobuf.ExtensionRegistryLite,com.google.protobuf.Descriptors.Descriptor,com.google.protobuf.Message.Builder,com.google.protobuf.FieldSet,int)mergeFrom(com.google.protobuf.Message)getInitializationErrorStringgetInitializationErrorString()List<String>SequencedCollection<String>findInitializationErrorsfindInitializationErrors()onChangedonChanged()getParentForChildrengetParentForChildren()parseUnknownFieldparseUnknownField(com.google.protobuf.CodedInputStream,com.google.protobuf.UnknownFieldSet.Builder,com.google.protobuf.ExtensionRegistryLite,int)isInitializedisInitialized()setUnknownFieldssetUnknownFields(com.google.protobuf.UnknownFieldSet)addRepeatedFieldaddRepeatedField(com.google.protobuf.Descriptors.FieldDescriptor,java.lang.Object)setRepeatedFieldsetRepeatedField(com.google.protobuf.Descriptors.FieldDescriptor,int,java.lang.Object)getRepeatedFieldgetRepeatedField(com.google.protobuf.Descriptors.FieldDescriptor,int)getRepeatedFieldCountgetRepeatedFieldCount(com.google.protobuf.Descriptors.FieldDescriptor)clearFieldclearField(com.google.protobuf.Descriptors.FieldDescriptor)setFieldsetField(com.google.protobuf.Descriptors.FieldDescriptor,java.lang.Object)getField(com.google.protobuf.Descriptors.FieldDescriptor)hasFieldhasField(com.google.protobuf.Descriptors.FieldDescriptor)newBuilderForFieldnewBuilderForField(com.google.protobuf.Descriptors.FieldDescriptor)Map<FieldDescriptor,Object>getAllFieldsgetAllFields()getDescriptorForTypegetDescriptorForType()internalGetFieldAccessorTableinternalGetFieldAccessorTable()isCleanisClean()markCleanmarkClean()onBuiltonBuilt()disposedispose()Builder(com.google.protobuf.GeneratedMessage.BuilderParent)Builder<?>(com.google.protobuf.GeneratedMessage.BuilderParent)noInitmutable_bitField0_donetag130x00000001170x00000002290x00000004370x00000008420x00000010ArrayList<ActiveLog>AbstractList<ActiveLog>AbstractCollection<ActiveLog>ArrayList<ActiveLog>()Spliterator<ActiveLog>? super ActiveLogConsumer<? super ActiveLog>Iterator<ActiveLog>Stream<ActiveLog>BaseStream<ActiveLog,Stream<ActiveLog>>Predicate<? super ActiveLog>? extends ActiveLogCollection<? extends ActiveLog>Iterable<? extends ActiveLog>add(org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog)AbstractCollection<ActiveLog>()addLast(org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog)addFirst(org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog)ListIterator<ActiveLog>add(int,org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog)set(int,org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog)Comparator<? super ActiveLog>UnaryOperator<ActiveLog>Function<ActiveLog,ActiveLog>AbstractList<ActiveLog>()ArrayList<ActiveLog>(java.util.Collection)ArrayList<ActiveLog>(int)Parser<>Parser<ActiveLog>? extends GeneratedMessageClass<? extends GeneratedMessage>? extends Builder<>Class<? extends Builder<>>Class<Checkpoint>dataprototypeRepeatedFieldBuilder<>/Users/burakyetistiren/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar/com/google/protobuf/RepeatedFieldBuilder.classRepeatedFieldBuilder<ActiveLog,Builder,ActiveLogOrBuilder>getActiveLogsFieldBuildergetActiveLogsFieldBuilder()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$Checkpoint$Builder.classactiveLogsBuilder_ensureActiveLogsIsMutableensureActiveLogsIsMutable()createcreate()maybeForceBuilderInitializationmaybeForceBuilderInitialization()Builder<Builder>Builder<Builder>()Builder<Builder>(com.google.protobuf.GeneratedMessage.BuilderParent)markDirtymarkDirty()List<IType>SequencedCollection<IType>Collection<IType>Iterable<IType>getMessageOrBuilderListgetMessageOrBuilderList()List<ActiveLogOrBuilder>SequencedCollection<ActiveLogOrBuilder>Collection<ActiveLogOrBuilder>Iterable<ActiveLogOrBuilder>List<BType>SequencedCollection<BType>Collection<BType>Iterable<BType>getBuilderListgetBuilderList()List<Builder>SequencedCollection<Builder>Collection<Builder>Iterable<Builder>List<MType>SequencedCollection<MType>Collection<MType>Iterable<MType>getMessageListgetMessageList()addBuilderaddBuilder(int,com.google.protobuf.GeneratedMessage)addBuilder(int,org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog)addBuilder(com.google.protobuf.GeneratedMessage)addBuilder(org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog)? extends MTypeIterable<? extends MType>addAllMessagesaddAllMessages(java.lang.Iterable)addMessageaddMessage(int,com.google.protobuf.GeneratedMessage)addMessage(int,org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog)addMessage(com.google.protobuf.GeneratedMessage)addMessage(org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog)setMessagesetMessage(int,com.google.protobuf.GeneratedMessage)setMessage(int,org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog)getMessageOrBuildergetMessageOrBuilder(int)getBuildergetBuilder(int)getMessagegetMessage(int)getCountgetCount()RepeatedFieldBuilderRepeatedFieldBuilder(java.util.List,boolean,com.google.protobuf.GeneratedMessage.BuilderParent,boolean)RepeatedFieldBuilder<ActiveLog,Builder,ActiveLogOrBuilder>(java.util.List,boolean,com.google.protobuf.GeneratedMessage.BuilderParent,boolean)from_bitField0_to_bitField0_parsedMessagebuilderForValue/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$ActiveLog.classcount_logFileID_ActiveLogActiveLog(com.google.protobuf.CodedInputStream,com.google.protobuf.ExtensionRegistryLite)ActiveLog(boolean)ActiveLog(com.google.protobuf.GeneratedMessage.Builder)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$ActiveLog$1.classAbstractParser<ActiveLog>AbstractParser<ActiveLog>()Class<ActiveLog>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$ActiveLog$Builder.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$LogFileMetaData.classbackupCheckpointWriteOrderID_backupCheckpointPosition_encryption_checkpointWriteOrderID_checkpointPosition_LogFileMetaDataLogFileMetaData(com.google.protobuf.CodedInputStream,com.google.protobuf.ExtensionRegistryLite)LogFileMetaData(boolean)LogFileMetaData(com.google.protobuf.GeneratedMessage.Builder)Parser<LogFileMetaData>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$LogFileMetaData$1.classAbstractParser<LogFileMetaData>AbstractParser<LogFileMetaData>()2533subBuilderParser<LogFileEncryption>490x00000020570x00000040Class<LogFileMetaData>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$LogFileMetaData$Builder.classSingleFieldBuilder<>/Users/burakyetistiren/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar/com/google/protobuf/SingleFieldBuilder.classSingleFieldBuilder<LogFileEncryption,Builder,LogFileEncryptionOrBuilder>getEncryptionFieldBuildergetEncryptionFieldBuilder()encryptionBuilder_mergeFrom(com.google.protobuf.GeneratedMessage)mergeFrom(org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption)setMessage(com.google.protobuf.GeneratedMessage)setMessage(org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption)getMessageOrBuilder()getBuilder()getMessage()SingleFieldBuilderSingleFieldBuilder(com.google.protobuf.GeneratedMessage,com.google.protobuf.GeneratedMessage.BuilderParent,boolean)SingleFieldBuilder<LogFileEncryption,Builder,LogFileEncryptionOrBuilder>(org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption,com.google.protobuf.GeneratedMessage.BuilderParent,boolean)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$LogFileEncryption.classparameters_keyAlias_cipherProvider_LogFileEncryptionLogFileEncryption(com.google.protobuf.CodedInputStream,com.google.protobuf.ExtensionRegistryLite)LogFileEncryption(boolean)LogFileEncryption(com.google.protobuf.GeneratedMessage.Builder)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$LogFileEncryption$1.classAbstractParser<LogFileEncryption>AbstractParser<LogFileEncryption>()1826Class<LogFileEncryption>bss/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$LogFileEncryption$Builder.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$TransactionEventHeader.classtransactionID_type_TransactionEventHeaderTransactionEventHeader(com.google.protobuf.CodedInputStream,com.google.protobuf.ExtensionRegistryLite)TransactionEventHeader(boolean)TransactionEventHeader(com.google.protobuf.GeneratedMessage.Builder)Parser<TransactionEventHeader>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$TransactionEventHeader$1.classAbstractParser<TransactionEventHeader>AbstractParser<TransactionEventHeader>()Class<TransactionEventHeader>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$TransactionEventHeader$Builder.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$Put.classchecksum_event_Put(com.google.protobuf.CodedInputStream,com.google.protobuf.ExtensionRegistryLite)Put(boolean)Put(com.google.protobuf.GeneratedMessage.Builder)Parser<Put>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$Put$1.classAbstractParser<Put>AbstractParser<Put>()Parser<FlumeEvent>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$Put$Builder.classSingleFieldBuilder<FlumeEvent,Builder,FlumeEventOrBuilder>getEventFieldBuildergetEventFieldBuilder()eventBuilder_mergeFrom(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent)setMessage(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent)SingleFieldBuilder<FlumeEvent,Builder,FlumeEventOrBuilder>(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent,com.google.protobuf.GeneratedMessage.BuilderParent,boolean)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$Take.classoffset_fileID_Take(com.google.protobuf.CodedInputStream,com.google.protobuf.ExtensionRegistryLite)Take(boolean)Take(com.google.protobuf.GeneratedMessage.Builder)Parser<Take>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$Take$1.classAbstractParser<Take>AbstractParser<Take>()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$Take$Builder.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$Rollback.classRollback(com.google.protobuf.CodedInputStream,com.google.protobuf.ExtensionRegistryLite)Rollback(boolean)Rollback(com.google.protobuf.GeneratedMessage.Builder)Parser<Rollback>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$Rollback$1.classAbstractParser<Rollback>AbstractParser<Rollback>()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$Rollback$Builder.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$Commit.classCommit(com.google.protobuf.CodedInputStream,com.google.protobuf.ExtensionRegistryLite)Commit(boolean)Commit(com.google.protobuf.GeneratedMessage.Builder)Parser<Commit>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$Commit$1.classAbstractParser<Commit>AbstractParser<Commit>()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$Commit$Builder.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$TransactionEventFooter.classTransactionEventFooterTransactionEventFooter(com.google.protobuf.CodedInputStream,com.google.protobuf.ExtensionRegistryLite)TransactionEventFooter(boolean)TransactionEventFooter(com.google.protobuf.GeneratedMessage.Builder)Parser<TransactionEventFooter>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$TransactionEventFooter$1.classAbstractParser<TransactionEventFooter>AbstractParser<TransactionEventFooter>()Class<TransactionEventFooter>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$TransactionEventFooter$Builder.class? extends FlumeEventHeaderOrBuilderList<? extends FlumeEventHeaderOrBuilder>SequencedCollection<? extends FlumeEventHeaderOrBuilder>Collection<? extends FlumeEventHeaderOrBuilder>Iterable<? extends FlumeEventHeaderOrBuilder>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$FlumeEvent.classbody_headers_FlumeEvent(com.google.protobuf.CodedInputStream,com.google.protobuf.ExtensionRegistryLite)FlumeEvent(boolean)FlumeEvent(com.google.protobuf.GeneratedMessage.Builder)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$FlumeEvent$1.classAbstractParser<FlumeEvent>AbstractParser<FlumeEvent>()ArrayList<FlumeEventHeader>AbstractList<FlumeEventHeader>AbstractCollection<FlumeEventHeader>ArrayList<FlumeEventHeader>()Spliterator<FlumeEventHeader>? super FlumeEventHeaderConsumer<? super FlumeEventHeader>Iterator<FlumeEventHeader>Stream<FlumeEventHeader>BaseStream<FlumeEventHeader,Stream<FlumeEventHeader>>Predicate<? super FlumeEventHeader>? extends FlumeEventHeaderCollection<? extends FlumeEventHeader>Iterable<? extends FlumeEventHeader>add(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader)AbstractCollection<FlumeEventHeader>()addLast(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader)addFirst(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader)ListIterator<FlumeEventHeader>add(int,org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader)set(int,org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader)Comparator<? super FlumeEventHeader>UnaryOperator<FlumeEventHeader>Function<FlumeEventHeader,FlumeEventHeader>AbstractList<FlumeEventHeader>()ArrayList<FlumeEventHeader>(java.util.Collection)ArrayList<FlumeEventHeader>(int)Parser<FlumeEventHeader>Class<FlumeEvent>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$FlumeEvent$Builder.classRepeatedFieldBuilder<FlumeEventHeader,Builder,FlumeEventHeaderOrBuilder>getHeadersFieldBuildergetHeadersFieldBuilder()headersBuilder_ensureHeadersIsMutableensureHeadersIsMutable()List<FlumeEventHeaderOrBuilder>SequencedCollection<FlumeEventHeaderOrBuilder>Collection<FlumeEventHeaderOrBuilder>Iterable<FlumeEventHeaderOrBuilder>addBuilder(int,org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader)addBuilder(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader)addMessage(int,org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader)addMessage(org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader)setMessage(int,org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader)RepeatedFieldBuilder<FlumeEventHeader,Builder,FlumeEventHeaderOrBuilder>(java.util.List,boolean,com.google.protobuf.GeneratedMessage.BuilderParent,boolean)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$FlumeEventHeader.classvalue_key_FlumeEventHeaderFlumeEventHeader(com.google.protobuf.CodedInputStream,com.google.protobuf.ExtensionRegistryLite)FlumeEventHeader(boolean)FlumeEventHeader(com.google.protobuf.GeneratedMessage.Builder)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$FlumeEventHeader$1.classAbstractParser<FlumeEventHeader>AbstractParser<FlumeEventHeader>()Class<FlumeEventHeader>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-file-channel/target/classes/org/apache/flume/channel/file/proto/ProtosFactory$FlumeEventHeader$Builder.class Generated by the protocol buffer compiler.  DO NOT EDIT! source: filechannel.proto required sfixed32 version = 1;<code>required sfixed32 version = 1;</code> required sfixed64 writeOrderID = 2;<code>required sfixed64 writeOrderID = 2;</code> required sfixed32 queueSize = 3;<code>required sfixed32 queueSize = 3;</code> required sfixed32 queueHead = 4;<code>required sfixed32 queueHead = 4;</code> repeated .ActiveLog activeLogs = 5;<code>repeated .ActiveLog activeLogs = 5;</code>Protobuf type {@code Checkpoint} Use Checkpoint.newBuilder() to construct. Construct using org.apache.flume.channel.file.proto.ProtosFactory.Checkpoint.newBuilder() @@protoc_insertion_point(builder_scope:Checkpoint) @@protoc_insertion_point(class_scope:Checkpoint) required sfixed32 logFileID = 1;<code>required sfixed32 logFileID = 1;</code> required sfixed32 count = 2;<code>required sfixed32 count = 2;</code>Protobuf type {@code ActiveLog} Use ActiveLog.newBuilder() to construct. Construct using org.apache.flume.channel.file.proto.ProtosFactory.ActiveLog.newBuilder() @@protoc_insertion_point(builder_scope:ActiveLog) @@protoc_insertion_point(class_scope:ActiveLog) required sfixed32 logFileID = 2;<code>required sfixed32 logFileID = 2;</code> required sfixed64 checkpointPosition = 3;<code>required sfixed64 checkpointPosition = 3;</code> required sfixed64 checkpointWriteOrderID = 4;<code>required sfixed64 checkpointWriteOrderID = 4;</code> optional .LogFileEncryption encryption = 5;<code>optional .LogFileEncryption encryption = 5;</code> optional sfixed64 backupCheckpointPosition = 6;<code>optional sfixed64 backupCheckpointPosition = 6;</code> optional sfixed64 backupCheckpointWriteOrderID = 7;<code>optional sfixed64 backupCheckpointWriteOrderID = 7;</code>Protobuf type {@code LogFileMetaData} Use LogFileMetaData.newBuilder() to construct. Construct using org.apache.flume.channel.file.proto.ProtosFactory.LogFileMetaData.newBuilder() @@protoc_insertion_point(builder_scope:LogFileMetaData) @@protoc_insertion_point(class_scope:LogFileMetaData) required string cipherProvider = 1;<code>required string cipherProvider = 1;</code> required string keyAlias = 2;<code>required string keyAlias = 2;</code> optional bytes parameters = 3;<code>optional bytes parameters = 3;</code>Protobuf type {@code LogFileEncryption} Use LogFileEncryption.newBuilder() to construct. Construct using org.apache.flume.channel.file.proto.ProtosFactory.LogFileEncryption.newBuilder() @@protoc_insertion_point(builder_scope:LogFileEncryption) @@protoc_insertion_point(class_scope:LogFileEncryption) required sfixed32 type = 1;<code>required sfixed32 type = 1;</code> required sfixed64 transactionID = 2;<code>required sfixed64 transactionID = 2;</code> required sfixed64 writeOrderID = 3;<code>required sfixed64 writeOrderID = 3;</code>Protobuf type {@code TransactionEventHeader} Use TransactionEventHeader.newBuilder() to construct. Construct using org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventHeader.newBuilder() @@protoc_insertion_point(builder_scope:TransactionEventHeader) @@protoc_insertion_point(class_scope:TransactionEventHeader) required .FlumeEvent event = 1;<code>required .FlumeEvent event = 1;</code> optional sfixed64 checksum = 2;<code>optional sfixed64 checksum = 2;</code>Protobuf type {@code Put} Use Put.newBuilder() to construct. Construct using org.apache.flume.channel.file.proto.ProtosFactory.Put.newBuilder() @@protoc_insertion_point(builder_scope:Put) @@protoc_insertion_point(class_scope:Put) required sfixed32 fileID = 1;<code>required sfixed32 fileID = 1;</code> required sfixed32 offset = 2;<code>required sfixed32 offset = 2;</code>Protobuf type {@code Take} Use Take.newBuilder() to construct. Construct using org.apache.flume.channel.file.proto.ProtosFactory.Take.newBuilder() @@protoc_insertion_point(builder_scope:Take) @@protoc_insertion_point(class_scope:Take)Protobuf type {@code Rollback} Use Rollback.newBuilder() to construct. Construct using org.apache.flume.channel.file.proto.ProtosFactory.Rollback.newBuilder() @@protoc_insertion_point(builder_scope:Rollback) @@protoc_insertion_point(class_scope:Rollback)Protobuf type {@code Commit} Use Commit.newBuilder() to construct. Construct using org.apache.flume.channel.file.proto.ProtosFactory.Commit.newBuilder() @@protoc_insertion_point(builder_scope:Commit) @@protoc_insertion_point(class_scope:Commit)Protobuf type {@code TransactionEventFooter} Use TransactionEventFooter.newBuilder() to construct. Construct using org.apache.flume.channel.file.proto.ProtosFactory.TransactionEventFooter.newBuilder() @@protoc_insertion_point(builder_scope:TransactionEventFooter) @@protoc_insertion_point(class_scope:TransactionEventFooter) repeated .FlumeEventHeader headers = 1;<code>repeated .FlumeEventHeader headers = 1;</code> required bytes body = 2;<code>required bytes body = 2;</code>Protobuf type {@code FlumeEvent} Use FlumeEvent.newBuilder() to construct. Construct using org.apache.flume.channel.file.proto.ProtosFactory.FlumeEvent.newBuilder() @@protoc_insertion_point(builder_scope:FlumeEvent) @@protoc_insertion_point(class_scope:FlumeEvent) required string key = 1;<code>required string key = 1;</code> optional string value = 2;<code>optional string value = 2;</code>Protobuf type {@code FlumeEventHeader} Use FlumeEventHeader.newBuilder() to construct. Construct using org.apache.flume.channel.file.proto.ProtosFactory.FlumeEventHeader.newBuilder() @@protoc_insertion_point(builder_scope:FlumeEventHeader) @@protoc_insertion_point(class_scope:FlumeEventHeader) @@protoc_insertion_point(outer_class_scope)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channel/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channelFlume NG Spillable Memory channelorg.apache.flume.channel.spillable.memory/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channel/src/main/java/org/apache/flume/channel/SpillableMemoryChannel.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channel/src/main/java/org/apache/flume/channel/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channel/src/main/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channel/src/main/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channel/src/main/java/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channel/src/main/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channel/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channel/srcorg.apache.flume.channelestimateEventSizeestimateEventSize(org.apache.flume.Event)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channel/target/classes/org/apache/flume/channel/SpillableMemoryChannel.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channel/target/classes/org/apache/flume/channel/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channel/target/classes/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channel/target/classes/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channel/target/classes/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channel/target/classes/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channel/targetgetOverflowTxgetOverflowTx()resizePrimaryQueueresizePrimaryQueue(int)overflowDeactivationThresholdoverflowTimeoutoverflowCapacitymemoryCapacityoverflowActivatedoverflowDisabledmaxMemQueueSizetotalStoredmemQueRemainingqueueLockbytesRemainingbyteCapacityBufferPercentagelastByteCapacityavgEventSizebyteCapacitydefaultByteCapacityBufferPercentagedefaultByteCapacitydefaultAvgEventSize100"memoryCapacity""overflowTimeout""overflowDeactivationThreshold""byteCapacityBufferPercentage""byteCapacity""overflowCapacity""avgEventSize"Class<SpillableMemoryChannel>100000000.800.8ArrayDeque<Event>/modules/java.base/java/util/ArrayDeque.classAbstractCollection<Event>Collection<Event>Iterable<Event>Deque<Event>Queue<Event>SequencedCollection<Event>"queueLock"Spliterator<Event>? super EventConsumer<? super Event>Iterator<Event>Stream<Event>BaseStream<Event,Stream<Event>>Predicate<? super Event>? extends EventCollection<? extends Event>Iterable<? extends Event>add(org.apache.flume.Event)AbstractCollection<Event>()offer(org.apache.flume.Event)addLast(org.apache.flume.Event)addFirst(org.apache.flume.Event)push(org.apache.flume.Event)offerLast(org.apache.flume.Event)offerFirst(org.apache.flume.Event)ArrayDeque<>deletedelete(int)nonNullElementAtnonNullElementAt(java.lang.Object[],int)subsub(int,int,int)inc(int,int,int)ArrayDequeArrayDeque(java.util.Collection)ArrayDeque<Event>(java.util.Collection)ArrayDeque(int)ArrayDeque<Event>(int)ArrayDeque()ArrayDeque<Event>()tailelements/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channel/target/classes/org/apache/flume/channel/SpillableMemoryChannel$MutableInteger.classamountoverflowCounter/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channel/target/classes/org/apache/flume/channel/SpillableMemoryChannel$DrainOrderQueue.classArrayDeque<MutableInteger>AbstractCollection<MutableInteger>Collection<MutableInteger>Iterable<MutableInteger>Deque<MutableInteger>Queue<MutableInteger>SequencedCollection<MutableInteger>ArrayDeque<MutableInteger>(int)Spliterator<MutableInteger>? super MutableIntegerConsumer<? super MutableInteger>Iterator<MutableInteger>Stream<MutableInteger>BaseStream<MutableInteger,Stream<MutableInteger>>Predicate<? super MutableInteger>? extends MutableIntegerCollection<? extends MutableInteger>Iterable<? extends MutableInteger>add(org.apache.flume.channel.SpillableMemoryChannel.MutableInteger)AbstractCollection<MutableInteger>()offer(org.apache.flume.channel.SpillableMemoryChannel.MutableInteger)addLast(org.apache.flume.channel.SpillableMemoryChannel.MutableInteger)addFirst(org.apache.flume.channel.SpillableMemoryChannel.MutableInteger)push(org.apache.flume.channel.SpillableMemoryChannel.MutableInteger)offerLast(org.apache.flume.channel.SpillableMemoryChannel.MutableInteger)offerFirst(org.apache.flume.channel.SpillableMemoryChannel.MutableInteger)ArrayDeque<MutableInteger>(java.util.Collection)ArrayDeque<MutableInteger>()sb"  [ "  [ eventCountheadValue"Cannot take "Cannot take " in DrainOrder Queue" in DrainOrder Queue" in DrainOrder Queue head " in DrainOrder Queue head commitPutsToPrimarycommitPutsToPrimary()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/flume-spillable-memory-channel/target/classes/org/apache/flume/channel/SpillableMemoryChannel$SpillableMemoryTransaction.classcommitPutsToOverflow_corecommitPutsToOverflow_core(org.apache.flume.Transaction)commitPutsToOverflowcommitPutsToOverflow()putCommitputCommit()takeCommittakeCommit()takeListByteCountputListByteCountoverflowPutCountlargestPutTxSizelargestTakeTxSizetakeCalledputCalleduseOverflowoverflowPutTxoverflowTakeTx5000eventByteSize"Put queue in "Put queue in " channel's Transaction having capacity " channel's Transaction having capacity " full, consider reducing batch size of sources" full, consider reducing batch size of sourcestakeSuceeded"Take is backing off as channel is empty."Take is backing off as channel is empty.drainOrderTop"Take is switching to primary"Take is switching to primary"Take is switching to overflow"Take is switching to overflow"Queue.poll returned NULL despite"
                + " semaphore signalling existence of entry"Queue.poll returned NULL despite semaphore signalling existence of entry"Put Committed. Drain Order Queue state : "Put Committed. Drain Order Queue state : "Take Committed. Drain Order Queue state : "Take Committed. Drain Order Queue state : memoryPercentFree"Overflow Deactivated"Overflow Deactivatedtimeout"Spillable Memory Channel's " +
                "memory capacity has been reached and overflow is " +
                "disabled. Consider increasing memoryCapacity."Spillable Memory Channel's memory capacity has been reached and overflow is disabled. Consider increasing memoryCapacity."Spillable Memory Channel's "
                + "memory capacity has been reached.  "Spillable Memory Channel's memory capacity has been reached.  " bytes are free and overflow is disabled. Consider "
                + "increasing byteCapacity or capacity." bytes are free and overflow is disabled. Consider increasing byteCapacity or capacity."Unable to insert event into memory " +
                "queue in spite of spare capacity, this is very unexpected"Unable to insert event into memory queue in spite of spare capacity, this is very unexpected"Rollback() of "Rollback() of " Take Tx" Take Tx" Put Tx" Put Tx"Empty Tx"Empty Tx"Not enough space in memory queue to rollback takes. This" +
                    " should never happen, please report"Not enough space in memory queue to rollback takes. This should never happen, please reportnewMemoryCapacity" must be >= 0" must be >= 0"Invalid "Invalid " specified, initializing " specified, initializing " channel to default value of {}" channel to default value of {}newOverflowTimeout"Incorrect value for "Incorrect value for "'s "'s " setting. Using default value {}" setting. Using default value {}newThreshold100.0". Using default value {} %". Using default value {} %"Error parsing "Error parsing ". Using default=". Using default=". ". ". Using default = ". Using default = .010.01" setting for " setting for "Couldn't acquire permits to downsize the byte capacity, " +
                        "resizing has been aborted"Couldn't acquire permits to downsize the byte capacity, resizing has been aborted"For channel "For channel " cannot be set to 0 if " cannot be set to 0 if " is also 0. " +
            "Using default value " is also 0. Using default value " = " = "0"diff"Memory buffer currently contains more events than the new size. " +
                    "Downsizing has been aborted."Memory buffer currently contains more events than the new size. Downsizing has been aborted.newQueueoverFlowCounttotalCountSpillableMemoryChannel will use main memory for buffering events until it has reached capacity.Thereafter file channel will be used as overflow. config settingsMax number of events to be stored in memorySeconds to wait before enabling disk overflow when memory fills upInternal use only. To remain undocumented in User guide. Determines thepercent free space available in mem queue when we stop spilling to overflowpercent of buffer between byteCapacity and the estimated event size.max number of bytes used for all events in the queue.max number of events in overflow.file channel setting that is overriden by Spillable Channelfile channel capacity overridden by Spillable ChannelEstimated average size of events expected to be in the channel percent memory consumption control for synchronizing access to primary/overflow channels & drain order This semaphore tracks number of free slots in primary channel (includes all active put lists) .. used to determine if the puts should go into primary or overflow tracks number of events in both channels. Takes will block on this max sie of memory Queue if true indicates the overflow should not be used at all. indicates if overflow can be used. invariant: false if overflowDisabled is true. if true overflow can be used. invariant: false if overflowDisabled is true. max events that the channel can hold  in memory mem full % at which we stop spill to overflow  pop on a empty queue will throw NoSuchElementException invariant: 0 will never be left in the queue for debugging only # of items in overflow channel this condition is optimization to avoid redundant conversions of int -> Integer -> string in hot path Take-Txn for overflow Put-Txn for overflow set on first invocation to put set on first invocation to take not a constraint, just hint for allocation # of puts going to overflow in this Txn Take will limit itself to a single channel within a transaction. This ensures commits/rollbacks are restricted to a single channel. takes should now occur from primary channel takes should now occur from overflow channel takeList is thd pvt, so no need to do this in synchronized block decide if overflow needs to be used check for enough event slots(memoryCapacity) for using memory queue check if we have enough byteCapacity for using memory queue reattempt only once if overflow is full first time around drop lock & reattempt update counters and semaphores TransactionRead parameters from context<li>memoryCapacity = total number of events allowed at one time in the memory queue.<li>overflowCapacity = total number of events allowed at one time in the overflow file channel.<li>byteCapacity = the max number of bytes used for events in the memory queue.<li>byteCapacityBufferPercentage = type int. Defines the percent of buffer between byteCapacityand the estimated event size.<li>overflowTimeout = type int. Number of seconds to wait on a full memory before deciding toenable overflow does not support reconfig when running 1) Memory Capacity overflowTimeout - wait time before switching to overflow when mem is full 3) Memory consumption control file channel capacity Determine if File channel needs to be disabled Configure File channel override keep-alive for  File channel if (memoryCapacity <= newMemoryCapacity)Each event occupies at least 1 slot, so return 1./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-channels/pom.xmlFlume NG Channelsmodules/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clientshttp://maven.apache.org/POM/4.0.0
  http://maven.apache.org/xsd/maven-4.0.0.xsdflume-ng-clients..Flume NG Log4j Appenderorg.apache.flume.clients.log4jappendermaven-assembly-plugindescriptorssrc/main/assembly/descriptor.xmlmake-assemblysinglelog4j1.2.17 pull in flume core only for unit tests. TODO: not ideal /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/main/assembly/descriptor.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/main/assembly/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/srchttp://maven.apache.org/ASSEMBLY/2.1.0assemblyhttp://maven.apache.org/ASSEMBLY/2.1.0 http://maven.apache.org/xsd/assembly-2.1.0.xsd Use this ID so that we get the same output artifact name as when we used the built in descriptorRef formatsjarincludeBaseDirectoryfileSetsfileSetdirectorysrc/main/resources-for-jar-with-deps/META-INFoutputDirectoryMETA-INFdependencySetsdependencySetunpackunpackOptions Rely on our curated aggregate LICENSE/NOTICE rather than one from some particular dependency excludeMETA-INF/LICENSE*META-INF/NOTICE*runtime/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/main/java/org/apache/flume/clients/log4jappender/LoadBalancingLog4jAppender.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/main/java/org/apache/flume/clients/log4jappender/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/main/java/org/apache/flume/clients/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/main/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/main/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/main/java/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/main/javagetPropertiesgetProperties(java.lang.String,java.lang.String,java.lang.String,long)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/target/classes/org/apache/flume/clients/log4jappender/LoadBalancingLog4jAppender.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/target/classes/org/apache/flume/clients/log4jappender/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/target/classes/org/apache/flume/clients/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/target/classes/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/target/classes/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/target/classes/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/target/classes/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/targetconfiguredmaxBackoffselectorhostshostNameserrorMsg"Flume Log4jAppender not configured correctly! Cannot" +
          " send events to Flume."Flume Log4jAppender not configured correctly! Cannot send events to Flume.errormsg"RPC client creation failed! "RPC client creation failed! hostsAndPorts"hosts must not be null"hosts must not be nullhostAndPort"h"hEnum<ClientType>Comparable<ClientType>compareTo(org.apache.flume.api.RpcClientFactory.ClientType)EnumDesc<ClientType>DynamicConstantDesc<ClientType>Optional<EnumDesc<ClientType>>Class<ClientType>Enum<ClientType>(java.lang.String,int)millis"Misconfigured max backoff, value must be greater than 0"Misconfigured max backoff, value must be greater than 0Appends Log4j Events to an external Flume client which is decribed by theLog4j configuration file. The appender takes the following requiredparameters:<strong>Hosts</strong> : A space separated list of host:port of the first hopat which Flume (through an AvroSource) is listening for events.<strong>Selector</strong> : Selection mechanism. Must be either ROUND_ROBIN,RANDOM or custom FQDN to class that inherits from LoadBalancingSelector. Ifempty defaults to ROUND_ROBINThe appender also takes the following optional parameters:<strong>MaxBackoff</strong> : A long value representing the maximum amount oftime in milliseconds the Load balancing client will backoff from a node thathas failed to consume an eventA sample log4j properties file which appends to a source would look like:<pre>log4j.appender.out2 = org.apache.flume.clients.log4jappender.LoadBalancingLog4jAppenderlog4j.appender.out2.Hosts = fooflumesource.com:25430 barflumesource.com:25430log4j.appender.out2.Selector = RANDOMlog4j.logger.org.apache.flume.clients.log4jappender = DEBUG,out2</p></pre>log4j.appender.out2.Selector = ROUND_ROBINlog4j.appender.out2.MaxBackoff = 60000<i>Note: Change the last line to the package of the class(es), that will dothe appending.For example if classes from the package com.bar.foo areappending, the last line would be:</i><p>log4j.logger.com.bar.foo = DEBUG,out2</p>Activate the options set using <tt>setHosts()</tt>, <tt>setSelector</tt>and <tt>setMaxBackoff</tt>FlumeExceptionif the LoadBalancingRpcClient cannot be instantiated./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/main/java/org/apache/flume/clients/log4jappender/Log4jAppender.javareconnectreconnect()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/target/classes/org/apache/flume/clients/log4jappender/Log4jAppender.classserializeserialize(java.lang.Object,org.apache.avro.Schema)org.apache.avro.ioDatumWriter<Object>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/Users/burakyetistiren/.m2/repository/org/apache/avro/Users/burakyetistiren/.m2/repository/org/apache/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/io/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/io/DatumWriter.classschemaList<Event>parseEventsparseEvents(org.apache.log4j.spi.LoggingEvent)rpcClientclientAddressavroSchemaUrlavroReflectionEnabledunsafeModeporthostnameflumeEvents"Cannot Append to Appender! Appender either closed or" +
          " not setup correctly!"Cannot Append to Appender! Appender either closed or not setup correctly!ListIterator<Event>add(int,org.apache.flume.Event)set(int,org.apache.flume.Event)Comparator<? super Event>UnaryOperator<Event>Function<Event,Event>"Flume append() failed."Flume append() failed." Exception follows." Exception follows.loggingEventheadersWithEncodingmessageseventsLinkedList<Event>/modules/java.base/java/util/LinkedList.classAbstractSequentialList<Event>/modules/java.base/java/util/AbstractSequentialList.classAbstractList<Event>LinkedList<Event>()AbstractList<Event>()AbstractSequentialListAbstractSequentialList()AbstractSequentialList<Event>()LinkedList<>AbstractSequentialList<>/modules/java.base/java/util/LinkedList$Node.classnodenode(int)Node<Event>unlink(java.util.LinkedList.Node)linkBeforelinkBefore(java.lang.Object,java.util.LinkedList.Node)linkBefore(org.apache.flume.Event,java.util.LinkedList.Node)linkLastlinkLast(java.lang.Object)linkLast(org.apache.flume.Event)LinkedListLinkedList(java.util.Collection)LinkedList<Event>(java.util.Collection)LinkedList()Set<Object>Collection<Object>Iterable<Object>singleLoggingEvent"UTF8"UTF8hdrs"Cannot find ID for schema. Adding header for schema, " +
        "which may be inefficient. Consider setting up an Avro Schema Cache."Cannot find ID for schema. Adding header for schema, which may be inefficient. Consider setting up an Avro Schema Cache.datumdatumSchemaorg.apache.avro.reflectReflectDatumWriter<Object>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/reflect/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/reflect/ReflectDatumWriter.classorg.apache.avro.specificSpecificDatumWriter<Object>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/specific/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/specific/SpecificDatumWriter.classorg.apache.avro.genericGenericDatumWriter<Object>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/generic/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/generic/GenericDatumWriter.classReflectDatumWriter<Object>(org.apache.avro.Schema)write(java.lang.Object,org.apache.avro.io.Encoder)setSchemasetSchema(org.apache.avro.Schema)writeFixedwriteFixed(org.apache.avro.Schema,java.lang.Object,org.apache.avro.io.Encoder)writeByteswriteBytes(java.lang.Object,org.apache.avro.io.Encoder)writeStringwriteString(java.lang.Object,org.apache.avro.io.Encoder)writeString(org.apache.avro.Schema,java.lang.Object,org.apache.avro.io.Encoder)Entry<Object,Object>Iterable<Entry<Object,Object>>getMapEntriesgetMapEntries(java.lang.Object)getMapSizegetMapSize(java.lang.Object)writeMapwriteMap(org.apache.avro.Schema,java.lang.Object,org.apache.avro.io.Encoder)Iterator<?>getArrayElementsgetArrayElements(java.lang.Object)getArraySizegetArraySize(java.lang.Object)resolveUnionresolveUnion(org.apache.avro.Schema,java.lang.Object)writeArraywriteArray(org.apache.avro.Schema,java.lang.Object,org.apache.avro.io.Encoder)writeEnumwriteEnum(org.apache.avro.Schema,java.lang.Object,org.apache.avro.io.Encoder)writeFieldwriteField(java.lang.Object,org.apache.avro.Schema.Field,org.apache.avro.io.Encoder,java.lang.Object)writeRecordwriteRecord(org.apache.avro.Schema,java.lang.Object,org.apache.avro.io.Encoder)addAvroTypeMsgaddAvroTypeMsg(org.apache.avro.AvroTypeException,java.lang.String)addClassCastMsgaddClassCastMsg(java.lang.ClassCastException,java.lang.String)npenpe(java.lang.NullPointerException,java.lang.String)writeWithoutConversionwriteWithoutConversion(org.apache.avro.Schema,java.lang.Object,org.apache.avro.io.Encoder)org.apache.avroConversion<>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/Conversion.classconvertconvert(org.apache.avro.Schema,org.apache.avro.LogicalType,org.apache.avro.Conversion,java.lang.Object)Conversion<T>write(org.apache.avro.Schema,java.lang.Object,org.apache.avro.io.Encoder)getDatagetData()GenericDatumWriterGenericDatumWriter(org.apache.avro.Schema,org.apache.avro.generic.GenericData)GenericDatumWriter<Object>(org.apache.avro.Schema,org.apache.avro.generic.GenericData)GenericDatumWriter(org.apache.avro.Schema)GenericDatumWriter<Object>(org.apache.avro.Schema)GenericDatumWriter(org.apache.avro.generic.GenericData)GenericDatumWriter<Object>(org.apache.avro.generic.GenericData)GenericDatumWriter()GenericDatumWriter<Object>()getSpecificDatagetSpecificData()SpecificDatumWriterSpecificDatumWriter(org.apache.avro.specific.SpecificData)SpecificDatumWriter<Object>(org.apache.avro.specific.SpecificData)SpecificDatumWriter(org.apache.avro.Schema,org.apache.avro.specific.SpecificData)SpecificDatumWriter<Object>(org.apache.avro.Schema,org.apache.avro.specific.SpecificData)SpecificDatumWriter(org.apache.avro.Schema)SpecificDatumWriter<Object>(org.apache.avro.Schema)SpecificDatumWriter(java.lang.Class)Class<Object>SpecificDatumWriter<Object>(java.lang.Class)SpecificDatumWriter()SpecificDatumWriter<Object>()ReflectDatumWriterReflectDatumWriter(org.apache.avro.reflect.ReflectData)ReflectDatumWriter<Object>(org.apache.avro.reflect.ReflectData)ReflectDatumWriter(org.apache.avro.Schema,org.apache.avro.reflect.ReflectData)ReflectDatumWriter<Object>(org.apache.avro.Schema,org.apache.avro.reflect.ReflectData)ReflectDatumWriter(org.apache.avro.Schema)ReflectDatumWriter(java.lang.Class,org.apache.avro.reflect.ReflectData)ReflectDatumWriter<Object>(java.lang.Class,org.apache.avro.reflect.ReflectData)ReflectDatumWriter(java.lang.Class)ReflectDatumWriter<Object>(java.lang.Class)ReflectDatumWriter()ReflectDatumWriter<Object>()"Error while trying to close RpcClient."Error while trying to close RpcClient."Flume log4jappender already closed!"Flume log4jappender already closed!"h1"h1"Failed to resolve local host address! "Failed to resolve local host address! Appends Log4j Events to an external Flume client which is described bythe Log4j configuration file. The appender takes two required parameters:<strong>Hostname</strong> : This is the hostname of the first hop<strong>Port</strong> : This the port on the above host where the FlumeSource is listening for events.<pre><p>log4j.appender.out2 = org.apache.flume.clients.log4jappender.Log4jAppenderlog4j.appender.out2.Port = 25430log4j.appender.out2.Hostname = foobarflumesource.comlog4j.logger.org.apache.flume.clients.log4jappender = DEBUG,out2</p></pre><p><i>Note: Change the last line to the package of the class(es), that willdo the appending.For example if classes from the packagecom.bar.foo are appending, the last line would be:</i></p><pre><p>log4j.logger.com.bar.foo = DEBUG,out2</p></pre>If this constructor is used programmatically rather than from a log4j confyou must set the <tt>port</tt> and <tt>hostname</tt> and then call<tt>activateOptions()</tt> before calling <tt>append()</tt>.Sets the hostname and port. Even if these are passed the<tt>activateOptions()</tt> function must be called before calling<tt>append()</tt>, else <tt>append()</tt> will throw an Exception.The first hop where the client should connect to.The port to connect on the host.Append the LoggingEvent, to send to the first Flume hop.The LoggingEvent to be appended to the flume.if the appender was closed,or the hostname and port were not setup, there was a timeout, or therewas a connection error.If rpcClient is null, it means either this appender object was neversetup by setting hostname and port and then calling activateOptionsor this appender object was closed by calling close(), so we throw anexception to show the appender is no longer accessible.To get the level back simply useLoggerEvent.toLevel(hdrs.get(Integer.parseInt(Log4jAvroHeaders.LOG_LEVEL.toString()))This function should be synchronized to make sure one threaddoes not close an appender another thread is using, and hence riskinga null pointer exception.Closes underlying client.If <tt>append()</tt> is called after this function is called,it will throw an exception.if errors occur during close Any append calls after this will result in an Exception. This method is named quite incorrectly in the interface. It should probably be called canUseLayout or something. According to the docs, even if the appender can work without a layout, if it can work with one, this method must return true.Set the first flume hop hostname.Set the port on the hostname to connect to.Activate the options set using <tt>setPort()</tt>and <tt>setHostname()</tt>if the <tt>hostname</tt> and<tt>port</tt> combination is invalid.Resolves local host address so it can be included in event headers.if local host address can not be resolved.Make it easy to reconnect on failure/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/main/java/org/apache/flume/clients/log4jappender/Log4jAvroHeaders.javaLog4jAvroHeadersLog4jAvroHeaders(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/target/classes/org/apache/flume/clients/log4jappender/Log4jAvroHeaders.classheaderName"flume.client.log4j.logger.other"flume.client.log4j.logger.other"flume.client.log4j.logger.name"flume.client.log4j.logger.name"flume.client.log4j.log.level"flume.client.log4j.log.level"flume.client.log4j.message.encoding"flume.client.log4j.message.encoding"flume.client.log4j.address"flume.client.log4j.address"flume.client.log4j.timestamp"flume.client.log4j.timestamp"flume.avro.schema.literal"flume.avro.schema.literal"flume.avro.schema.url"flume.avro.schema.url/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/test/resources/flume-loadbalancing-backoff-log4jtest.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/test/resources/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/testlog4j.appender.out2org.apache.flume.clients.log4jappender.LoadBalancingLog4jAppenderlog4j.appender.out2.SelectorROUND_ROBINlog4j.appender.out2.MaxBackoff30000log4j.logger.org.apache.flume.clients.log4jappenderDEBUG,out2/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/test/resources/flume-loadbalancing-rnd-log4jtest.propertiesRANDOM/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/test/resources/flume-loadbalancinglog4jtest.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/test/resources/flume-log4jtest-avro-generic.propertiesorg.apache.flume.clients.log4jappender.Log4jAppenderlog4j.appender.out2.Hostnamelocalhostlog4j.appender.out2.AvroSchemaUrlfile:///tmp/myrecord.avsc/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/test/resources/flume-log4jtest-avro-reflect.propertieslog4j.appender.out2.AvroReflectionEnabled/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/flume-ng-log4jappender/src/test/resources/flume-log4jtest.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-clients/pom.xmlFlume NG ClientsAll flume NG clients will come under this module/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-config-filter-api/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-config-filter-api/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfiltersorg.apache.flume.configfilter.apiFlume NG Config Filters API/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-config-filter-api/src/main/java/org/apache/flume/configfilter/AbstractConfigFilter.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-config-filter-api/src/main/java/org/apache/flume/configfilter/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-config-filter-api/src/main/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-config-filter-api/src/main/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-config-filter-api/src/main/java/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-config-filter-api/src/main/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-config-filter-api/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-config-filter-api/srcorg.apache.flume.configfilter/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-config-filter-api/target/classes/org/apache/flume/configfilter/AbstractConfigFilter.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-config-filter-api/target/classes/org/apache/flume/configfilter/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-config-filter-api/target/classes/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-config-filter-api/target/classes/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-config-filter-api/target/classes/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-config-filter-api/target/classes/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-config-filter-api/targetA base implementation of the common methods for Configuration filters/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-config-filter-api/src/main/java/org/apache/flume/configfilter/ConfigFilter.javaConfigFilter is a tool for replacing sensitive or generated data in Flume configurationFilter method that returns the value associated with the given keythe key to look up in the concrete implementationsthe value represented by the keySets the component name. Required by the configuration management.Returns the component name. Required by the configuration management.String the component nameA method to configure the componentThe map of configuration options needed by concrete implementations./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-environment-variable-config-filter/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-environment-variable-config-filterorg.apache.flume.configfilter.envFlume NG Environment Variable Config Filtercom.github.stefanbirknersystem-rules/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-environment-variable-config-filter/src/main/java/org/apache/flume/configfilter/EnvironmentVariableConfigFilter.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-environment-variable-config-filter/src/main/java/org/apache/flume/configfilter/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-environment-variable-config-filter/src/main/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-environment-variable-config-filter/src/main/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-environment-variable-config-filter/src/main/java/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-environment-variable-config-filter/src/main/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-environment-variable-config-filter/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-environment-variable-config-filter/srcNO-OP/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-external-process-config-filter/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-external-process-config-filterorg.apache.flume.configfilter.externalFlume NG External Process Config Filter/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-external-process-config-filter/src/main/java/org/apache/flume/configfilter/ExternalProcessConfigFilter.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-external-process-config-filter/src/main/java/org/apache/flume/configfilter/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-external-process-config-filter/src/main/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-external-process-config-filter/src/main/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-external-process-config-filter/src/main/java/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-external-process-config-filter/src/main/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-external-process-config-filter/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-external-process-config-filter/srcgetResultFromStreamgetResultFromStream(java.io.InputStream)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-external-process-config-filter/target/classes/org/apache/flume/configfilter/ExternalProcessConfigFilter.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-external-process-config-filter/target/classes/org/apache/flume/configfilter/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-external-process-config-filter/target/classes/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-external-process-config-filter/target/classes/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-external-process-config-filter/target/classes/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-external-process-config-filter/target/classes/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/flume-ng-external-process-config-filter/targetexecCommandexecCommand(java.lang.String)commandCHARSET_DEFAULTCHARSET_KEYCOMMAND_KEYClass<ExternalProcessConfigFilter>"command""charset""Error while reading value for key {}: "Error while reading value for key {}: charsetName"Unsupported charset: "Unsupported charset: " must be set for " +
          "ExternalProcessConfigFilter" must be set for ExternalProcessConfigFiltersplitnewLengthcommandPartspstderr"Process (%s) exited with non-zero (%s) status code. Sterr: %s"Process (%s) exited with non-zero (%s) status code. Sterr: %sscanner"External process has more than one line of output. " +
              "Only the first line is used."External process has more than one line of output. Only the first line is used."External process has not produced any output."External process has not produced any output./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configfilters/pom.xmlFlume NG Config Filters/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to You under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
Flume NG Configurationorg.apache.flume.conf/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/Context.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/srcget(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/Context.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/targetget(java.lang.String,java.lang.String)paramtersImmutableMap<String,String>prefix"The given prefix does not end with a period ("The given prefix does not end with a period (getKeygetKey()copyOf(java.util.Map.Entry)Comparator<Entry<K,V>>comparingByValuecomparingByValue(java.util.Comparator)comparingByKeycomparingByKey(java.util.Comparator)Comparator<? super K>Comparable<? super V>comparingByValue()Comparable<? super K>comparingByKey()setValuesetValue(java.lang.Object)setValue(java.lang.String)getValuegetValue()mapdefaultValue"{ parameters:"{ parameters:The context is a key-value store used to pass configuration informationthroughout the system.Gets a copy of the backing map structure.immutable copy of backing map structureRemoves all of the mappings from this map.Get properties which start with a prefix. When a property is returned,the prefix is removed the from name. For example, if this method iscalled with a parameter &quot;hdfs.&quot; and the context contains:<code>{ hdfs.key = value, otherKey = otherValue }</code>this method will return a map containing:{ key = value}<b>Note:</b> The <tt>prefix</tt> must end with a period character. If notthis method will raise an IllegalArgumentException.key prefix to find and remove from keys in resulting mapmap with keys which matched prefix with prefix removed fromkeys in resulting map. If no keys are matched, the returned map isemptyIllegalArguemntExceptionif the given prefix does not end witha period character.Associates all of the given map's keys and values in the Context.Associates the specified value with the specified key in this context.If the context previously contained a mapping for the key, the old valueis replaced by the specified value.key with which the specified value is to be associatedto be associated with the specified keyReturns true if this Context contains a mapping for key.Otherwise, returns false.Gets value mapped to key, returning defaultValue if unmapped.to be foundreturned if key is unmappedvalue associated with keyGets value mapped to key, returning null if unmapped.Note that this method returns an object as opposed to aprimitive. The configuration key requested may not be mappedto a value and by returning the primitive object wrapper we canreturn null. If the key does not exist the return value ofthis method is assigned directly to a primitive, a{@link NullPointerException} will be thrown.value associated with key or null if unmapped/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/BasicConfigurationConstants.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/confBasicConfigurationConstantsBasicConfigurationConstants()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/BasicConfigurationConstants.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf"configfilters"configfilters"sources"sources"selector."selector."sinks"sinks"processor."processor."sinkgroups"sinkgroups"channels"channels"config"config"type" disable explicit object creation/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/ComponentConfiguration.javanotFoundConfigClass/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/ComponentConfiguration.classList<FlumeConfigurationError>SequencedCollection<FlumeConfigurationError>Collection<FlumeConfigurationError>Iterable<FlumeConfigurationError>componentNameLinkedList<FlumeConfigurationError>AbstractSequentialList<FlumeConfigurationError>AbstractList<FlumeConfigurationError>AbstractCollection<FlumeConfigurationError>Deque<FlumeConfigurationError>Queue<FlumeConfigurationError>LinkedList<FlumeConfigurationError>()Spliterator<FlumeConfigurationError>? super FlumeConfigurationErrorConsumer<? super FlumeConfigurationError>Iterator<FlumeConfigurationError>Stream<FlumeConfigurationError>BaseStream<FlumeConfigurationError,Stream<FlumeConfigurationError>>Predicate<? super FlumeConfigurationError>? extends FlumeConfigurationErrorCollection<? extends FlumeConfigurationError>Iterable<? extends FlumeConfigurationError>add(org.apache.flume.conf.FlumeConfigurationError)AbstractCollection<FlumeConfigurationError>()addLast(org.apache.flume.conf.FlumeConfigurationError)addFirst(org.apache.flume.conf.FlumeConfigurationError)ListIterator<FlumeConfigurationError>add(int,org.apache.flume.conf.FlumeConfigurationError)set(int,org.apache.flume.conf.FlumeConfigurationError)Comparator<? super FlumeConfigurationError>UnaryOperator<FlumeConfigurationError>Function<FlumeConfigurationError,FlumeConfigurationError>AbstractList<FlumeConfigurationError>()AbstractSequentialList<FlumeConfigurationError>()offer(org.apache.flume.conf.FlumeConfigurationError)push(org.apache.flume.conf.FlumeConfigurationError)offerLast(org.apache.flume.conf.FlumeConfigurationError)offerFirst(org.apache.flume.conf.FlumeConfigurationError)Node<FlumeConfigurationError>linkBefore(org.apache.flume.conf.FlumeConfigurationError,java.util.LinkedList.Node)linkLast(org.apache.flume.conf.FlumeConfigurationError)LinkedList<FlumeConfigurationError>(java.util.Collection)confType"Component has no type. Cannot configure. "Component has no type. Cannot configure. "Already configured component."Already configured component.indentCountindentSbindent"ComponentConfiguration["ComponentConfiguration["CONFIG: "CONFIG: ComponentTypeComponentType(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/ComponentConfiguration$ComponentType.class"ConfigFilter"ConfigFilter"Source"Source"Sink"Sink"SinkProcessor"SinkProcessor"Sinkgroup"Sinkgroup"Channel"Channel"ChannelSelector"ChannelSelectorAbstract implementation of the Component Configuration Manager. This classdoes the configuration in the object. The component specific versions, whichinherit from this, create the configuration based on the config file. Allsubclasses of this class store properties of the component. The propertiescan be stored as properties in this Type can be set by child class constructors, so check if it was./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/ComponentConfigurationFactory.javacomponent? extends ComponentConfigurationClass<? extends ComponentConfiguration>"Cannot create component without knowing its type!"Cannot create component without knowing its type!Constructor<? extends ComponentConfiguration>TypeVariable<Constructor<? extends ComponentConfiguration>>TypeVariable<Constructor<? extends ComponentConfiguration>>[]Constructor<? extends ComponentConfiguration>(java.lang.Class,java.lang.Class[],java.lang.Class[],int,int,java.lang.String,byte[],byte[])Map<String,? extends ComponentConfiguration>ComponentConfiguration[]TypeVariable<Class<? extends ComponentConfiguration>>TypeVariable<Class<? extends ComponentConfiguration>>[]ignored"Cannot create configuration. Unknown Type specified: "Cannot create configuration. Unknown Type specified: "Could not create configuration! " +
            " Due to "Could not create configuration!  Due to /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/ComponentWithClassName.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/ConfigFilterFactory.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/ConfigFilterFactory.classClass<ConfigFilterFactory>? extends ConfigFilterClass<? extends ConfigFilter>aClass"name""Creating instance of configfilter {}, type {}"Creating instance of configfilter {}, type {}configFilterMap<String,? extends ConfigFilter>ConfigFilter[]Constructor<? extends ConfigFilter>TypeVariable<Class<? extends ConfigFilter>>TypeVariable<Class<? extends ConfigFilter>>[]"Unable to create configfilter: "Unable to create configfilter: ", type: ", type: ", class: ", class: classnamesrcType"Configfilter type {} is a custom type"Configfilter type {} is a custom type"Unable to load configfilter type: "Unable to load configfilter type: /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/ConfigurationException.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/ConfigurationException.classarg0arg1/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/FlumeConfiguration.javaCONFIG_CHANNELSCONFIG_CHANNELS_PREFIXCONFIG_CONFIGCONFIG_CONFIGFILTERSCONFIG_CONFIGFILTERS_PREFIXCONFIG_SINKGROUPSCONFIG_SINKGROUPS_PREFIXCONFIG_SINKSCONFIG_SINKS_PREFIXCONFIG_SOURCESCONFIG_SOURCES_PREFIXWARNINGAGENT_CONFIGURATION_INVALIDAGENT_NAME_MISSINGCONFIG_ERRORDUPLICATE_PROPERTYINVALID_PROPERTYPROPERTY_NAME_NULLPROPERTY_PART_OF_ANOTHER_GROUPPROPERTY_VALUE_NULLaddErroraddError(java.lang.String,org.apache.flume.conf.FlumeConfigurationErrorType,org.apache.flume.conf.FlumeConfigurationError.ErrorOrWarning)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/FlumeConfiguration.classaddRawPropertyaddRawProperty(java.lang.String,java.lang.String)validateConfigurationvalidateConfiguration()errorsMap<String,AgentConfiguration>agentConfigMapClass<FlumeConfiguration>"line.separator""\n""  "  HashMap<String,AgentConfiguration>AbstractMap<String,AgentConfiguration>HashMap<String,AgentConfiguration>()? super AgentConfiguration? extends AgentConfigurationBiFunction<? super AgentConfiguration,? super AgentConfiguration,? extends AgentConfiguration>merge(java.lang.String,org.apache.flume.conf.FlumeConfiguration.AgentConfiguration,java.util.function.BiFunction)BiFunction<? super String,? super AgentConfiguration,? extends AgentConfiguration>Function<? super String,? extends AgentConfiguration>replace(java.lang.String,org.apache.flume.conf.FlumeConfiguration.AgentConfiguration)replace(java.lang.String,org.apache.flume.conf.FlumeConfiguration.AgentConfiguration,org.apache.flume.conf.FlumeConfiguration.AgentConfiguration)putIfAbsent(java.lang.String,org.apache.flume.conf.FlumeConfiguration.AgentConfiguration)BiConsumer<? super String,? super AgentConfiguration>getOrDefault(java.lang.Object,org.apache.flume.conf.FlumeConfiguration.AgentConfiguration)Entry<String,AgentConfiguration>Set<Entry<String,AgentConfiguration>>Collection<Entry<String,AgentConfiguration>>Iterable<Entry<String,AgentConfiguration>>Collection<AgentConfiguration>Iterable<AgentConfiguration>Map<? extends String,? extends AgentConfiguration>put(java.lang.String,org.apache.flume.conf.FlumeConfiguration.AgentConfiguration)AbstractMap<String,AgentConfiguration>()Node<String,AgentConfiguration>TreeNode<String,AgentConfiguration>newTreeNode(int,java.lang.String,org.apache.flume.conf.FlumeConfiguration.AgentConfiguration,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.flume.conf.FlumeConfiguration.AgentConfiguration,java.util.HashMap.Node)Node<String,AgentConfiguration>[]putVal(int,java.lang.String,org.apache.flume.conf.FlumeConfiguration.AgentConfiguration,boolean,boolean)HashMap<String,AgentConfiguration>(java.util.Map)HashMap<String,AgentConfiguration>(int)HashMap<String,AgentConfiguration>(int,float)Set<Entry<Object,Object>>Collection<Entry<Object,Object>>"Configuration property ignored: {} = {}"Configuration property ignored: {} = {}Iterator<Entry<String,AgentConfiguration>>Spliterator<Entry<String,AgentConfiguration>>? super Entry<String,AgentConfiguration>Consumer<? super Entry<String,AgentConfiguration>>Stream<Entry<String,AgentConfiguration>>BaseStream<Entry<String,AgentConfiguration>,Stream<Entry<String,AgentConfiguration>>>Predicate<? super Entry<String,AgentConfiguration>>? extends Entry<String,AgentConfiguration>Collection<? extends Entry<String,AgentConfiguration>>Iterable<? extends Entry<String,AgentConfiguration>>add(java.util.Map.Entry)agentNamesetValue(org.apache.flume.conf.FlumeConfiguration.AgentConfiguration)aconf"Agent configuration invalid for agent '{}'. It will be removed."Agent configuration invalid for agent '{}'. It will be removed."Channels:{}\n"Channels:{}
"Sinks {}\n"Sinks {}
"Sources {}\n"Sources {}
"Post-validation flume configuration contains configuration for agents: {}"Post-validation flume configuration contains configuration for agents: {}rawNamerawValue'.'configKeyerrorTypeparseConfigKeyparseConfigKey(java.lang.String,java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/FlumeConfiguration$AgentConfiguration.classMap<String,Context>addComponentConfigaddComponentConfig(java.lang.String,java.lang.String,java.lang.String,java.util.Map)addAsSourceConfigaddAsSourceConfig(java.lang.String,java.lang.String)addAsChannelValueaddAsChannelValue(java.lang.String,java.lang.String)addAsSinkConfigaddAsSinkConfig(java.lang.String,java.lang.String)addAsSinkGroupConfigaddAsSinkGroupConfig(java.lang.String,java.lang.String)addAsConfigFilterConfigaddAsConfigFilterConfig(java.lang.String,java.lang.String)addPropertyaddProperty(java.lang.String,java.lang.String)stringToSetstringToSet(java.lang.String,java.lang.String)getSpaceDelimitedListgetSpaceDelimitedList(java.util.Set)validGroupSinksvalidGroupSinks(java.util.Set,java.util.Map,org.apache.flume.conf.sink.SinkGroupConfiguration)validateGroupsvalidateGroups(java.util.Set)validateSinksvalidateSinks(java.util.Set)validateSourcesvalidateSources(java.util.Set)validateConfigFilterSetvalidateConfigFilterSet()validateChannelsvalidateChannels(java.util.Set)ComponentWithClassName[]getKnownComponentgetKnownComponent(java.lang.String,org.apache.flume.conf.ComponentWithClassName[])getKnownConfigFiltergetKnownConfigFilter(java.lang.String)getKnownSourcegetKnownSource(java.lang.String)getKnownSinkgetKnownSink(java.lang.String)getKnownChannelgetKnownChannel(java.lang.String)filterValuefilterValue(org.apache.flume.Context,java.lang.String)createConfigFilterPatterncreateConfigFilterPattern(org.apache.flume.configfilter.ConfigFilter)createConfigFilterscreateConfigFilters()Map[]Map<String,Context>[]runFiltersOnContextMapsrunFiltersOnContextMaps(java.util.Map[])runFiltersThroughConfigsrunFiltersThroughConfigs()isValidisValid()AgentConfigurationAgentConfiguration(java.lang.String,java.util.List)Map<String,Pattern>configFilterPatternCacheList<ConfigFilter>SequencedCollection<ConfigFilter>Collection<ConfigFilter>Iterable<ConfigFilter>configFiltersInstanceserrorListsinkgroupSetchannelSetsourceSetconfigFilterSetsinkSetsinkGroupContextMapchannelContextMapsinkContextMapsourceContextMapconfigFilterContextMapMap<String,ComponentConfiguration>configFilterConfigMapsinkgroupConfigMapchannelConfigMapsinkConfigMapsourceConfigMapconfigFiltersHashMap<String,ComponentConfiguration>AbstractMap<String,ComponentConfiguration>HashMap<String,ComponentConfiguration>()? super ComponentConfigurationBiFunction<? super ComponentConfiguration,? super ComponentConfiguration,? extends ComponentConfiguration>merge(java.lang.String,org.apache.flume.conf.ComponentConfiguration,java.util.function.BiFunction)BiFunction<? super String,? super ComponentConfiguration,? extends ComponentConfiguration>Function<? super String,? extends ComponentConfiguration>replace(java.lang.String,org.apache.flume.conf.ComponentConfiguration)replace(java.lang.String,org.apache.flume.conf.ComponentConfiguration,org.apache.flume.conf.ComponentConfiguration)putIfAbsent(java.lang.String,org.apache.flume.conf.ComponentConfiguration)BiConsumer<? super String,? super ComponentConfiguration>getOrDefault(java.lang.Object,org.apache.flume.conf.ComponentConfiguration)Entry<String,ComponentConfiguration>Set<Entry<String,ComponentConfiguration>>Collection<Entry<String,ComponentConfiguration>>Iterable<Entry<String,ComponentConfiguration>>Collection<ComponentConfiguration>Iterable<ComponentConfiguration>Map<? extends String,? extends ComponentConfiguration>put(java.lang.String,org.apache.flume.conf.ComponentConfiguration)AbstractMap<String,ComponentConfiguration>()Node<String,ComponentConfiguration>TreeNode<String,ComponentConfiguration>newTreeNode(int,java.lang.String,org.apache.flume.conf.ComponentConfiguration,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.flume.conf.ComponentConfiguration,java.util.HashMap.Node)Node<String,ComponentConfiguration>[]putVal(int,java.lang.String,org.apache.flume.conf.ComponentConfiguration,boolean,boolean)HashMap<String,ComponentConfiguration>(java.util.Map)HashMap<String,ComponentConfiguration>(int)HashMap<String,ComponentConfiguration>(int,float)HashMap<String,Context>AbstractMap<String,Context>HashMap<String,Context>()? super Context? extends ContextBiFunction<? super Context,? super Context,? extends Context>merge(java.lang.String,org.apache.flume.Context,java.util.function.BiFunction)BiFunction<? super String,? super Context,? extends Context>Function<? super String,? extends Context>replace(java.lang.String,org.apache.flume.Context)replace(java.lang.String,org.apache.flume.Context,org.apache.flume.Context)putIfAbsent(java.lang.String,org.apache.flume.Context)BiConsumer<? super String,? super Context>getOrDefault(java.lang.Object,org.apache.flume.Context)Entry<String,Context>Set<Entry<String,Context>>Collection<Entry<String,Context>>Iterable<Entry<String,Context>>Collection<Context>Iterable<Context>Map<? extends String,? extends Context>put(java.lang.String,org.apache.flume.Context)AbstractMap<String,Context>()Node<String,Context>TreeNode<String,Context>newTreeNode(int,java.lang.String,org.apache.flume.Context,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.flume.Context,java.util.HashMap.Node)Node<String,Context>[]putVal(int,java.lang.String,org.apache.flume.Context,boolean,boolean)HashMap<String,Context>(java.util.Map)HashMap<String,Context>(int)HashMap<String,Context>(int,float)ArrayList<ConfigFilter>AbstractList<ConfigFilter>AbstractCollection<ConfigFilter>ArrayList<ConfigFilter>()Spliterator<ConfigFilter>? super ConfigFilterConsumer<? super ConfigFilter>Iterator<ConfigFilter>Stream<ConfigFilter>BaseStream<ConfigFilter,Stream<ConfigFilter>>Predicate<? super ConfigFilter>Collection<? extends ConfigFilter>Iterable<? extends ConfigFilter>add(org.apache.flume.configfilter.ConfigFilter)AbstractCollection<ConfigFilter>()addLast(org.apache.flume.configfilter.ConfigFilter)addFirst(org.apache.flume.configfilter.ConfigFilter)ListIterator<ConfigFilter>add(int,org.apache.flume.configfilter.ConfigFilter)set(int,org.apache.flume.configfilter.ConfigFilter)Comparator<? super ConfigFilter>UnaryOperator<ConfigFilter>Function<ConfigFilter,ConfigFilter>AbstractList<ConfigFilter>()ArrayList<ConfigFilter>(java.util.Collection)ArrayList<ConfigFilter>(int)HashMap<String,Pattern>AbstractMap<String,Pattern>HashMap<String,Pattern>()? super Pattern? extends PatternBiFunction<? super Pattern,? super Pattern,? extends Pattern>merge(java.lang.String,java.util.regex.Pattern,java.util.function.BiFunction)BiFunction<? super String,? super Pattern,? extends Pattern>Function<? super String,? extends Pattern>replace(java.lang.String,java.util.regex.Pattern)replace(java.lang.String,java.util.regex.Pattern,java.util.regex.Pattern)putIfAbsent(java.lang.String,java.util.regex.Pattern)BiConsumer<? super String,? super Pattern>getOrDefault(java.lang.Object,java.util.regex.Pattern)Entry<String,Pattern>Set<Entry<String,Pattern>>Collection<Entry<String,Pattern>>Iterable<Entry<String,Pattern>>Collection<Pattern>Iterable<Pattern>Map<? extends String,? extends Pattern>put(java.lang.String,java.util.regex.Pattern)AbstractMap<String,Pattern>()Node<String,Pattern>TreeNode<String,Pattern>newTreeNode(int,java.lang.String,java.util.regex.Pattern,java.util.HashMap.Node)newNode(int,java.lang.String,java.util.regex.Pattern,java.util.HashMap.Node)Node<String,Pattern>[]putVal(int,java.lang.String,java.util.regex.Pattern,boolean,boolean)HashMap<String,Pattern>(java.util.Map)HashMap<String,Pattern>(int)HashMap<String,Pattern>(int,float)"Starting validation of configuration for agent: {}"Starting validation of configuration for agent: {}"Initial configuration: {}"Initial configuration: {}"Agent configuration for '{}' does not contain any channels. Marking it as invalid."Agent configuration for '{}' does not contain any channels. Marking it as invalid.HashSet<String>(java.util.Collection)AbstractCollection<String>()AbstractSet<String>()HashSet<T>AbstractSet<T>AbstractCollection<T>Collection<T>Set<T>newHashSetnewHashSet(int)HashSetHashSet(int,float,boolean)HashSet<String>(int,float,boolean)HashSet(int)HashSet<String>(int)HashSet(int,float)HashSet<String>(int,float)HashSet(java.util.Collection)HashSet()HashSet<String>()PRESENTHashMap<E,Object>AbstractMap<E,Object>"Agent configuration for '{}' does not contain any valid channels. " +
                "Marking it as invalid."Agent configuration for '{}' does not contain any valid channels. Marking it as invalid."Agent configuration for '{}' has no sources or sinks. Will be marked invalid."Agent configuration for '{}' has no sources or sinks. Will be marked invalid."Post validation configuration for {}"Post validation configuration for {}mapscomponentConfiguration"Error while creating config filter {}"Error while creating config filter {}"\\$\\{"\$\{"\\[(|'|\")" +  // delimiter :'," or nothing
              "(?<key>[-_a-zA-Z0-9]+)" + // key
              "\\1\\]" + // matching delimiter
              "\\}"\[(|'|")(?<key>[-_a-zA-Z0-9]+)\1\]\}contextKeycurrentValuematcherfilteredValue"key"filteredfullMatch"Replacing {} from config filter {}"Replacing {} from config filter {}"Error while matching and filtering configFilter: {} and key: {}"Error while matching and filtering configFilter: {} and key: {}ChannelType[]SinkType[]SourceType[]ConfigFilterType[]srciternewContextMaptempchannelSetchannelNamechannelContextchTypeconfigSpecified"OTHER"OTHEREnum<ChannelType>Comparable<ChannelType>compareTo(org.apache.flume.conf.channel.ChannelType)EnumDesc<ChannelType>DynamicConstantDesc<ChannelType>Optional<EnumDesc<ChannelType>>Class<ChannelType>Enum<ChannelType>(java.lang.String,int)"Created channel {}"Created channel {}"Could not configure channel {} due to: {}"Could not configure channel {} due to: {}"Agent configuration for '{}' has no configfilters."Agent configuration for '{}' has no configfilters.configFilterNameconfigFilterContextEnum<ConfigFilterType>Comparable<ConfigFilterType>compareTo(org.apache.flume.conf.configfilter.ConfigFilterType)EnumDesc<ConfigFilterType>DynamicConstantDesc<ConfigFilterType>Optional<EnumDesc<ConfigFilterType>>Class<ConfigFilterType>Enum<ConfigFilterType>(java.lang.String,int)"Created configfilter {}"Created configfilter {}"Could not configure configfilter {} due to: {}"Could not configure configfilter {} due to: {}"Configuration empty for: {}. Removed."Configuration empty for: {}. Removed.srcConftempsourceSet"Agent configuration for '{}' has no sources."Agent configuration for '{}' has no sources.sourceNamesrcContextEnum<SourceType>Comparable<SourceType>compareTo(org.apache.flume.conf.source.SourceType)EnumDesc<SourceType>DynamicConstantDesc<SourceType>Optional<EnumDesc<SourceType>>Class<SourceType>Enum<SourceType>(java.lang.String,int)"No Channels configured for "No Channels configured for "Could not configure source  {} due to: {}"Could not configure source  {} due to: {}"Configuration empty for: {}.Removed."Configuration empty for: {}.Removed.sinkConftempSinkset"Agent configuration for '{}' has no sinks."Agent configuration for '{}' has no sinks.sinkNamesinkContext"no context for sink{}"no context for sink{}sinkTypeEnum<SinkType>Comparable<SinkType>compareTo(org.apache.flume.conf.sink.SinkType)EnumDesc<SinkType>DynamicConstantDesc<SinkType>Optional<EnumDesc<SinkType>>Class<SinkType>Enum<SinkType>(java.lang.String,int)"Creating sink: {} using {}"Creating sink: {} using {}"Channel "Channel " not in active set." not in active set."Could not configure sink  {} due to: {}"Could not configure sink  {} due to: {}usedSinkssinkgroupNamegroupSinks"sinkgroup"sinkgroupsinkArrayArrayList<String>AbstractList<String>ArrayList<String>()addLast(java.lang.String)addFirst(java.lang.String)ListIterator<String>add(int,java.lang.String)set(int,java.lang.String)Comparator<? super String>UnaryOperator<String>Function<String,String>AbstractList<String>()ArrayList<String>(java.util.Collection)ArrayList<String>(int)"No available sinks for sinkgroup: "No available sinks for sinkgroup: ". Sinkgroup will be removed". Sinkgroup will be removed"Could not configure sink group {} due to: {}"Could not configure sink group {} due to: {}"Configuration error for: {}.Removed."Configuration error for: {}.Removed.groupConfsinkItcurSink"Agent configuration for '{}' sinkgroup '{}' sink '{}' in use by another group: " +
                  "'{}', sink not added"Agent configuration for '{}' sinkgroup '{}' sink '{}' in use by another group: '{}', sink not added"Agent configuration for '{}' sinkgroup '{}' sink not found: '{}', " +
                  " sink not added"Agent configuration for '{}' sinkgroup '{}' sink not found: '{}',  sink not addeddelim"AgentConfiguration["AgentConfiguration["CONFIG_FILTERS: "CONFIG_FILTERS: "SOURCES: "SOURCES: "CHANNELS: "CHANNELS: "SINKS: "SINKS: "AgentConfiguration created without Configuration stubs " +
              "for which only basic syntactical validation was performed["AgentConfiguration created without Configuration stubs for which only basic syntactical validation was performed["AgentConfiguration created with Configuration stubs " +
            "for which full validation was performed["AgentConfiguration created with Configuration stubs for which full validation was performed["Duplicate configfilter list specified for agent: {}"Duplicate configfilter list specified for agent: {}"Duplicate source list specified for agent: {}"Duplicate source list specified for agent: {}"Added sinks: {} Agent: {}"Added sinks: {} Agent: {}"Duplicate sink list specfied for agent: {}"Duplicate sink list specfied for agent: {}"Duplicate channel list specified for agent: {}"Duplicate channel list specified for agent: {}"Duplicate sinkgroup list specfied for agent: {}"Duplicate sinkgroup list specfied for agent: {}"Invalid property specified: {}"Invalid property specified: {}configPrefixcontextMapparsed"Processing:{}"Processing:{}"Created context for {}: {}"Created context for {}: {}ComponentNameAndConfigKeyComponentNameAndConfigKey(java.lang.String,java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/FlumeConfiguration$ComponentNameAndConfigKey.classFlumeConfiguration is an in memory representation of the hierarchicalconfiguration namespace required by the ConfigurationProvider.This class is instantiated with a map or properties object which is parsedto construct the hierarchy in memory. Once the entire set of properties havebeen parsed and populated, a validation routine is run that identifies andremoves invalid components.org.apache.flume.node.ConfigurationProviderCreates a populated Flume Configuration object.@deprecatedplease use the other constructor Construct the in-memory component hierarchy Now iterate thru the agentContext and create agent configs and add them to agentConfigMap validate and remove improperly configured components Null names and values not supported Remove leading and trailing spaces Empty values are not supported All configuration keys must have a prefix defined as agent name Agent name must be specified for all properties Configuration key must be specified for every property Each configuration key must begin with one of the three prefixes: sources, sinks, or channels.Checks the validity of the agent configuration. This method assumes thatall necessary configuration keys have been populated and are ready forvalidation.During the validation process, the components with invalid configurationwill be dropped. If at the end of this process, the minimum necessarycomponents are not available, the configuration itself will be consideredinvalid.true if the configuration is valid, false otherwise Make sure that at least one channel is specified If no sources or sinks are present, then this is invalid Now rewrite the sources/sinks/channelsJAVA EL expression style ${myFilterName['my_key']} orJAVA EL expression style ${myFilterName["my_key"]} orJAVA EL expression style ${myFilterName[my_key]} ${<filterComponentName> delimiter :'," or nothing key matching delimiterIf it is a known component it will do the full validation required forthat component, else it will do the validation required for that class.The logic for the following code:Is it a known component?-Yes: Get the ChannelType and set the string name of that toconfig and set configSpecified to true.-No.Look for config type for the given component:-Config Found:Set config to the type mentioned, set configSpecified to true-No Config found:Set config to OTHER, configSpecified to false,do basic validation. Leave the context in thecontextMap to process later. Setting it to other returnsa vanilla configuration(Source/Sink/Channel Configuration),which does basic syntactic validation. This object is notput into the map, so the context is retained which can bepicked up - this is meant for older classes which don'timplement ConfigurableComponent. Context exists in map. Get the configuration object for the channel: Not a known channel - cannot do specific validation to this channel Could not configure channel - skip it. No need to add to error list - already added before exception is thrownArrays.split() call will throw NPE if the sources string is empty-Yes: Get the SourceType and set the string name of that to Possible reason the configuration can fail here: Old component is configured directly using Context validateComponent(sourceSet, sourceConfigMap, CLASS_SOURCE, ATTR_TYPE, ATTR_CHANNELS); Preconditions.checkArgument(channelSet != null && channelSet.size() > 0);-Yes: Get the SinkType and set the string name of that to Filter out any sinks that have invalid channel validateComponent(sinkSet, sinkConfigMap, CLASS_SINK, ATTR_TYPE, ATTR_CHANNEL);Validates that each group has at least one sink, blocking other groupsfrom acquiring itSet of valid sinksSet of valid sinkgroupsCheck availability of sinks for group[in]Existing valid sinks[in/out]Sinks already in use by other groups[in]sinkgroup configurationList of sinks available and reserved for group Check for configFilters Check for sources Check for sinks Check for channels Check for sinkgroups key must start with prefix key must have a component name part after the prefix of the format: <prefix><component-name>.<config-key> name and config key must be non-empty/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/FlumeConfigurationError.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/FlumeConfigurationError.classErrorOrWarningErrorOrWarning()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/FlumeConfigurationError$ErrorOrWarning.classComponent which had an error, specific key in error(which can be null)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/FlumeConfigurationErrorType.javaFlumeConfigurationErrorTypeFlumeConfigurationErrorType(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/FlumeConfigurationErrorType.class"Agent configuration is invalid."Agent configuration is invalid."Property needs a name."Property needs a name."Property value missing."Property value missing."Agent name is required."Agent name is required."Configuration Key is invalid."Configuration Key is invalid."Property already configured."Property already configured."No such property."No such property."This property is part of another group."This property is part of another group."Required attributes missing."Required attributes missing."This attribute name is invalid."This attribute name is invalid."Value in configuration is invalid for this key, assigned default value."Value in configuration is invalid for this key, assigned default value."Configuration of component failed."Configuration of component failed./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/LogPrivacyUtil.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/LogPrivacyUtil.classClass<LogPrivacyUtil>"org.apache.flume.log.rawdata"org.apache.flume.log.rawdata"org.apache.flume.log.printconfig"org.apache.flume.log.printconfig"Logging of configuration details of the agent has been turned on by " +
          "setting {} to true. Please use this setting with extra caution as it may result " +
          "in logging of private data. This setting is not recommended in " +
          "production environments."Logging of configuration details of the agent has been turned on by setting {} to true. Please use this setting with extra caution as it may result in logging of private data. This setting is not recommended in production environments."Logging of configuration details is disabled. To see configuration details " +
          "in the log run the agent with -D{}=true JVM " +
          "argument. Please note that this is not recommended in production " +
          "systems as it may leak private information to the logfile."Logging of configuration details is disabled. To see configuration details in the log run the agent with -D{}=true JVM argument. Please note that this is not recommended in production systems as it may leak private information to the logfile."Logging raw data has been turned on by setting {} to true. Please use it with " +
          "extra caution as it may result in logging of potentially sensitive user data. " +
          "This setting is not recommended in production environments."Logging raw data has been turned on by setting {} to true. Please use it with extra caution as it may result in logging of potentially sensitive user data. This setting is not recommended in production environments.Utility class to help any Flume component determine whether logging potentially sensitiveinformation is allowed or not.<p/>InterfaceAudience.Public<br/>InterfaceStability.Evolvingsystem property name to enable logging of potentially sensitive user datasystem property name to enable logging of information related to the validation of agentconfiguration at startup.Tells whether logging of configuration details - including secrets - is allowed or not. Thisis driven by a system property defined by LOG_PRINTCONFIG_PROPtrue only if logging is allowedTells whether logging of potentially sensitive user data is allowed or not. Thisis driven by a system property defined by LOG_RAWDATA_PROP/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/channel/ChannelConfiguration.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/channelorg.apache.flume.conf.channelChannelConfigurationTypeChannelConfigurationType(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/channel/ChannelConfiguration$ChannelConfigurationType.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/channelchannelConfigurationType"org.apache.flume.conf.channel.MemoryChannelConfiguration"org.apache.flume.conf.channel.MemoryChannelConfiguration"org.apache.flume.conf.channel.FileChannelConfiguration"org.apache.flume.conf.channel.FileChannelConfiguration"org.apache.flume.conf.channel.JdbcChannelConfiguration"org.apache.flume.conf.channel.JdbcChannelConfiguration"org.apache.flume.conf.channel.SpillableMemoryChannelConfiguration"org.apache.flume.conf.channel.SpillableMemoryChannelConfiguration? extends ChannelConfigurationClass<? extends ChannelConfiguration>clazzinstanceConstructor<? extends ChannelConfiguration>TypeVariable<Constructor<? extends ChannelConfiguration>>TypeVariable<Constructor<? extends ChannelConfiguration>>[]Constructor<? extends ChannelConfiguration>(java.lang.Class,java.lang.Class[],java.lang.Class[],int,int,java.lang.String,byte[],byte[])Map<String,? extends ChannelConfiguration>ChannelConfiguration[]TypeVariable<Class<? extends ChannelConfiguration>>TypeVariable<Class<? extends ChannelConfiguration>>[]File channelJDBC channel provided by org.apache.flume.channel.jdbc.JdbcChannelSpillable Memory channel Could not find the configuration stub, do basic validation Let the caller know that this was created because of this exception./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/channel/ChannelSelectorConfiguration.javaEnum<ChannelSelectorType>Comparable<ChannelSelectorType>compareTo(org.apache.flume.conf.channel.ChannelSelectorType)EnumDesc<ChannelSelectorType>DynamicConstantDesc<ChannelSelectorType>Optional<EnumDesc<ChannelSelectorType>>Class<ChannelSelectorType>Enum<ChannelSelectorType>(java.lang.String,int)channelNamesChannelSelectorConfigurationTypeChannelSelectorConfigurationType(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/channel/ChannelSelectorConfiguration$ChannelSelectorConfigurationType.classselectorType"org.apache.flume.conf.channel." +
            "MultiplexingChannelSelectorConfiguration"org.apache.flume.conf.channel.MultiplexingChannelSelectorConfiguration? extends ChannelSelectorConfigurationClass<? extends ChannelSelectorConfiguration>Constructor<? extends ChannelSelectorConfiguration>TypeVariable<Constructor<? extends ChannelSelectorConfiguration>>TypeVariable<Constructor<? extends ChannelSelectorConfiguration>>[]Constructor<? extends ChannelSelectorConfiguration>(java.lang.Class,java.lang.Class[],java.lang.Class[],int,int,java.lang.String,byte[],byte[])Map<String,? extends ChannelSelectorConfiguration>ChannelSelectorConfiguration[]TypeVariable<Class<? extends ChannelSelectorConfiguration>>TypeVariable<Class<? extends ChannelSelectorConfiguration>>[]"Configuration error!"Configuration error! unless it is set to some other type Components where it is null, no configuration is necessary./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/channel/ChannelSelectorType.javaChannelSelectorTypeChannelSelectorType(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/channel/ChannelSelectorType.classchannelSelectorClassName"org.apache.flume.channel.ReplicatingChannelSelector"org.apache.flume.channel.ReplicatingChannelSelector"org.apache.flume.channel.LoadBalancingChannelSelector"org.apache.flume.channel.LoadBalancingChannelSelector"org.apache.flume.channel.MultiplexingChannelSelector"org.apache.flume.channel.MultiplexingChannelSelectorEnumeration of built in channel selector types available in the system.Place holder for custom channel selectors not part of this enumeration.Replicating channel selector.Load balancing channel selector.Multiplexing channel selector./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/channel/ChannelType.javaChannelTypeChannelType(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/channel/ChannelType.classchannelClassName"org.apache.flume.channel.file.FileChannel"org.apache.flume.channel.file.FileChannel"org.apache.flume.channel.MemoryChannel"org.apache.flume.channel.MemoryChannel"org.apache.flume.channel.jdbc.JdbcChannel"org.apache.flume.channel.jdbc.JdbcChannel"org.apache.flume.channel.SpillableMemoryChannel"org.apache.flume.channel.SpillableMemoryChannelEnumeration of built in channel types available in the system.Place holder for custom channels not part of this enumeration.FileChannelMemory channelMemoryChannelSpillableMemoryChannel/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/configfilter/ConfigFilterConfiguration.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/configfilterorg.apache.flume.conf.configfilterConfigFilterConfigurationTypeConfigFilterConfigurationType(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/configfilter/ConfigFilterConfiguration$ConfigFilterConfigurationType.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/configfilterconfigurationName"org.apache.flume.conf.configfilter.EnvironmentVariableConfigFilterConfiguration"org.apache.flume.conf.configfilter.EnvironmentVariableConfigFilterConfiguration"org.apache.flume.conf.configfilter.HadoopCredentialStoreConfigFilterConfiguration"org.apache.flume.conf.configfilter.HadoopCredentialStoreConfigFilterConfiguration"org.apache.flume.conf.configfilter.ExternalProcessConfigFilterConfiguration"org.apache.flume.conf.configfilter.ExternalProcessConfigFilterConfiguration? extends ConfigFilterConfigurationClass<? extends ConfigFilterConfiguration>Constructor<? extends ConfigFilterConfiguration>TypeVariable<Constructor<? extends ConfigFilterConfiguration>>TypeVariable<Constructor<? extends ConfigFilterConfiguration>>[]Constructor<? extends ConfigFilterConfiguration>(java.lang.Class,java.lang.Class[],java.lang.Class[],int,int,java.lang.String,byte[],byte[])Map<String,? extends ConfigFilterConfiguration>ConfigFilterConfiguration[]TypeVariable<Class<? extends ConfigFilterConfiguration>>TypeVariable<Class<? extends ConfigFilterConfiguration>>[]"Couldn't create configuration"Couldn't create configuration/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/configfilter/ConfigFilterType.javaConfigFilterTypeConfigFilterType(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/configfilter/ConfigFilterType.classclassName"org.apache.flume.configfilter.EnvironmentVariableConfigFilter"org.apache.flume.configfilter.EnvironmentVariableConfigFilter"org.apache.flume.configfilter.HadoopCredentialStoreConfigFilter"org.apache.flume.configfilter.HadoopCredentialStoreConfigFilter"org.apache.flume.configfilter.ExternalProcessConfigFilter"org.apache.flume.configfilter.ExternalProcessConfigFilter/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/sink/SinkConfiguration.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/sinkorg.apache.flume.conf.sink"No channel configured for sink: "No channel configured for sink: basicStr"CHANNEL:"CHANNEL:SinkConfigurationTypeSinkConfigurationType(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/sink/SinkConfiguration$SinkConfigurationType.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/sinksinkConfigurationName"org.apache.flume.conf.sink.NullSinkConfiguration"org.apache.flume.conf.sink.NullSinkConfiguration"org.apache.flume.conf.sink.RollingFileSinkConfiguration"org.apache.flume.conf.sink.RollingFileSinkConfiguration"org.apache.flume.conf.sink.HDFSSinkConfiguration"org.apache.flume.conf.sink.HDFSSinkConfiguration"org.apache.flume.conf.sink.IRCSinkConfiguration"org.apache.flume.conf.sink.IRCSinkConfiguration"org.apache.flume.conf.sink.AvroSinkConfiguration"org.apache.flume.conf.sink.AvroSinkConfiguration"org.apache.flume.conf.sink.ThriftSinkConfiguration"org.apache.flume.conf.sink.ThriftSinkConfiguration"org.apache.flume.sink.elasticsearch.ElasticSearchSinkConfiguration"org.apache.flume.sink.elasticsearch.ElasticSearchSinkConfiguration"org.apache.flume.sink.hbase.HBaseSinkConfiguration"org.apache.flume.sink.hbase.HBaseSinkConfiguration"org.apache.flume.sink.hbase2.HBase2SinkConfiguration"org.apache.flume.sink.hbase2.HBase2SinkConfiguration"org.apache.flume.sink.solr.morphline" +
      ".MorphlineSolrSinkConfiguration"org.apache.flume.sink.solr.morphline.MorphlineSolrSinkConfiguration"org.apache.flume.sink.hive.HiveSinkConfiguration"org.apache.flume.sink.hive.HiveSinkConfiguration"org.apache.flume.sink.http.HttpSinkConfiguration"org.apache.flume.sink.http.HttpSinkConfiguration? extends SinkConfigurationClass<? extends SinkConfiguration>Constructor<? extends SinkConfiguration>TypeVariable<Constructor<? extends SinkConfiguration>>TypeVariable<Constructor<? extends SinkConfiguration>>[]Constructor<? extends SinkConfiguration>(java.lang.Class,java.lang.Class[],java.lang.Class[],int,int,java.lang.String,byte[],byte[])Map<String,? extends SinkConfiguration>SinkConfiguration[]TypeVariable<Class<? extends SinkConfiguration>>TypeVariable<Class<? extends SinkConfiguration>>[]Place holder for custom sinks not part of this enumeration.Null sinkNullSinkLogger sinkLoggerSinkRolling file sinkRollingFileSinkHDFS Sink provided by org.apache.flume.sink.hdfs.HDFSEventSinkIRC Sink provided by org.apache.flume.sink.irc.IRCSinkAvro sinkAvroSinkThrift sinkThriftSinkElasticSearch Sinkorg.apache.flume.sink.elasticsearch.ElasticSearchSinkHBase Sinkorg.apache.flume.sink.hbase.HBaseSinkAsyncHBase Sinkorg.apache.flume.sink.hbase.AsyncHBaseSinkHBase2 sinkorg.apache.flume.sink.hbase2.HBase2SinkMorphlineSolr sinkorg.apache.flume.sink.solr.morphline.MorphlineSolrSinkHive Sinkorg.apache.flume.sink.hive.HiveSinkHTTP Sinkorg.apache.flume.sink.http.HttpSink/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/sink/SinkGroupConfiguration.javagetKnownSinkProcessorgetKnownSinkProcessor(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/sink/SinkGroupConfiguration.classprocessorConfprocessorContextparamsspType"-processor"-processorEnum<SinkProcessorType>Comparable<SinkProcessorType>compareTo(org.apache.flume.conf.sink.SinkProcessorType)EnumDesc<SinkProcessorType>DynamicConstantDesc<SinkProcessorType>Optional<EnumDesc<SinkProcessorType>>Class<SinkProcessorType>Enum<SinkProcessorType>(java.lang.String,int)SinkProcessorType[]sinkProcessClassName/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/sink/SinkProcessorConfiguration.java"default"SinkProcessorConfigurationTypeSinkProcessorConfigurationType(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/sink/SinkProcessorConfiguration$SinkProcessorConfigurationType.classprocessorClassName"org.apache.flume.conf.sink.LoadBalancingSinkProcessorConfiguration"org.apache.flume.conf.sink.LoadBalancingSinkProcessorConfiguration"org.apache.flume.conf.sink.FailoverSinkProcessorConfiguration"org.apache.flume.conf.sink.FailoverSinkProcessorConfiguration? extends SinkProcessorConfigurationClass<? extends SinkProcessorConfiguration>Constructor<? extends SinkProcessorConfiguration>TypeVariable<Constructor<? extends SinkProcessorConfiguration>>TypeVariable<Constructor<? extends SinkProcessorConfiguration>>[]Constructor<? extends SinkProcessorConfiguration>(java.lang.Class,java.lang.Class[],java.lang.Class[],int,int,java.lang.String,byte[],byte[])Map<String,? extends SinkProcessorConfiguration>SinkProcessorConfiguration[]TypeVariable<Class<? extends SinkProcessorConfiguration>>TypeVariable<Class<? extends SinkProcessorConfiguration>>[]"Could not instantiate configuration!"Could not instantiate configuration!Load balancing channel selectorFailover processorFailoverSinkProcessorStandard processorDefaultSinkProcessor/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/sink/SinkProcessorType.javaSinkProcessorTypeSinkProcessorType(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/sink/SinkProcessorType.class"org.apache.flume.sink.FailoverSinkProcessor"org.apache.flume.sink.FailoverSinkProcessor"org.apache.flume.sink.DefaultSinkProcessor"org.apache.flume.sink.DefaultSinkProcessor"org.apache.flume.sink.LoadBalancingSinkProcessor"org.apache.flume.sink.LoadBalancingSinkProcessorLoad balancing processor/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/sink/SinkType.javaSinkTypeSinkType(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/sink/SinkType.classsinkClassName"org.apache.flume.sink.NullSink"org.apache.flume.sink.NullSink"org.apache.flume.sink.LoggerSink"org.apache.flume.sink.LoggerSink"org.apache.flume.sink.RollingFileSink"org.apache.flume.sink.RollingFileSink"org.apache.flume.sink.hdfs.HDFSEventSink"org.apache.flume.sink.hdfs.HDFSEventSink"org.apache.flume.sink.irc.IRCSink"org.apache.flume.sink.irc.IRCSink"org.apache.flume.sink.AvroSink"org.apache.flume.sink.AvroSink"org.apache.flume.sink.ThriftSink"org.apache.flume.sink.ThriftSink"org.apache.flume.sink.elasticsearch.ElasticSearchSink""org.apache.flume.sink.hbase.HBaseSink""org.apache.flume.sink.hbase.AsyncHBaseSink""org.apache.flume.sink.hbase2.HBase2Sink""org.apache.flume.sink.solr.morphline.MorphlineSolrSink""org.apache.flume.sink.hive.HiveSink""org.apache.flume.sink.http.HttpSink"Enumeration of built in sink types available in the system.ElasticSearch sinkHBase sinkAsyncHBase sink/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/source/SourceConfiguration.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/sourceorg.apache.flume.conf.sourcegetKnownChannelSelectorgetKnownChannelSelector(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/source/SourceConfiguration.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/sourcechannelListselectorParamsselTypeselectorContext"No channels set for "No channels set for Enum<ChannelSelectorConfigurationType>Comparable<ChannelSelectorConfigurationType>compareTo(org.apache.flume.conf.channel.ChannelSelectorConfiguration.ChannelSelectorConfigurationType)EnumDesc<ChannelSelectorConfigurationType>DynamicConstantDesc<ChannelSelectorConfigurationType>Optional<EnumDesc<ChannelSelectorConfigurationType>>Class<ChannelSelectorConfigurationType>Enum<ChannelSelectorConfigurationType>(java.lang.String,int)"Failed to configure component!"Failed to configure component!"CHANNELS:"CHANNELS:ChannelSelectorType[]clNameSourceConfigurationTypeSourceConfigurationType(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/source/SourceConfiguration$SourceConfigurationType.classsrcConfigurationName"org.apache.flume.conf.source.NetcatSourceConfiguration"org.apache.flume.conf.source.NetcatSourceConfiguration"org.apache.flume.conf.source.ExecSourceConfiguration"org.apache.flume.conf.source.ExecSourceConfiguration"org.apache.flume.conf.source.AvroSourceConfiguration"org.apache.flume.conf.source.AvroSourceConfiguration"org.apache.flume.conf.source.SyslogTcpSourceConfiguration"org.apache.flume.conf.source.SyslogTcpSourceConfiguration"org.apache.flume.conf.source.SyslogUDPSourceConfiguration"org.apache.flume.conf.source.SyslogUDPSourceConfiguration"org.apache.flume.source.MultiportSyslogTCPSourceConfiguration"org.apache.flume.source.MultiportSyslogTCPSourceConfiguration"org.apache.flume.conf.source.SpoolDirectorySourceConfiguration"org.apache.flume.conf.source.SpoolDirectorySourceConfiguration"org.apache.flume.source.http.HTTPSourceConfiguration"org.apache.flume.source.http.HTTPSourceConfiguration"org.apache.flume.source.http.ThriftSourceConfiguration"org.apache.flume.source.http.ThriftSourceConfiguration"org.apache.flume.conf.source.jms.JMSSourceConfiguration"org.apache.flume.conf.source.jms.JMSSourceConfiguration"org.apache.flume.source.taildir.TaildirSourceConfiguration"org.apache.flume.source.taildir.TaildirSourceConfiguration"org.apache.flume.conf.source.NetcatUdpSourceConfiguration"org.apache.flume.conf.source.NetcatUdpSourceConfiguration? extends SourceConfigurationClass<? extends SourceConfiguration>Constructor<? extends SourceConfiguration>TypeVariable<Constructor<? extends SourceConfiguration>>TypeVariable<Constructor<? extends SourceConfiguration>>[]Constructor<? extends SourceConfiguration>(java.lang.Class,java.lang.Class[],java.lang.Class[],int,int,java.lang.String,byte[],byte[])Map<String,? extends SourceConfiguration>SourceConfiguration[]TypeVariable<Class<? extends SourceConfiguration>>TypeVariable<Class<? extends SourceConfiguration>>[]"Error creating configuration"Error creating configurationNetcat source.NetcatSourceExec source.ExecSourceAvro source.AvroSourceSyslog Tcp Sourceorg.apache.flume.source.SyslogTcpSourceSyslog Udp Sourceorg.apache.flume.source.SyslogUDPSourceMultiport Syslog TCP Sourceorg.apache.flume.source.MultiportSyslogTCPSourceSpool directory sourceorg.apache.flume.source.SpoolDirectorySourceHTTP Sourceorg.apache.flume.source.http.HTTPSourceorg.apache.flume.source.ThriftSourceJMS Sourceorg.apache.flume.source.jms.JMSSourceTAILDIR Sourceorg.apache.flume.source.taildir.TaildirSourceNetcat UDP Sourceorg.apache.flume.source.NetcatUdpSource/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/src/main/java/org/apache/flume/conf/source/SourceType.javaSourceTypeSourceType(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-configuration/target/classes/org/apache/flume/conf/source/SourceType.classsourceClassName"org.apache.flume.source.SequenceGeneratorSource"org.apache.flume.source.SequenceGeneratorSource"org.apache.flume.source.NetcatSource"org.apache.flume.source.NetcatSource"org.apache.flume.source.ExecSource"org.apache.flume.source.ExecSource"org.apache.flume.source.AvroSource"org.apache.flume.source.AvroSource"org.apache.flume.source.SyslogTcpSource""org.apache.flume.source.MultiportSyslogTCPSource""org.apache.flume.source.SyslogUDPSource""org.apache.flume.source.SpoolDirectorySource""org.apache.flume.source.http.HTTPSource""org.apache.flume.source.ThriftSource""org.apache.flume.source.jms.JMSSource""org.apache.flume.source.taildir.TaildirSource""org.apache.flume.source.NetcatUdpSource"Enumeration of built in source types available in the system.Place holder for custom sources not part of this enumeration.Sequence generator file source.SyslogTcpSourceMultiportSyslogTCPSourceSyslogUDPSourceThrift SourceTaildir Source/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-coreFlume NG Core115121org.apache.flume.corepluginManagementorg.eclipse.m2elifecycle-mapping${lifecycle-mapping.version}lifecycleMappingMetadatapluginExecutionspluginExecutionpluginExecutionFilteravro-maven-pluginversionRange[${avro.version},)idl-protocolexecutecom.thoughtworks.paranamerparanamer-maven-plugin[${mvn-paranamer-plugin.version},)generateignoremaven-antrun-plugin[${mvn-antrun-plugin.version},)sourceDirectory${project.build.directory}/generated-sources/avro${project.build.directory}/classesprofilesprofilenot-windowsactivationosfamily!Windows${mvn-antrun-plugin.version}generate-versionmkdir${project.build.directory}/generated-sources/javaexecexecutableshargline${basedir}/scripts/saveVersion.sh ${project.version} ${project.build.directory}org.codehaus.mojobuild-helper-maven-plugin${mvn-build-helper-plugin}add-sourcesourcetarget/generated-sources/javawindowsWindows${project.build.directory}/generated-sources/java/org/apache/flumepowershell-executionpolicy unrestricted -file ${basedir}\scripts\saveVersion.ps1  ${project.version} ${project.build.directory}testscommons-iocommons-codeccom.jcraftjzliblog4j-corecommons-cliavroavro-ipc-nettyio.nettynetty-alljoda-timeorg.eclipse.jettyjetty-servletjetty-utiljetty-serverjetty-jmxcom.google.code.gsongsonorg.apache.httpcomponentshttpclienthttpcoreorg.apache.thriftlibthriftorg.apache.minamina-coreio.prometheussimpleclientsimpleclient_servlet/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/Channel.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/srcA channel connects a {@link Source} to a {@link Sink}. The sourceacts as producer while the sink acts as a consumer of events. The channelitself is the buffer between the two.A channel exposes a {@link Transaction} interface that can be used byits clients to ensure atomic {@linkplain #put(Event) put} and{@linkplain #take() take} semantics.This is necessary to guarantee single hop reliability between agents.For instance, a source will successfully produce an {@linkplain Event event}if and only if that event can be committed to the source's associatedchannel. Similarly, a sink will consume an event if andonly if its respective endpoint can accept the event. Theextent of transaction support varies for different channel implementationsranging from strong to best-effort semantics.Channels are associated with unique {@linkplain NamedComponent names} thatcan be used for separating configuration and working namespaces.Channels must be thread safe, protecting any internal invariants as noguarantees are given as to when and by how many sources/sinks they maybe simultaneously accessed by.org.apache.flume.Sourceorg.apache.flume.Sinkorg.apache.flume.Transaction<p>Puts the given event into the channel.</p><p><strong>Note</strong>: This method must be invoked within an active{@link Transaction} boundary. Failure to do so can lead to unpredictableresults.</p>the event to transport.ChannelExceptionin case this operation fails.org.apache.flume.Transaction#begin()<p>Returns the next event from the channel if available. If the channeldoes not have any events available, this method must return {@code null}.the next available event or {@code null} if no events areavailable.the transaction instance associated with this channel./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/ChannelException.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/ChannelException.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target<p>A channel exception is raised whenever a channel operation fails.</p>the exception messagethe causal exception/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/ChannelFactory.java? extends ChannelClass<? extends Channel>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/ChannelFullException.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/ChannelFullException.class8098141359417449525L8098141359417449525/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/ChannelSelector.javaList<Channel>SequencedCollection<Channel>Collection<Channel>Iterable<Channel>Allows the selection of a subset of channels from the given set based onits implementation policy. Different implementations of this interfaceembody different policies that affect the choice of channels that a sourcewill push the incoming events to.all channels the selector could select from.Returns a list of required channels. A failure in writing the event tothese channels must be communicated back to the source that received thisevent.the list of required channels that this selector has selected forthe given event.Returns a list of optional channels. A failure in writing the event tothese channels must be ignored.the list of optional channels that this selector has selected forthe list of all channels that this selector is configured to workwith./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/Clock.javaFacade for System.currentTimeMillis for Testing/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/Constants.javaConstantsConstants()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/Constants.class"flume.called.from.service"flume.called.from.serviceDisables the fail-fast startup behavior. This would be used in thescenario where the agent is expected to start, but the configfile be populated at a later point in time./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/CounterGroup.javaHashMap<String,AtomicLong>AbstractMap<String,AtomicLong>Map<String,AtomicLong>counters/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/CounterGroup.classHashMap<String,AtomicLong>()? super AtomicLong? extends AtomicLongBiFunction<? super AtomicLong,? super AtomicLong,? extends AtomicLong>merge(java.lang.String,java.util.concurrent.atomic.AtomicLong,java.util.function.BiFunction)BiFunction<? super String,? super AtomicLong,? extends AtomicLong>Function<? super String,? extends AtomicLong>replace(java.lang.String,java.util.concurrent.atomic.AtomicLong)replace(java.lang.String,java.util.concurrent.atomic.AtomicLong,java.util.concurrent.atomic.AtomicLong)putIfAbsent(java.lang.String,java.util.concurrent.atomic.AtomicLong)BiConsumer<? super String,? super AtomicLong>getOrDefault(java.lang.Object,java.util.concurrent.atomic.AtomicLong)Entry<String,AtomicLong>Set<Entry<String,AtomicLong>>Collection<Entry<String,AtomicLong>>Iterable<Entry<String,AtomicLong>>Collection<AtomicLong>Iterable<AtomicLong>Map<? extends String,? extends AtomicLong>put(java.lang.String,java.util.concurrent.atomic.AtomicLong)AbstractMap<String,AtomicLong>()Node<String,AtomicLong>TreeNode<String,AtomicLong>newTreeNode(int,java.lang.String,java.util.concurrent.atomic.AtomicLong,java.util.HashMap.Node)newNode(int,java.lang.String,java.util.concurrent.atomic.AtomicLong,java.util.HashMap.Node)Node<String,AtomicLong>[]putVal(int,java.lang.String,java.util.concurrent.atomic.AtomicLong,boolean,boolean)HashMap<String,AtomicLong>(java.util.Map)HashMap<String,AtomicLong>(int)HashMap<String,AtomicLong>(int,float)deltacounterGroupsetValue(java.util.concurrent.atomic.AtomicLong)"{ name:"{ name:" counters:" counters:Used for counting events, collecting metrics, etc./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/EventDrivenSource.javaA {@link Source} that does not need an external driver to poll for{@linkplain Event events} to ingest; it provides its own event-drivenmechanism to invoke event processing./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/NamedComponent.javaEnables a component to be tagged with a name so that it can be referredto uniquely within the configuration system./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/PollableSource.javaStatusStatus()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/PollableSource$Status.classA {@link Source} that requires an external driver to poll to determinewhether there are {@linkplain Event events} that are available to ingestfrom the source.org.apache.flume.source.EventDrivenSourceRunnerAttempt to pull an item from the source, sending it to the channel.When driven by an {@link EventDrivenSourceRunner} process is guaranteedto be called only by a single thread at a time, with no concurrency.Any other mechanism driving a pollable source must follow the samesemantics.{@code READY} if one or more events were created from the source.{@code BACKOFF} if no events could be created from the source.EventDeliveryExceptionIf there was a failure in delivering tothe attached channel, or if a failure occurred in acquiring data fromthe source./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/Sink.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/Sink$Status.classA sink is connected to a <tt>Channel</tt> and consumes its contents,sending them to a configured destination that may vary according tothe sink type.Sinks can be grouped together for various behaviors using <tt>SinkGroup</tt>and <tt>SinkProcessor</tt>. They are polled periodically by a<tt>SinkRunner</tt> via the processor</p>Sinks are associated with unique names that can be used for separatingconfiguration and working namespaces.While the {@link Sink#process()} call is guaranteed to only be accessedby a single thread, other calls may be concurrently accessed and shouldthus be protected.org.apache.flume.Channelorg.apache.flume.SinkProcessororg.apache.flume.SinkRunner<p>Sets the channel the sink will consume from</p>The channel to be polledthe channel associated with this sink<p>Requests the sink to attempt to consume data from attached channel</p><p><strong>Note</strong>: This method should be consuming from the channelwithin the bounds of a Transaction. On successful delivery, the transactionshould be committed, and on failure it should be rolled back.READY if 1 or more Events were successfully delivered, BACKOFF ifno data could be retrieved from the channel feeding this sinkIn case of any kind of failure todeliver data to the next hop destination./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/SinkFactory.java? extends SinkClass<? extends Sink>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/SinkProcessor.javaList<Sink>SequencedCollection<Sink>Collection<Sink>Iterable<Sink>Interface for a device that allows abstraction of the behavior of multiplesinks, always assigned to a SinkRunnerA sink processors {@link SinkProcessor#process()} method will only beaccessed by a single runner thread. However configuration methodssuch as {@link Configurable#configure} may be concurrently accessed.org.apache.flume.sink.SinkGroup<p>Handle a request to poll the owned sinks.</p><p>The processor is expected to call {@linkplain Sink#process()} onwhatever sink(s) appropriate, handling failures as appropriate andthrowing {@link EventDeliveryException} when there is a failure todeliver any events according to the delivery policy defined by thesink processor implementation. See specific implementations of thisinterface for delivery behavior and policies.</p>Returns {@code READY} if events were successfully consumed,or {@code BACKOFF} if no events were available in the channel to consume.if the behavior guaranteed by the processorcouldn't be carried out.<p>Set all sinks to work with.</p><p>Sink specific parameters are passed to the processor via configure</p>A non-null, non-empty list of sinks to be chosen from by theprocessor/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/SinkRunner.javapolicy/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/SinkRunner.classlifecycleStaterunnerThreadrunnermaxBackoffSleepbackoffSleepIncrementClass<SinkRunner>"SinkRunner-PollingRunner-"SinkRunner-PollingRunner-? extends SinkProcessorClass<? extends SinkProcessor>Map<String,? extends SinkProcessor>SinkProcessor[]Constructor<? extends SinkProcessor>TypeVariable<Class<? extends SinkProcessor>>TypeVariable<Class<? extends SinkProcessor>>[]"Waiting for runner thread to exit"Waiting for runner thread to exit"Interrupted while waiting for runner thread to exit. Exception follows."Interrupted while waiting for runner thread to exit. Exception follows."SinkRunner: { policy:"SinkRunner: { policy:" counterGroup:" counterGroup:/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/SinkRunner$PollingRunner.classshouldStop"Polling sink runner starting"Polling sink runner startingEnum<Status>Comparable<Status>compareTo(org.apache.flume.Sink.Status)EnumDesc<Status>DynamicConstantDesc<Status>Optional<EnumDesc<Status>>Class<Status>Enum<Status>(java.lang.String,int)"runner.backoffs"runner.backoffs"runner.backoffs.consecutive"runner.backoffs.consecutive"Interrupted while processing an event. Exiting."Interrupted while processing an event. Exiting."runner.interruptions"runner.interruptions"Unable to deliver event. Exception follows."Unable to deliver event. Exception follows."runner.deliveryErrors"runner.deliveryErrors"runner.errors"runner.errors"Polling runner exiting. Metrics:{}"Polling runner exiting. Metrics:{}A driver for {@linkplain Sink sinks} that polls them, attempting to{@linkplain Sink#process() process} events if any are available in the{@link Channel}.Note that, unlike {@linkplain Source sources}, all sinks are polled.org.apache.flume.SourceRunner{@link Runnable} that {@linkplain SinkProcessor#process() polls} a{@link SinkProcessor} and manages event delivery notification,{@link Sink.Status BACKOFF} delay handling, etc./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/Source.javachannelProcessorA source generates {@plainlink Event events} and calls methods on theconfigured {@link ChannelProcessor} to persist those events into theconfigured {@linkplain Channel channels}.Sources are associated with unique {@linkplain NamedComponent names} that canbe used for separating configuration and working namespaces.No guarantees are given regarding thread safe access.Specifies which channel processor will handle this source's events.Returns the channel processor that will handle this source's events./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/SourceFactory.java? extends SourceClass<? extends Source>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/SourceRunner.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/SourceRunner.class"No known runner type for source "No known runner type for source A source runner controls how a source is driven.This is an abstract class used for instantiating derived classes.Static factory method to instantiate a source runner implementation thatcorresponds to the type of {@link Source} specified.The source to runA runner that can run the specified sourceIllegalArgumentExceptionif the specified source does not implementa supported derived interface of {@link SourceRunner}./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/SystemClock.javaDefault implementation of Clock which uses System/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/Transaction.javaTransactionStateTransactionState()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/Transaction$TransactionState.class<p>Provides the transaction boundary while accessing a channel.</p><p>A <tt>Transaction</tt> instance is used to encompass channel accessvia the following idiom:</p><pre><code>Channel ch = ...Transaction tx = ch.getTransaction();try {tx.begin();...// ch.put(event) or ch.take()tx.commit();} catch (ChannelException ex) {tx.rollback();} finally {tx.close();}</code></pre><p>Depending upon the implementation of the channel, the transactionsemantics may be strong, or best-effort only.</p>Transactions must be thread safe. To provide  a guarantee of thread safeaccess to Transactions, see {@link BasicChannelSemantics} and{@link  BasicTransactionSemantics}.<p>Starts a transaction boundary for the current channel operation. If atransaction is already in progress, this method will join that transactionusing reference counting.</p><p><strong>Note</strong>: For every invocation of this method there mustbe a corresponding invocation of {@linkplain #close()} method. Failureto ensure this can lead to dangling transactions and unpredictable results.Indicates that the transaction can be successfully committed. It isrequired that a transaction be in progress when this method is invoked.Indicates that the transaction can must be aborted. It is<p>Ends a transaction boundary for the current channel operation. If ausing reference counting. The transaction is completed only if thereare no more references left for this transaction.</p>be a corresponding invocation of {@linkplain #begin()} method. Failure/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/VersionAnnotation.javaElementType[]This class is about package attribute that capturesversion info of Flume that was compiled.Get the Flume versionthe version string "1.1"Get the subversion revision.the revision number as a string (eg. "100755")Get the branch from which this was compiled.The branch name, e.g. "trunk"Get the username that compiled Flume.Get the date when Flume was compiled.the date in unix 'date' formatGet the url for the subversion repository.Get a checksum of the source files from whichFlume was compiled.a string that uniquely identifies the source/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/annotations/Disposable.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/annotationsorg.apache.flume.annotationsRUNTIME/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/annotations/InterfaceAudience.javaInterfaceAudienceInterfaceAudience()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/annotations/InterfaceAudience.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/annotationsAnnotation to inform users of a package, class or method's intended audience.Currently the audience can be {@link Public}, {@link LimitedPrivate} or{@link Private}. <br>All public classes must have InterfaceAudience annotation. <br><ul><li>Public classes that are not marked with this annotation must beconsidered by default as {@link Private}.</li><li>External applications must only use classes that are marked{@link Public}. Avoid using non public classes as these classescould be removed or change in incompatible ways.</li><li>Flume projects must only use classes that are marked{@link LimitedPrivate} or {@link Public}</li><li> Methods may have a different annotation that it is more restrictivecompared to the audience classification of the class. Example: A classmight be {@link Public}, but a method may be {@link LimitedPrivate}</li></ul>Intended for use by any project or application.Intended only for the project(s) specified in the annotation.For example, "Common", "HDFS", "MapReduce", "ZooKeeper", "HBase".Intended for use only within Flume Audience can't exist on its own/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/annotations/InterfaceStability.javaAnnotation to inform users of how much to rely on a particular package,class or method not changing over time. Currently the stability can be{@link Stable}, {@link Evolving} or {@link Unstable}. <br><ul><li>All classes that are annotated with {@link Public} or{@link LimitedPrivate} must have InterfaceStability annotation. </li><li>Classes that are {@link Private} are to be considered unstable unlessa different InterfaceStability annotation states otherwise.</li><li>Incompatible changes must not be made to classes marked as stable.</li></ul>Can evolve while retaining compatibility for minor release boundaries.;can break compatibility only at major release (ie. at m.0).Evolving, but can break compatibility at minor release (i.e. m.x)No guarantee is provided as to reliability or stability across anylevel of release granularity./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/annotations/Recyclable.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/channel/AbstractChannel.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/channel/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/AbstractChannel.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel? extends AbstractChannelClass<? extends AbstractChannel>Map<String,? extends AbstractChannel>AbstractChannel[]Constructor<? extends AbstractChannel>TypeVariable<Class<? extends AbstractChannel>>TypeVariable<Class<? extends AbstractChannel>>[]"{name: "{name: "}"/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/channel/AbstractChannelSelector.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/AbstractChannelSelector.classMap<String,Channel>channelNameMapHashMap<String,Channel>AbstractMap<String,Channel>HashMap<String,Channel>()? super ChannelBiFunction<? super Channel,? super Channel,? extends Channel>merge(java.lang.String,org.apache.flume.Channel,java.util.function.BiFunction)BiFunction<? super String,? super Channel,? extends Channel>Function<? super String,? extends Channel>replace(java.lang.String,org.apache.flume.Channel)replace(java.lang.String,org.apache.flume.Channel,org.apache.flume.Channel)putIfAbsent(java.lang.String,org.apache.flume.Channel)BiConsumer<? super String,? super Channel>getOrDefault(java.lang.Object,org.apache.flume.Channel)Entry<String,Channel>Set<Entry<String,Channel>>Collection<Entry<String,Channel>>Iterable<Entry<String,Channel>>Map<? extends String,? extends Channel>put(java.lang.String,org.apache.flume.Channel)AbstractMap<String,Channel>()Node<String,Channel>TreeNode<String,Channel>newTreeNode(int,java.lang.String,org.apache.flume.Channel,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.flume.Channel,java.util.HashMap.Node)Node<String,Channel>[]putVal(int,java.lang.String,org.apache.flume.Channel,boolean,boolean)HashMap<String,Channel>(java.util.Map)HashMap<String,Channel>(int)HashMap<String,Channel>(int,float)chconfiguredChannelsArrayList<Channel>AbstractList<Channel>AbstractCollection<Channel>ArrayList<Channel>()Spliterator<Channel>Consumer<? super Channel>Iterator<Channel>Stream<Channel>BaseStream<Channel,Stream<Channel>>Predicate<? super Channel>Collection<? extends Channel>Iterable<? extends Channel>add(org.apache.flume.Channel)AbstractCollection<Channel>()addLast(org.apache.flume.Channel)addFirst(org.apache.flume.Channel)ListIterator<Channel>add(int,org.apache.flume.Channel)set(int,org.apache.flume.Channel)Comparator<? super Channel>UnaryOperator<Channel>Function<Channel,Channel>AbstractList<Channel>()ArrayList<Channel>(java.util.Collection)ArrayList<Channel>(int)chNames"Selector channel not found: "Selector channel not found: A map of name to channel instance.Given a list of channel names as space delimited string,returns list of channels.List of {@linkplain Channel}s represented by the names./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/channel/BasicChannelSemantics.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/BasicChannelSemantics.classThreadLocal<BasicTransactionSemantics>currentTransactionThreadLocal<BasicTransactionSemantics>()childValue(org.apache.flume.channel.BasicTransactionSemantics)createMap(java.lang.Thread,org.apache.flume.channel.BasicTransactionSemantics)setCarrierThreadLocal(org.apache.flume.channel.BasicTransactionSemantics)set(org.apache.flume.channel.BasicTransactionSemantics)transaction"No transaction exists for this thread"No transaction exists for this threadAn implementation of basic {@link Channel} semantics, including theimplied thread-local semantics of the {@link Transaction} class,which is required to extend {@link BasicTransactionSemantics}.Called upon first getTransaction() request, while synchronized onthis {@link Channel} instance.  Use this method to delay theinitializization resources until just before the firsttransaction begins.Called to create new {@link Transaction} objects, which mustextend {@link BasicTransactionSemantics}.  Each object is usedfor only one transaction, but is stored in a thread-local andretrieved by <code>getTransaction</code> for the duration of thattransaction.Ensures that a transaction exists for this thread and thendelegates the <code>put</code> to the thread's {@linkBasicTransactionSemantics} instance.delegates the <code>take</code> to the thread's {@linkInitializes the channel if it is not already, then checks to seeif there is an open transaction for this thread, creating a newone via <code>createTransaction</code> if not.the current <code>Transaction</code> object for thecalling thread/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/channel/BasicTransactionSemantics.javainitialThreadId/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/BasicTransactionSemantics.classstate"put() called from different thread than getTransaction()!"put() called from different thread than getTransaction()!"put() called when transaction is %s!"put() called when transaction is %s!"put() called with null event!"put() called with null event!"take() called from different thread than getTransaction()!"take() called from different thread than getTransaction()!"take() called when transaction is %s!"take() called when transaction is %s!"begin() called from different thread than getTransaction()!"begin() called from different thread than getTransaction()!"begin() called when transaction is "begin() called when transaction is "!"!"commit() called from different thread than getTransaction()!"commit() called from different thread than getTransaction()!"commit() called when transaction is %s!"commit() called when transaction is %s!"rollback() called from different thread than getTransaction()!"rollback() called from different thread than getTransaction()!"rollback() called when transaction is %s!"rollback() called when transaction is %s!"close() called from different thread than getTransaction()!"close() called from different thread than getTransaction()!"close() called when transaction is %s"
            + " - you must either commit or rollback first"close() called when transaction is %s - you must either commit or rollback first"BasicTransactionSemantics: {"BasicTransactionSemantics: {" state:" state:" initialThreadId:" initialThreadId:StateState()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/BasicTransactionSemantics$State.classAn implementation of basic {@link Transaction} semantics designedto work in concert with {@link BasicChannelSemantics} to simplifycreation of robust {@link Channel} implementations.  This classensures that each transaction implementation method is called onlywhile the transaction is in the correct state for that method, andonly by the thread that created the transaction.  Nested calls to<code>begin()</code> and <code>close()</code> are supported as longas they are balanced.Subclasses need only implement <code>doPut</code>,<code>doTake</code>, <code>doCommit</code>, and<code>doRollback</code>, and the developer can rest assured thatthose methods are called only after transaction state preconditionshave been properly met.  <code>doBegin</code> and<code>doClose</code> may also be implemented if there is work to bedone at those points.All InterruptedException exceptions thrown from the implementationsof the <code>doXXX</code> methods are automatically wrapped tobecome ChannelExceptions, but only after restoring the interruptedstatus of the thread so that any subsequent blocking method callswill themselves throw InterruptedException rather than blocking.The exception to this rule is <code>doTake</code>, which simplyreturns null instead of wrapping and propagating theInterruptedException, though it still first restores theinterrupted status of the thread.The method to which {@link BasicChannelSemantics} delegates callsto <code>put</code>.to <code>take</code>.the current state of the transactionThe state of the {@link Transaction} to which it belongs.<dl><dt>NEW</dt><dd>A newly created transaction that has not yet begun.</dd><dt>OPEN</dt><dd>A transaction that is open. It is permissible to commit or rollback.</dd><dt>COMPLETED</dt><dd>This transaction has been committed or rolled back. It is illegal toperform any further operations beyond closing it.</dd><dt>CLOSED</dt><dd>A closed transaction. No further operations are permitted.</dd></dl>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/channel/ChannelProcessor.javaconfigureInterceptorsconfigureInterceptors(org.apache.flume.Context)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/ChannelProcessor.classinterceptorChainClass<ChannelProcessor>List<Interceptor>SequencedCollection<Interceptor>Collection<Interceptor>Iterable<Interceptor>interceptorsLinkedList<Interceptor>AbstractSequentialList<Interceptor>AbstractList<Interceptor>AbstractCollection<Interceptor>Deque<Interceptor>Queue<Interceptor>interceptorListStr"interceptors"interceptorNamesinterceptorContexts"interceptors."interceptors.factoryinterceptorNameinterceptorContext"Type not specified for interceptor "Type not specified for interceptor "Interceptor.Type not specified for "Interceptor.Type not specified for add(org.apache.flume.interceptor.Interceptor)Spliterator<Interceptor>? super InterceptorConsumer<? super Interceptor>Iterator<Interceptor>Stream<Interceptor>BaseStream<Interceptor,Stream<Interceptor>>Predicate<? super Interceptor>? extends InterceptorCollection<? extends Interceptor>Iterable<? extends Interceptor>addLast(org.apache.flume.interceptor.Interceptor)addFirst(org.apache.flume.interceptor.Interceptor)ListIterator<Interceptor>add(int,org.apache.flume.interceptor.Interceptor)set(int,org.apache.flume.interceptor.Interceptor)Comparator<? super Interceptor>UnaryOperator<Interceptor>Function<Interceptor,Interceptor>"Builder class not found. Exception follows."Builder class not found. Exception follows."Interceptor.Builder not found."Interceptor.Builder not found."Could not instantiate Builder. Exception follows."Could not instantiate Builder. Exception follows."Interceptor.Builder not constructable."Interceptor.Builder not constructable."Unable to access Builder. Exception follows."Unable to access Builder. Exception follows."Unable to access Interceptor.Builder."Unable to access Interceptor.Builder.Map<Channel,List<Event>>reqChannelQueueLinkedHashMap<Channel,List<Event>>/modules/java.base/java/util/LinkedHashMap.classHashMap<Channel,List<Event>>AbstractMap<Channel,List<Event>>SequencedMap<Channel,List<Event>>LinkedHashMap<Channel,List<Event>>()? super List<Event>? extends List<Event>BiFunction<? super List<Event>,? super List<Event>,? extends List<Event>>merge(org.apache.flume.Channel,java.util.List,java.util.function.BiFunction)BiFunction<? super Channel,? super List<Event>,? extends List<Event>>compute(org.apache.flume.Channel,java.util.function.BiFunction)computeIfPresent(org.apache.flume.Channel,java.util.function.BiFunction)Function<? super Channel,? extends List<Event>>computeIfAbsent(org.apache.flume.Channel,java.util.function.Function)replace(org.apache.flume.Channel,java.util.List)replace(org.apache.flume.Channel,java.util.List,java.util.List)putIfAbsent(org.apache.flume.Channel,java.util.List)BiConsumer<? super Channel,? super List<Event>>getOrDefault(java.lang.Object,java.util.List)Entry<Channel,List<Event>>Set<Entry<Channel,List<Event>>>Collection<Entry<Channel,List<Event>>>Iterable<Entry<Channel,List<Event>>>Collection<List<Event>>Iterable<List<Event>>Set<Channel>Map<? extends Channel,? extends List<Event>>put(org.apache.flume.Channel,java.util.List)AbstractMap<Channel,List<Event>>()Node<Channel,List<Event>>TreeNode<Channel,List<Event>>newTreeNode(int,org.apache.flume.Channel,java.util.List,java.util.HashMap.Node)newNode(int,org.apache.flume.Channel,java.util.List,java.util.HashMap.Node)Node<Channel,List<Event>>[]putVal(int,org.apache.flume.Channel,java.util.List,boolean,boolean)HashMap<Channel,List<Event>>(java.util.Map)HashMap<Channel,List<Event>>()HashMap<Channel,List<Event>>(int)HashMap<Channel,List<Event>>(int,float)SequencedSet<Entry<K,V>>SequencedCollection<Entry<K,V>>sequencedEntrySetsequencedEntrySet()SequencedSet<Entry<Channel,List<Event>>>SequencedCollection<Entry<Channel,List<Event>>>SequencedCollection<V>sequencedValuessequencedValues()SequencedCollection<List<Event>>sequencedKeySetsequencedKeySet()SequencedSet<Channel>putLast(java.lang.Object,java.lang.Object)putLast(org.apache.flume.Channel,java.util.List)putFirst(java.lang.Object,java.lang.Object)putFirst(org.apache.flume.Channel,java.util.List)pollLastEntrypollLastEntry()pollFirstEntrypollFirstEntry()lastEntrylastEntry()firstEntryfirstEntry()LinkedHashMap<>LinkedHashMap<K,V>newLinkedHashMapnewLinkedHashMap(int)valuesToArray(java.lang.Object[],boolean)keysToArray(java.lang.Object[],boolean)K1V1Node<K1,V1>Entry<K1,V1>nseensee(java.util.HashMap.Node)removeEldestEntryremoveEldestEntry(java.util.Map.Entry)LinkedHashMapLinkedHashMap(int,float,boolean)LinkedHashMap<Channel,List<Event>>(int,float,boolean)LinkedHashMap(java.util.Map)LinkedHashMap<Channel,List<Event>>(java.util.Map)LinkedHashMap()LinkedHashMap(int)LinkedHashMap<Channel,List<Event>>(int)LinkedHashMap(int,float)LinkedHashMap<Channel,List<Event>>(int,float)putModePUT_LASTPUT_FIRSTPUT_NORMaccessOrderoptChannelQueue"Event list must not be null"Event list must not be nullreqChannelsoptChannelseventQueueArrayList<Event>ArrayList<Event>()ArrayList<Event>(java.util.Collection)ArrayList<Event>(int)reqChanneltx"Transaction object must not be null"Transaction object must not be nullbatch"Error while writing to required channel: "Error while writing to required channel: "Unable to put batch on required " +
              "channel: "Unable to put batch on required channel: optChannel"Unable to put batch on optional channel: "Unable to put batch on optional channel: requiredChannelsoptionalChannels"Unable to put event on required " +
              "channel: "Unable to put event on required channel: "Unable to put event on optional channel: "Unable to put event on optional channel: A channel processor exposes operations to put {@link Event}s into{@link Channel}s. These operations will propagate a {@link ChannelException}if any errors occur while attempting to write to {@code required} channels.Each channel processor instance is configured with a {@link ChannelSelector}instance that specifies which channels are{@linkplain ChannelSelector#getRequiredChannels(Event) required} and whichchannels are{@linkplain ChannelSelector#getOptionalChannels(Event) optional}.The Context of the associated Source is passed. WARNING: throws FlumeException (is that ok?) run through and instantiate all the interceptors specified in the ContextAttempts to {@linkplain Channel#put(Event) put} the given events into eachconfigured channel. If any {@code required} channel throws a{@link ChannelException}, that exception will be propagated.<p>Note that if multiple channels are configured, some {@link Transaction}smay have already been committed while others may be rolled back in thecase of an exception.A list of events to put into the configured channels.when a write to a required channel fails. Process required channels Process optional channelsAttempts to {@linkplain Channel#put(Event) put} the given event into eachThe event to put into the configured channels./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/channel/ChannelSelectorFactory.javagetSelectorForTypegetSelectorForType(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/ChannelSelectorFactory.classClass<ChannelSelectorFactory>selectorClassName"Selector type {} is a custom type"Selector type {} is a custom type? extends ChannelSelectorClass<? extends ChannelSelector>selectorClassMap<String,? extends ChannelSelector>ChannelSelector[]Constructor<? extends ChannelSelector>TypeVariable<Class<? extends ChannelSelector>>TypeVariable<Class<? extends ChannelSelector>>[]"Unable to load selector type: "Unable to load selector type: /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/channel/ChannelUtils.javaChannelUtilsChannelUtils()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/ChannelUtils.classClass<ChannelUtils>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/ChannelUtils$1.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/ChannelUtils$2.classCallable<>/modules/java.base/java/util/concurrent/Callable.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/ChannelUtils$3.classCallable<Event>callcall()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/ChannelUtils$4.classCallable<List<Event>>transactorCallable<Object>Callable<T>committedinterruptede2"Failed to roll back transaction, exception follows:"Failed to roll back transaction, exception follows:"Failed to close transaction after error, exception follows:"Failed to close transaction after error, exception follows:A collection of utilities for interacting with {@link Channel}objects.  Use of these utilities prevents error-prone replicationof required transaction usage semantics, and allows for moreconcise code.However, as a side-effect of its generality, and in particular ofits use of {@link Callable}, any checked exceptions thrown byuser-created transactors will be silently wrapped with {@linkChannelException} before being propagated.  Only direct use of{@link #transact(Channel,Callable)} suffers from this issue, eventhough all other methods are based upon it, because none of theother methods are capable of producing or allowing checkedexceptions in the first place.A convenience method for single-event <code>put</code> transactions.#transact(Channel,Callable)A convenience method for multiple-event <code>put</code> transactions.A convenience method for single-event <code>take</code> transactions.a single event, or null if the channel has none availableA convenience method for multiple-event <code>take</code> transactions.a list of at most <code>max</code> eventsA convenience method for transactions that don't require a returnvalue.  Simply wraps the <code>transactor</code> using {@linkExecutors#callable} and passes that to {@link#transact(Channel,Callable)}.Executors#callable(Runnable)A general optimistic implementation of {@link Transaction} clientsemantics.  It gets a new transaction object from the<code>channel</code>, calls <code>begin()</code> on it, and theninvokes the supplied <code>transactor</code> object.  If anexception is thrown, then the transaction is rolled back;otherwise the transaction is committed and the value returned bythe <code>transactor</code> is returned.  In either case, thetransaction is closed before the function exits.  All secondaryexceptions (i.e. those thrown by<code>Transaction.rollback()</code> or<code>Transaction.close()</code> while recovering from an earlierexception) are logged, allowing the original exception to bepropagated instead.This implementation is optimistic in that it expects transactionrollback to be infrequent: it will rollback a transaction onlywhen the supplied <code>transactor</code> throws an exception,and exceptions are a fairly heavyweight mechanism for handlingfrequently-occurring events.the value returned by <code>transactor.call()</code>Disallows instantiation/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/channel/DefaultChannelFactory.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/DefaultChannelFactory.classClass<DefaultChannelFactory>channelClass"Creating instance of channel {} type {}"Creating instance of channel {} type {}Map<String,? extends Channel>Channel[]Constructor<? extends Channel>TypeVariable<Class<? extends Channel>>TypeVariable<Class<? extends Channel>>[]"Unable to create channel: "Unable to create channel: channelType"Channel type {} is a custom type"Channel type {} is a custom type"Unable to load channel type: "Unable to load channel type: /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/channel/LoadBalancingChannelSelector.javapicker/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/LoadBalancingChannelSelector.classemptyList"Channel picker returned null"Channel picker returned nullstrPolicy"policy"Enum<Policy>Comparable<Policy>compareTo(org.apache.flume.channel.LoadBalancingChannelSelector.Policy)EnumDesc<Policy>DynamicConstantDesc<Policy>Optional<EnumDesc<Policy>>Class<Policy>Enum<Policy>(java.lang.String,int)"Invalid policy: "Invalid policy: ? extends ChannelPickerClass<? extends ChannelPicker>Map<String,? extends ChannelPicker>ChannelPicker[]Constructor<? extends ChannelPicker>TypeVariable<Class<? extends ChannelPicker>>TypeVariable<Class<? extends ChannelPicker>>[]"Cannot instantiate policy class from policy enum "Cannot instantiate policy class from policy enum PolicyPolicy(java.lang.Class)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/LoadBalancingChannelSelector$Policy.classClass<RoundRobinPolicy>Class<RandomPolicy>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/LoadBalancingChannelSelector$RoundRobinPolicy.classapplyAsIntapplyAsInt(int,int)xy/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/LoadBalancingChannelSelector$RandomPolicy.classpickLoad balancing channel selector. This selector allows for load balancingbetween channels based on various policy configuration options. This serves a similar purposeto the LoadBalancingSinkProcessor except it allows the sinks to run in their own threads insteadof in just one.<p>The <tt>LoadBalancingChannelSelector</tt> maintains an indexed list ofactive channels on which the load must be distributed. This implementationsupports distributing load using either via <tt>ROUND_ROBIN</tt> or via<tt>RANDOM</tt> selection mechanism. The choice of selection mechanismdefaults to <tt>ROUND_ROBIN</tt> type, but can be overridden viaconfiguration.</p> instantiate policy instantiate pickerDefinitions for the various policy typesSelects channels in a round-robin fashionSelects a channel at random/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/channel/MemoryChannel.javagetBytesRemainingValuegetBytesRemainingValue()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/MemoryChannel.classresizeQueueresizeQueue(int)queueStoredLinkedBlockingDeque<Event>AbstractQueue<Event>BlockingDeque<Event>BlockingQueue<Event>defaultKeepAlivebyteCapacitySlotSizedefaultTransCapacitydefaultCapacityClass<MemoryChannel>takeByteCounter/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/MemoryChannel$MemoryTransaction.classputByteCounterLinkedBlockingDeque<Event>(int)AbstractQueue<Event>()Collection<? super Event>Iterable<? super Event>offer(org.apache.flume.Event,long,java.util.concurrent.TimeUnit)put(org.apache.flume.Event)offerLast(org.apache.flume.Event,long,java.util.concurrent.TimeUnit)offerFirst(org.apache.flume.Event,long,java.util.concurrent.TimeUnit)putLast(org.apache.flume.Event)putFirst(org.apache.flume.Event)LinkedBlockingDeque<Event>(java.util.Collection)LinkedBlockingDeque<Event>()"Put queue for MemoryTransaction of capacity "Put queue for MemoryTransaction of capacity " full, consider committing more frequently, " +
            "increasing capacity or increasing thread count" full, consider committing more frequently, increasing capacity or increasing thread count"Take list for MemoryTransaction, capacity "Take list for MemoryTransaction, capacity " full, consider committing more frequently, " +
            "increasing capacity, or increasing thread count" full, consider committing more frequently, increasing capacity, or increasing thread count"Queue.poll returned NULL despite semaphore " +
          "signalling existence of entry"remainingChange"Cannot commit transaction. Byte capacity " +
              "allocated to store event body "Cannot commit transaction. Byte capacity allocated to store event body "reached. Please increase heap space/byte capacity allocated to " +
              "the channel as the sinks may not be keeping up with the sources"reached. Please increase heap space/byte capacity allocated to the channel as the sinks may not be keeping up with the sources"Space for commit to queue couldn't be acquired." +
              " Sinks are likely not keeping up with sources, or the buffer size is too tight"Space for commit to queue couldn't be acquired. Sinks are likely not keeping up with sources, or the buffer size is too tight"Queue add failed, this shouldn't be able to happen"Queue add failed, this shouldn't be able to happen"Not enough space in memory channel " +
            "queue to rollback takes. This should never happen, please report"Not enough space in memory channel queue to rollback takes. This should never happen, please report"Invalid transation capacity specified, initializing channel"
          + " to default capacity of {}"Invalid transation capacity specified, initializing channel to default capacity of {}"Transaction Capacity of Memory Channel cannot be higher than " +
            "the capacity."Transaction Capacity of Memory Channel cannot be higher than the capacity."Couldn't acquire permits to downsize the byte capacity, resizing has been aborted"oldCapacity"Couldn't acquire permits to downsize the queue, resizing has been aborted"Couldn't acquire permits to downsize the queue, resizing has been abortedMemoryChannel is the recommended channel to use when speeds whichwriting to disk is impractical is required or durability of data is notrequired.Additionally, MemoryChannel should be used when a channel is required forunit testing purposes. lock to guard queue, mainly needed to keep it locked down during resizes it should never be held through a blocking operation invariant that tracks the amount of space remaining in the queue(with all uncommitted takeLists deducted) we maintain the remaining permits = queue.remaining - takeList.size() this allows local threads waiting for space in the queue to commit without denying access to the shared lock to threads that would make more space on the queue used to make "reservations" to grab data from the queue. by using this we can block for a while to get data without locking all other threads out like we would if we tried to use a blocking call on queue maximum items in a transaction queue<li>capacity = type long that defines the total number of events allowed at one time in the queue.<li>transactionCapacity = type long that defines the total number of events allowed in one transaction.<li>byteCapacity = type long that defines the max number of bytes used for events in the queue.<li>byteCapacityBufferPercentage = type int that defines the percent of buffer between byteCapacity and the estimated event size.<li>keep-alive = type int that defines the number of second to wait for a queue permit/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/channel/MultiplexingChannelSelector.javadefaultChannels/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/MultiplexingChannelSelector.classMap<String,List<Channel>>channelMappingEMPTY_LIST"header""flume.selector.header"flume.selector.header"mapping."mapping."optional"Class<MultiplexingChannelSelector>headerValue? super List<Channel>? extends List<Channel>BiFunction<? super List<Channel>,? super List<Channel>,? extends List<Channel>>merge(java.lang.String,java.util.List,java.util.function.BiFunction)BiFunction<? super String,? super List<Channel>,? extends List<Channel>>Function<? super String,? extends List<Channel>>replace(java.lang.String,java.util.List)replace(java.lang.String,java.util.List,java.util.List)putIfAbsent(java.lang.String,java.util.List)BiConsumer<? super String,? super List<Channel>>Entry<String,List<Channel>>Set<Entry<String,List<Channel>>>Collection<Entry<String,List<Channel>>>Iterable<Entry<String,List<Channel>>>Collection<List<Channel>>Iterable<List<Channel>>Map<? extends String,? extends List<Channel>>put(java.lang.String,java.util.List)hdrmapConfigoptionalChannelsMappingHashMap<String,List<Channel>>AbstractMap<String,List<Channel>>HashMap<String,List<Channel>>()AbstractMap<String,List<Channel>>()Node<String,List<Channel>>TreeNode<String,List<Channel>>newTreeNode(int,java.lang.String,java.util.List,java.util.HashMap.Node)newNode(int,java.lang.String,java.util.List,java.util.HashMap.Node)Node<String,List<Channel>>[]putVal(int,java.lang.String,java.util.List,boolean,boolean)HashMap<String,List<Channel>>(java.util.Map)HashMap<String,List<Channel>>(int)HashMap<String,List<Channel>>(int,float)"No channel configured for when "
            + "header value is: "No channel configured for when header value is: "Selector channel configured twice"Selector channel configured twiceconfChannelsreqdChannelsThis header value does not point to anythingReturn default channel(s) here.This should not go to default channel(s)because this seems to be a bad way to configure.If no mapping is configured, it is ok.All events will go to the default channel(s).Remove channels from optional channels, which are alreadyconfigured to be required channels.Check if there are required channels, else defaults to default channels/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/channel/PseudoTxnMemoryChannel.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/PseudoTxnMemoryChannel.classArrayBlockingQueue<Event>ArrayBlockingQueue<Event>(int)ArrayBlockingQueue<Event>(int,boolean,java.util.Collection)ArrayBlockingQueue<Event>(int,boolean)"No queue defined (Did you forget to configure me?"No queue defined (Did you forget to configure me?"Failed to put("Failed to put("Failed to take()"Failed to take()sharedInstance/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/PseudoTxnMemoryChannel$NoOpTransaction.classA capacity-capped {@link Channel} implementation that supports in-memorybuffering and delivery of events.This channel is appropriate for<q>best effort</q> delivery of events where high throughput is favored overdata durability. To be clear, <b>this channel offers absolutely no guaranteeof event delivery</b> in the face of (any) component failure.TODO: Discuss guarantees, corner cases re: potential data loss (e.g. consumerbegins a tx, takes events, and gets SIGKILL before rollback).<b>Configuration options</b><table><tr><th>Parameter</th><th>Description</th><th>Unit / Type</th><th>Default</th></tr><td><tt>capacity</tt></td><td>The in-memory capacity of this channel. Store up to <tt>capacity</tt>events before refusing new events.</td><td>events / int</td><td>50</td><td><tt>keep-alive</tt></td><td>The amount of time (seconds) to wait for an event before returning<tt>null</tt> on {@link #take()}.</td><td>seconds / int</td><td>3</td></table><b>Metrics</b>TODOA no-op transaction implementation that does nothing at all./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/channel/ReplicatingChannelSelector.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/channel/ReplicatingChannelSelector.classoptionalListoptionalChannelReplicating channel selector. This selector allows the event to be placedin all the channels that the source is configured with.Configuration to set a subset of the channels as optional.Seems like there are lot of components within flume that do not callconfigure method. It is conceiveable that custom component tests toodo that. So in that case, revert to old behavior./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/client/avro/AvroCLIClient.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/client/avro/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/clientorg.apache.flume.client.avro/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/client/avro/AvroCLIClient.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/client/avro/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/clientparseCommandLineparseCommandLine(java.lang.String[])parseHeadersparseHeaders(org.apache.commons.cli.CommandLine)sentdirNamerpcClientPropsFileMAX_LINE_LENGTHBATCH_SIZEClass<AvroCLIClient>2000"Unable to parse command line options - {}"Unable to parse command line options - {}"Unable to send data to Flume. Exception follows."Unable to send data to Flume. Exception follows."Unable to open connection to Flume. Exception follows."Unable to open connection to Flume. Exception follows."Unable to deliver events to Flume. Exception follows."Unable to deliver events to Flume. Exception follows."Exiting"ExitingcommandLineheaderFile"headerFile"propertiesEntry"Inserting Header Key ["Inserting Header Key ["] header value ["] header value ["Unable to load headerFile"Unable to load headerFile"Unable to close headerFile"Unable to close headerFile"P"P"rpcProps"rpcProps"RPC client properties file with " +
            "server connection params"RPC client properties file with server connection params"p""port""port of the avro source"port of the avro source"H"H"host"host"hostname of the avro source"hostname of the avro source"F"F"filename"filename"file to stream to avro source"file to stream to avro source"dirname"dirname"directory to stream to avro source"directory to stream to avro source"R""file containing headers as " +
            "key/value pairs on each new line"file containing headers as key/value pairs on each new line"help"help"display help text"display help text'h'"flume-ng avro-client"flume-ng avro-client"The --dirname option assumes that a spooling directory exists " +
          "where immutable log files are dropped."The --dirname option assumes that a spooling directory exists where immutable log files are dropped."--filename and --dirname options cannot be used simultaneously"--filename and --dirname options cannot be used simultaneously"Either --rpcProps or both --host and --port " +
          "must be specified."Either --rpcProps or both --host and --port must be specified."RPC client properties " +
          "file must be specified after --rpcProps argument."RPC client properties file must be specified after --rpcProps argument."RPC client properties file %s does not exist!"RPC client properties file %s does not exist!"You must specify a port to connect to with --port"You must specify a port to connect to with --port"You must specify a hostname to connect to with --host"You must specify a hostname to connect to with --hostlastChecksentBytesbatchSize"avrocli"avrocli"Packed {} bytes, {} events"Packed {} bytes, {} events"Finished"Finished"Closing reader"Closing reader"Closing RPC client"Closing RPC clientHeader Format : key1=value1, key2=value2,.../Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/client/avro/EventReader.javaA line reader produces a stream of lines for the {@link AvroCLIClient} toingest into Flume. The stream may be finite or infinite.Get the next line associated with the input stream. If this returns{@code null}, the input underlying input source is considered finished.Note that this is allowed to block for indefinite amounts of time waitingto generate a new line.Get up to {@code n} lines associated with the input stream. If this returnsless than n lines, the input underlying input source is consideredfinished. Note that this is allowed to block for indefinite amounts oftime waiting to generate a new line.Clean-up any state associated with this reader./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/client/avro/ReliableEventReader.javaA reliable event reader.Clients must call commit() after each read operation, otherwise theimplementation must reset its internal buffers and return the same eventsas it did previously.Indicate to the implementation that the previously-returned events havebeen successfully processed and committed./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/client/avro/ReliableSpoolingFileEventReader.javadeleteMetaFiledeleteMetaFile()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/client/avro/ReliableSpoolingFileEventReader.classcom.google.common.base/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/base/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/base/Optional.class"Use Optional.of(value) or Optional.absent()"Use Optional.of(value) or Optional.absent()Optional<FileInfo>openFileopenFile(java.io.File)smallerLexicographicalsmallerLexicographical(java.io.File,java.io.File)getNextFilegetNextFile()deleteCurrentFiledeleteCurrentFile(java.io.File)rollCurrentFileInTrackerDirrollCurrentFileInTrackerDir(java.io.File)rollCurrentFilerollCurrentFile(java.io.File)retireCurrentFileretireCurrentFile()fillHeaderfillHeader(java.util.List)readDeserializerEventsreadDeserializerEvents(int)getListFilesCountgetListFilesCount()getRelPathToSpoolDirgetRelPathToSpoolDir(java.nio.file.Path)Set<Path>Collection<Path>Iterable<Path>isFileInTrackerDirisFileInTrackerDir(java.util.Set,java.nio.file.Path)getTrackerDirCompletedFilesgetTrackerDirCompletedFiles()getCandidateFilesgetCandidateFiles(java.nio.file.Path)ReliableSpoolingFileEventReaderReliableSpoolingFileEventReader(java.io.File,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean,java.lang.String,boolean,java.lang.String,java.lang.String,org.apache.flume.Context,java.lang.String,java.lang.String,java.lang.String,org.apache.flume.serialization.DecodeErrorPolicy,org.apache.flume.source.SpoolDirectorySourceConfigurationConstants.ConsumeOrder,boolean,org.apache.flume.instrumentation.SourceCounter)trackerDirectoryAbsolutePathlistFilesCountcandidateFileIterfirstTimeReadlastFileReadcurrentFilesourceCounterrecursiveDirectorySearchconsumeOrderdecodeErrorPolicyinputCharsettrackingPolicydeletePolicybaseNameHeaderfileNameHeaderannotateBaseNameannotateFileNametrackerDirectorymetaFileincludePatterndeserializerContextdeserializerTypecompletedSuffixspoolDirPathspoolDirectoryFORMATTERmetaFileNameClass<ReliableSpoolingFileEventReader>".flumespool-main.meta".flumespool-main.meta"yyyy/MM/dd HH:mm:ss.SSS"yyyy/MM/dd HH:mm:ss.SSStrackerDirPathEnum<DeletePolicy>Comparable<DeletePolicy>compareTo(org.apache.flume.client.avro.ReliableSpoolingFileEventReader.DeletePolicy)EnumDesc<DeletePolicy>DynamicConstantDesc<DeletePolicy>Optional<EnumDesc<DeletePolicy>>Class<DeletePolicy>Enum<DeletePolicy>(java.lang.String,int)"Delete policies other than " +
          "NEVER and IMMEDIATE are not yet supported"Delete policies other than NEVER and IMMEDIATE are not yet supportedEnum<TrackingPolicy>Comparable<TrackingPolicy>compareTo(org.apache.flume.client.avro.ReliableSpoolingFileEventReader.TrackingPolicy)EnumDesc<TrackingPolicy>DynamicConstantDesc<TrackingPolicy>Optional<EnumDesc<TrackingPolicy>>Class<TrackingPolicy>Enum<TrackingPolicy>(java.lang.String,int)"Tracking policies other than " +
              "RENAME and TRACKER_DIR are not supported"Tracking policies other than RENAME and TRACKER_DIR are not supported"Initializing {} with directory={}, metaDir={}, " +
                   "deserializer={}"Initializing {} with directory={}, metaDir={}, deserializer={}Map<String,ReliableSpoolingFileEventReader>ReliableSpoolingFileEventReader[]Constructor<ReliableSpoolingFileEventReader>? super ReliableSpoolingFileEventReaderClass<? super ReliableSpoolingFileEventReader>TypeVariable<Class<ReliableSpoolingFileEventReader>>TypeVariable<Class<ReliableSpoolingFileEventReader>>[]"Directory does not exist: "Directory does not exist: "Path is not a directory: "Path is not a directory: canary"flume-spooldir-perm-check-"flume-spooldir-perm-check-".canary".canarylinesOpenOption[]"testing flume file permissions\n"testing flume file permissions
"Empty canary file %s"Empty canary file %s"Unable to delete canary file "Unable to delete canary file "Successfully created and deleted canary file: {}"Successfully created and deleted canary file: {}"Unable to read and modify files" +
                " in the spooling directory: "Unable to read and modify files in the spooling directory: "Unable to mkdir nonexistent meta directory "Unable to mkdir nonexistent meta directory "Specified meta directory is not a directory"Specified meta directory is not a directorycandidateFilesArrayList<File>()AbstractCollection<File>()AbstractList<File>()ArrayList<File>(java.util.Collection)ArrayList<File>(int)trackerDirCompletedFiles? super Pathjava.nio.fileFileVisitor<? super Path>/modules/java.base/java/nio/file/FileVisitor.class/modules/java.base/java/nio/file/modules/java.base/java/nio/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/client/avro/ReliableSpoolingFileEventReader$1.classSimpleFileVisitor<Path>/modules/java.base/java/nio/file/SimpleFileVisitor.classFileVisitor<Path>postVisitDirectorypostVisitDirectory(java.lang.Object,java.io.IOException)postVisitDirectory(java.nio.file.Path,java.io.IOException)visitFileFailedvisitFileFailed(java.lang.Object,java.io.IOException)visitFileFailed(java.nio.file.Path,java.io.IOException)visitFilevisitFile(java.lang.Object,java.nio.file.attribute.BasicFileAttributes)visitFile(java.nio.file.Path,java.nio.file.attribute.BasicFileAttributes)preVisitDirectorypreVisitDirectory(java.lang.Object,java.nio.file.attribute.BasicFileAttributes)preVisitDirectory(java.nio.file.Path,java.nio.file.attribute.BasicFileAttributes)SimpleFileVisitorSimpleFileVisitor()SimpleFileVisitor<Path>()attrsdirectoryNamecandidate"I/O exception occurred while listing directories. " +
                   "Files already matched will be returned. "I/O exception occurred while listing directories. Files already matched will be returned. completedFilesHashSet<Path>AbstractSet<Path>AbstractCollection<Path>HashSet<Path>()Spliterator<Path>Consumer<? super Path>Iterator<Path>Stream<Path>BaseStream<Path,Stream<Path>>Predicate<? super Path>? extends PathCollection<? extends Path>Iterable<? extends Path>add(java.nio.file.Path)AbstractCollection<Path>()AbstractSet<Path>()HashSet<Path>(int,float,boolean)HashSet<Path>(int)HashSet<Path>(int,float)HashSet<Path>(java.util.Collection)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/client/avro/ReliableSpoolingFileEventReader$2.classpathrelPathtrackerPathisPresentisPresent()presentInstancespresentInstances(java.lang.Iterable)? extends TOptional<? extends T>? extends Optional<? extends T>Iterable<? extends Optional<? extends T>>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/base/Function.classOptional<V>transformtransform(com.google.common.base.Function)Function<? super T,V>? super FileInfoFunction<? super FileInfo,V>asSetasSet()Set<FileInfo>Collection<FileInfo>Iterable<FileInfo>orNullorNull()Supplier<? extends T>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/base/Supplier.classoror(com.google.common.base.Supplier)? extends FileInfoSupplier<? extends FileInfo>or(com.google.common.base.Optional)Optional<? extends FileInfo>or(java.lang.Object)or(org.apache.flume.client.avro.ReliableSpoolingFileEventReader.FileInfo)OptionalOptional()Optional<FileInfo>()Optional<T>toJavaUtiltoJavaUtil()toJavaUtil(com.google.common.base.Optional)fromJavaUtilfromJavaUtil(java.util.Optional)fromNullablefromNullable(java.lang.Object)absentabsent()fileInfo" lastModified: " lastModified: " size: " size: "File should not roll when " +
            "commit is outstanding."File should not roll when commit is outstanding."Last read was never committed - resetting mark position."Last read was never committed - resetting mark position."Last read took us just up to a file boundary. " +
                  "Rolling to the next file, if there is one."Last read took us just up to a file boundary. Rolling to the next file, if there is one.desbasenamefileToRoll"File has been modified since being read: "File has been modified since being read: "File has changed size since being read: "File has changed size since being read: "Unsupported delete policy: "Unsupported delete policy: dest"Preparing to move file {} to {}"Preparing to move file {} to {}deleted"Completed file "Completed file " already exists, but files match, so continuing." already exists, but files match, so continuing."Unable to delete file "Unable to delete file ". It will likely be ingested another time.". It will likely be ingested another time."File name has been re-used with different" +
            " files. Spooling assumptions violated for "File name has been re-used with different files. Spooling assumptions violated for "File name has been re-used with different" +
          " files. Spooling assumptions violated for "renamed"Successfully rolled file {} to {}"Successfully rolled file {} to {}"Unable to move "Unable to move ". This will likely cause duplicate events. Please verify that " +
            "flume has sufficient permissions to perform these operations.". This will likely cause duplicate events. Please verify that flume has sufficient permissions to perform these operations.relToRoll"Preparing to create tracker file for {} at {}"Preparing to create tracker file for {} at {}"File name has been re-used with different" +
              " files. Spooling assumptions violated for ""Could not create tracker file: "Could not create tracker file: "Preparing to delete file {}"Preparing to delete file {}"Unable to delete nonexistent file: {}"Unable to delete nonexistent file: {}"Unable to delete spool file: "Unable to delete spool file: selectedFilecandidateFilef1f2nextPathtrackerdeserializer"Tracker target %s does not equal expected filename %s"Tracker target %s does not equal expected filename %s"Could not find file: "Could not find file: "Exception opening file: "Exception opening file: "Unable to delete old meta file "Unable to delete old meta file /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/client/avro/ReliableSpoolingFileEventReader$FileInfo.classlastModifiedDeletePolicyDeletePolicy()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/client/avro/ReliableSpoolingFileEventReader$DeletePolicy.classTrackingPolicyTrackingPolicy()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/client/avro/ReliableSpoolingFileEventReader$TrackingPolicy.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/client/avro/ReliableSpoolingFileEventReader$Builder.class<p>A {@link ReliableEventReader} which reads log data from files storedin a spooling directory and renames each file once all of its data has beenread (through {@link EventDeserializer#readEvent()} calls). The user must{@link #commit()} each read, to indicate that the lines have been fullyprocessed.<p>Read calls will return no data if there are no files left to read. Thisclass, in general, is not thread safe.<p>This reader assumes that files with unique file names are left in thespooling directory and not modified once they are placed there. Any userbehavior which violates these assumptions, when detected, will result in aFlumeException being thrown.<p>This class makes the following guarantees, if above assumptions are met:<li> Once a log file has been renamed with the {@link #completedSuffix},all of its records have been read through the{@link EventDeserializer#readEvent()} function and{@link #commit()}ed at least once.<li> All files in the spooling directory will eventually be openedand delivered to a {@link #readEvents(int)} caller.Always contains the last file from which lines have been read.Instance var to Cache directory listingCreate a ReliableSpoolingFileEventReader to watch the given directory. Sanity checks validate delete policy validate tracking policy Verify directory exists and is readable/writable Do a canary test to make sure we have access to spooling directory if relative path, treat as relative to spool directory ensure that meta directory exists ensure that the meta directory is a directoryRecursively gather candidate filesthe directory to gather files fromlist of files within the passed in directory The top directory should always be listedReturn the filename which generated the data from the last successful{@link #readEvents(int)} call. Returns null if called before any filecontents are read.Return the filename, lastModified, and size which generated the data from the last successful public interface Check if new files have arrived since last call Return empty list if no new filesIt's possible that the last read took us just up to a file boundary.If so, try to roll to the next file, if there is one.Loop until events is not empty or there is no next file in case of 0 byte filesCommit the last lines which were read.Closes currentFile and attempt to rename it.If these operations fail in a way that may cause duplicate log entries,an error is logged but no exceptions are thrown. If these operations failin a way that indicates potential misuse of the spooling directory, aFlumeException will be thrown.if files do not conform to spooling assumptions Verify that spooling assumptions hold TODO: implement delay in the futureRename the given spooled file Before renaming, check whether destination file name existsIf we are here, it means the completed file already exists. In almostevery case this means the user is violating an assumption of Flume(that log files are placed in the spooling directory with uniquenames). However, there is a corner case on Windows systems where thefile was already rolled but the rename was not atomic. If that seemslikely, we let it pass with only a warning. Dest file exists and not on windows Destination file does not already exist. We are good to go! now we no longer need the meta fileIf we are here then the file cannot be renamed for a reason otherthan that the destination file exists (actually, that remainspossible w/ small probability due to TOC-TOU conditions).Create an empty file as an indicatorcreate the parent dirs firstDelete the given spooled fileReturns the next file to be consumed from the chosen directory.If the directory is empty or the chosen file is not readable,this will return an absent option.If the {@link #consumeOrder} variable is {@link ConsumeOrder#OLDEST}then returns the oldest file. If the {@link #consumeOrder} variableis {@link ConsumeOrder#YOUNGEST} then returns the youngest file.If two or more files are equally old/young, then the file name withlower lexicographical value is returned.If the {@link #consumeOrder} variable is {@link ConsumeOrder#RANDOM}then cache the directory listing to amortize retreival cost, and returnany arbitary file from the directory. No matching file in spooling directory. Selected file is random. ts is same pick smallest lexicographically. candidate is younger (cand-ts > selec-ts) default order is OLDEST candidate is older (cand-ts < selec-ts).Opens a file for consuming{@link FileInfo} for the file to consume or absent option if thefile does not exists or readable. roll the meta file, if needed sanity check File could have been deleted in the interimAn immutable class with information about a file being processed.Special builder class for ReliableSpoolingFileEventReader/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/client/avro/SimpleTextLineEventReader.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/client/avro/SimpleTextLineEventReader.classA {@link EventReader} implementation which delegates to a{@link BufferedReader}./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/conf/BatchSizeSupported.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/confThis interface indicates that a component does batching and the batch sizeis publicly available.Returns the batch size/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/conf/Configurable.javaAny class marked as Configurable may have a context including itssub-configuration passed to it, requesting it configure itself.Request the implementing class to (re)configure itself.When configuration parameters are changed, they must bereflected by the component asap.There are no thread safety guarantees on when configure might be called./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/conf/ConfigurableComponent.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/conf/Configurables.java"Required parameter "Required parameter " must exist and may not be null" must exist and may not be null"Optional parameter "Optional parameter " may not be null" may not be nullMethods for working with {@link Configurable}s.Check that {@code target} implements {@link Configurable} and, if so, askit to configure itself using the supplied {@code context}.An object that potentially implements Configurable.The configuration contexttrue if {@code target} implements Configurable, false otherwise./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/conf/TransactionCapacitySupported.javaThis interface indicates that a component has a transaction capacityand it is publicly available.Returns the transaction capacity/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/event/EventHelper.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/eventorg.apache.flume.event/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/event/EventHelper.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/eventDEFAULT_MAX_BYTESHEXDUMP_OFFSET"00000000"00000000Class<EventHelper>maxBytes"null"nullhexDump"Exception while dumping event"Exception while dumping event"...Exception while dumping: "...Exception while dumping: "{ headers:"{ headers:" body:" body: do nothing... in this case, HexDump.dump() will throw an exception remove offset since it's not relevant for such a small dataset/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/formatter/output/BucketPath.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/formatter/output/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/formatterorg.apache.flume.formatter.outputroundDownroundDown(int,int,long,java.util.TimeZone)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/formatter/output/BucketPath.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/formatter/output/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/formatterclock"%(\\w|%)|%\\{([\\w\\.-]+)\\}|%\\[(\\w+)\\]"%(\w|%)|%\{([\w\.-]+)\}|%\[(\w+)\]HashMap<String,SimpleDateFormat>AbstractMap<String,SimpleDateFormat>Map<String,SimpleDateFormat>ThreadLocal<HashMap<String,SimpleDateFormat>>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/formatter/output/BucketPath$1.classchildValue(java.util.HashMap)createMap(java.lang.Thread,java.util.HashMap)setCarrierThreadLocal(java.util.HashMap)set(java.util.HashMap)ThreadLocal<HashMap<String,SimpleDateFormat>>()HashMap<String,SimpleDateFormat>()? super SimpleDateFormat? extends SimpleDateFormatBiFunction<? super SimpleDateFormat,? super SimpleDateFormat,? extends SimpleDateFormat>merge(java.lang.String,java.text.SimpleDateFormat,java.util.function.BiFunction)BiFunction<? super String,? super SimpleDateFormat,? extends SimpleDateFormat>Function<? super String,? extends SimpleDateFormat>replace(java.lang.String,java.text.SimpleDateFormat)replace(java.lang.String,java.text.SimpleDateFormat,java.text.SimpleDateFormat)putIfAbsent(java.lang.String,java.text.SimpleDateFormat)BiConsumer<? super String,? super SimpleDateFormat>getOrDefault(java.lang.Object,java.text.SimpleDateFormat)Entry<String,SimpleDateFormat>Set<Entry<String,SimpleDateFormat>>Collection<Entry<String,SimpleDateFormat>>Iterable<Entry<String,SimpleDateFormat>>Collection<SimpleDateFormat>Iterable<SimpleDateFormat>Map<? extends String,? extends SimpleDateFormat>put(java.lang.String,java.text.SimpleDateFormat)AbstractMap<String,SimpleDateFormat>()Node<String,SimpleDateFormat>TreeNode<String,SimpleDateFormat>newTreeNode(int,java.lang.String,java.text.SimpleDateFormat,java.util.HashMap.Node)newNode(int,java.lang.String,java.text.SimpleDateFormat,java.util.HashMap.Node)Node<String,SimpleDateFormat>[]putVal(int,java.lang.String,java.text.SimpleDateFormat,boolean,boolean)HashMap<String,SimpleDateFormat>(java.util.Map)HashMap<String,SimpleDateFormat>(int)HashMap<String,SimpleDateFormat>(int,float)'a'a"weekday_short"weekday_short'A'"weekday_full"weekday_full'b'"monthname_short"monthname_short'B'B"monthname_full"monthname_full'c'"datetime"datetime'd'd"day_of_month_xx"day_of_month_xx'e'"day_of_month_x"day_of_month_x'D'D"date_short"date_short'H'"hour_24_xx"hour_24_xx'I'I"hour_12_xx"hour_12_xx'j'j"day_of_year_xxx"day_of_year_xxx'k'k"hour_24"hour_24'l'"hour_12"hour_12'm'm"month_xx"month_xx'n'"month_x"month_x'M'M"minute_xx"minute_xx'p'"am_pm"am_pm's'"unix_seconds"unix_seconds'S'"seconds_xx"seconds_xx't'"unix_millis"unix_millis'y'"year_xx"year_xx'Y'Y"year_xxxx"year_xxxx'z'z"timezone_delta"timezone_deltaneedRoundingunittimeZoneuseLocalTimestampstringlocalCachesimpleDateFormatreplacementString"localhost""ip"ip"fqdn"fqdn"The static escape string '"The static escape string '"'"
                + " was provided but does not match any of (localhost,IP,FQDN)"' was provided but does not match any of (localhost,IP,FQDN)timestampHeaderformatStringdate"timestamp"timestamp"Expected timestamp in " +
            "the Flume event headers, but it was null"Expected timestamp in the Flume event headers, but it was null"Flume wasn't able to parse timestamp header"
        + " in the event to resolve time based bucketing. Please check that"
        + " you're correctly populating timestamp header (for example using"
        + " TimestampInterceptor source interceptor)."Flume wasn't able to parse timestamp header in the event to resolve time based bucketing. Please check that you're correctly populating timestamp header (for example using TimestampInterceptor source interceptor).'%'%"%""EEE"EEE"EEEE"EEEE"MMM"MMM"MMMM"MMMM"EEE MMM d HH:mm:ss yyyy"EEE MMM d HH:mm:ss yyyy"dd"dd"d""MM/dd/yy"MM/dd/yy"HH"HH"hh"hh"DDD"DDD"MM"MM"mm"mm"M""a""ss"ss"yy"yy"yyyy"yyyy"ZZZ"ZZZuseLocalTimeStampreplacement"Expected to match single character tag in string "Expected to match single character tag in string "\\\\"\\"\\\\\\\\"\\\\"\\$"\$"\\\\\\$"\\\$mappingclkcanonicalHostName/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/formatter/output/BucketPath$InetAddressCache.classhostAddresshostNameInetAddressCacheInetAddressCache()addr"Unable to get localhost"Unable to get localhostThese are useful to other classes which might want to search for tags instrings.Returns true if in contains a substring matching TAG_REGEX (i.e. of theform %{...} or %x. It's a date two digit 1 or 2 digit "MM/dd/yy"; three digits 1 or 2 digits This is different from unix date (which would insert a tab character here) LOG.warn("Unrecognized escape in event format string: %" + c);Hardcoded lookups for %x style escape replacement. Add your own!All shorthands are Date format strings, currently.Returns the empty string if an escape is not recognized.Dates follow the same format as unix date, with a few exceptions.<p>This static method will be REMOVED in a future version of Flume</p>A wrapper around{@link BucketPath#replaceShorthand(char, Map, TimeZone, boolean, int,int, boolean)}with the timezone set to the default.- The character to replace.- Event headers- The timezone to use for formatting the timestamp- Should the timestamp be rounded down?- if needRounding is true, what unit to round down to. Thismust be one of the units specified by {@link java.util.Calendar} -HOUR, MINUTE or SECOND. Defaults to second, if none of these are present.Ignored if needRounding is false.- if needRounding is true,The time should be rounded to the largest multiple of thisvalue, smaller than the time supplied, defaults to 1, if <= 0(rounds offto the second/minute/hour immediately lower than the timestamp supplied.Not intended as a public APIReplace all substrings of form %{tagname} with get(tagname).toString() andall shorthand substrings of form %x with a special value.Any unrecognized / not found tags will be replaced with the empty string.TODO(henry): we may want to consider taking this out of Event and into amore general class when we get more use cases for this pattern.{@link BucketPath#escapeString(String, Map, TimeZone, boolean, int, int,boolean)}Escaped string. Group 2 is the %{...} pattern          LOG.warn("Tag " + matcher.group(2) + " not found"); Group 3 is the %[...] pattern. The %x pattern. Since we know the match is a single character, we can switch on that rather than the string. The replacement string must have '$' and '\' chars escaped. This replacement string is pretty arcane. replace : '$' -> for java '\$' -> for regex "\\$" replacement: '\$' -> for regex '\\\$' -> for java "\\\\\\$" replace : '\' -> for java "\\" -> for regex "\\\\" replacement: '\\' -> for regex "\\\\" -> for java "\\\\\\\\" note: order mattersInstead of replacing escape sequences in a string, this method returns amapping of an attribute name to the value based on the escape sequencefound in the argument string.May not be called from outside unit tests./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/formatter/output/DefaultPathManager.javaFILE_PREFIX/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/formatter/output/DefaultPathManager.classFILE_EXTENSIONDEFAULT_FILE_EXTENSIONDEFAULT_FILE_PREFIXextensionfilePrefixfileIndexbaseDirectoryseriesTimestamp"extension""prefix""-"/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/formatter/output/EventFormatter.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/formatter/output/PathManager.java"pathManager."pathManager.Creates the files used by the RollingFileSink.{@link Context} prefixKnows how to construct this path manager.<br/><b>Note: Implementations MUST provide a public a no-arg constructor.</b>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/formatter/output/PathManagerFactory.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/formatter/output/PathManagerFactory.classClass<PathManagerFactory>managerTypebuilderClass"path manager type must not be null"path manager type must not be null"Not in enum, loading builder class: {}"Not in enum, loading builder class: {}": does not appear to implement ": does not appear to implement Create PathManager instances. try to find builder class in enum of known output serializers/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/formatter/output/PathManagerType.javaPathManagerTypePathManagerType(java.lang.Class)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/formatter/output/PathManagerType.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/formatter/output/RollTimePathManager.javalastRoll/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/formatter/output/RollTimePathManager.classformatter"yyyyMMddHHmmss"yyyyMMddHHmmss/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/formatter/output/TextDelimitedOutputFormatter.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/ChannelCounter.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentationorg.apache.flume.instrumentationATTRIBUTES/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/ChannelCounter.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentationCOUNTER_CHANNEL_CAPACITYCOUNTER_EVENT_TAKE_SUCCESSCOUNTER_EVENT_PUT_SUCCESSCOUNTER_EVENT_TAKE_ATTEMPTCOUNTER_EVENT_PUT_ATTEMPTCOUNTER_CHANNEL_SIZE"channel.current.size"channel.current.size"channel.event.put.attempt"channel.event.put.attempt"channel.event.take.attempt"channel.event.take.attempt"channel.event.put.success"channel.event.put.success"channel.event.take.success"channel.event.take.success"channel.capacity"channel.capacityattributesnewSize/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/ChannelCounterMBean.javaThis interface represents a channel counter mbean. Any class implementingthis interface must sub-class{@linkplain org.apache.flume.instrumentation.MonitoredCounterGroup}. Thisinterface might change between minor releases. Please see{@linkplain org.apache.flume.instrumentation.ChannelCounter} class./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/ChannelProcessorCounter.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/GangliaServer.javaList<HostInfo>SequencedCollection<HostInfo>Collection<HostInfo>Iterable<HostInfo>getHostsFromStringgetHostsFromString(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/GangliaServer.classpadpad()GANGLIA_CONTEXTisGanglia3pollFrequencyservicesocketList<SocketAddress>SequencedCollection<SocketAddress>Collection<SocketAddress>Iterable<SocketAddress>addressesArrayList<SocketAddress>AbstractList<SocketAddress>AbstractCollection<SocketAddress>ArrayList<SocketAddress>()Spliterator<SocketAddress>? super SocketAddressConsumer<? super SocketAddress>Iterator<SocketAddress>Stream<SocketAddress>BaseStream<SocketAddress,Stream<SocketAddress>>Predicate<? super SocketAddress>? extends SocketAddressCollection<? extends SocketAddress>Iterable<? extends SocketAddress>add(java.net.SocketAddress)AbstractCollection<SocketAddress>()addLast(java.net.SocketAddress)addFirst(java.net.SocketAddress)ListIterator<SocketAddress>add(int,java.net.SocketAddress)set(int,java.net.SocketAddress)Comparator<? super SocketAddress>UnaryOperator<SocketAddress>Function<SocketAddress,SocketAddress>AbstractList<SocketAddress>()ArrayList<SocketAddress>(java.util.Collection)ArrayList<SocketAddress>(int)"pollFrequency""hosts""isGanglia3"Class<GangliaServer>1500"double""flume."flume.bytesnewOffset240xffpacket"Could not send metrics to metrics server: "Could not send metrics to metrics server: "Could not create socket for metrics collection."Could not create socket for metrics collection.ex2"Unknown error occured"Unknown error occured"Waiting for ganglia service to stop"Waiting for ganglia service to stop"Interrupted while waiting"
                + " for ganglia monitor to shutdown"Interrupted while waiting for ganglia monitor to shutdownList<Runnable>SequencedCollection<Runnable>Collection<Runnable>Iterable<Runnable>"string""Sending ganglia3 formatted message."Sending ganglia3 formatted message."float""Sending ganglia 3.1 formatted message: "Sending ganglia 3.1 formatted message: 128"GROUP"GROUP"flume"flume133"%s"%slocalHosts"Hosts list cannot be empty."Hosts list cannot be empty.hostInfoListArrayList<HostInfo>AbstractList<HostInfo>AbstractCollection<HostInfo>ArrayList<HostInfo>()Spliterator<HostInfo>? super HostInfoConsumer<? super HostInfo>Iterator<HostInfo>Stream<HostInfo>BaseStream<HostInfo,Stream<HostInfo>>Predicate<? super HostInfo>? extends HostInfoCollection<? extends HostInfo>Iterable<? extends HostInfo>add(org.apache.flume.api.HostInfo)AbstractCollection<HostInfo>()addLast(org.apache.flume.api.HostInfo)addFirst(org.apache.flume.api.HostInfo)ListIterator<HostInfo>add(int,org.apache.flume.api.HostInfo)set(int,org.apache.flume.api.HostInfo)Comparator<? super HostInfo>UnaryOperator<HostInfo>Function<HostInfo,HostInfo>AbstractList<HostInfo>()ArrayList<HostInfo>(java.util.Collection)ArrayList<HostInfo>(int)"Invalid ganglia host: "Invalid ganglia host: "ganglia_host-"ganglia_host-"No valid ganglia hosts defined!"No valid ganglia hosts defined!server/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/GangliaServer$GangliaCollector.classMap<String,Map<String,String>>metricsMap? super Map<String,String>? extends Map<String,String>BiFunction<? super Map<String,String>,? super Map<String,String>,? extends Map<String,String>>merge(java.lang.String,java.util.Map,java.util.function.BiFunction)BiFunction<? super String,? super Map<String,String>,? extends Map<String,String>>Function<? super String,? extends Map<String,String>>replace(java.lang.String,java.util.Map)replace(java.lang.String,java.util.Map,java.util.Map)putIfAbsent(java.lang.String,java.util.Map)BiConsumer<? super String,? super Map<String,String>>getOrDefault(java.lang.Object,java.util.Map)Entry<String,Map<String,String>>Set<Entry<String,Map<String,String>>>Collection<Entry<String,Map<String,String>>>Iterable<Entry<String,Map<String,String>>>Collection<Map<String,String>>Iterable<Map<String,String>>Map<? extends String,? extends Map<String,String>>put(java.lang.String,java.util.Map)attributeMapattribute"Unexpected error"Unexpected errorA Ganglia server that polls JMX based at a configured frequency (defaults toonce every 60 seconds). This implementation can send data to ganglia 3 andganglia 3.1. <p><b>Mandatory Parameters:</b><p> <tt>hosts: </tt> List of comma separatedhostname:ports of ganglia servers to report metrics to. <p> <b>OptionalParameters: </b><p> <tt>pollFrequency:</tt>Interval in seconds betweenconsecutive reports to ganglia servers. Default = 60 seconds.<p><tt>isGanglia3:</tt> Report to ganglia 3 ? Default = false - reports toganglia 3.1.The Ganglia protocol specific stuff: the xdr_* methodsand the sendToGanglia* methods have been shamelessly ripped offfrom Hadoop. All hail the yellow elephant! as per libgmond.cPuts a string into the buffer by first writing the size of the string as anint, followed by the bytes of the string, padded if necessary to a multipleof 4.the string to be written to buffer at offset locationPads the buffer with zero bytes up to the nearest multiple of 4.Puts an integer into the buffer as 4 bytes, big-endian.Start this server, causing it to poll JMX at the configured frequency.Stop this server.Seconds between consecutive JMX polls.Seconds between consecutive JMX pollsWhen true, ganglia 3 messages will be sent, else Ganglia3.1 formatted messages are sent.True if the server is currently sending ganglia 3 formatted msgs.False if the server returns Ganglia 3.1 The param is a string, and so leave the type as is. metric type metric_id = metadata_msg hostname metric name spoof = False units slope tmax, the maximum time between metrics dmax, the maximum data valueNum of the entries in extra_value field for Ganglia 3.1.xGroup attributeGroup value Now we send out a message with the actual value. Technically, we only need to send out the metadata message once for each metric, but I don't want to have to record which metrics we did and did not send. we are sending a string value hostName format field metric valueWorker which polls JMX for all mbeans with{@link javax.management.ObjectName} within the flume namespace:org.apache.flume. All attributes of such beans are sent to the all hostsspecified by the server that owns it's instance./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/MonitorService.javaInterface that any monitoring service should implement. If the monitorservice is to be started up when Flume starts, it should implement thisand the class name should be passed in during startup, with any additionalcontext it requires./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/MonitoredCounterGroup.javaregisterregister()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/MonitoredCounterGroup.classregisteredstopTimestartTimecounterMapCOUNTER_GROUP_STOP_TIMECOUNTER_GROUP_START_TIMEClass<MonitoredCounterGroup>"start.time"start.time"stop.time"stop.timecounterInitMap"Component type: "Component type: ", name: ", name: " started" startedobjName"org.apache.flume."org.apache.flume.Enum<Type>Comparable<Type>compareTo(org.apache.flume.instrumentation.MonitoredCounterGroup.Type)EnumDesc<Type>DynamicConstantDesc<Type>Optional<EnumDesc<Type>>Class<Type>Enum<Type>(java.lang.String,int)":type=":type="Monitored counter group for type: "Monitored counter group for type: ": Another MBean is already registered with this name. "
              + "Unregistering that pre-existing MBean now...": Another MBean is already registered with this name. Unregistering that pre-existing MBean now...": Successfully unregistered pre-existing MBean.": Successfully unregistered pre-existing MBean.": Successfully registered new MBean.": Successfully registered new MBean."Failed to register monitored counter group for type: "Failed to register monitored counter group for type: typePrefixmapKeys" stopped" stopped"Shutdown Metric for type: "Shutdown Metric for type: ", "
        + "name: "" == " == counterMapKeycounterMapValue", "
          + "name: "counterIterator"{"{counterName"="=Type()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/MonitoredCounterGroup$Type.classUsed for keeping track of internal metrics using atomic integers</p>This is used by a variety of component types such as Sources, Channels,Sinks, SinkProcessors, ChannelProcessors, Interceptors and Serializers. Key for component's start time in MonitoredCounterGroup.counterMap key for component's stop time in MonitoredCounterGroup.counterMap Initialize the countersStarts the componentInitializes the values for the stop time as well as all the keys in theinternal map to zero and sets the start time to the current time inmilliseconds since midnight January 1, 1970 UTCRegisters the counter.This method is exposed only for testing, and there should be no need forany implementations to call this method directly.Shuts Down the ComponentUsed to indicate that the component is shutting down.Sets the stop time and then prints out the metrics fromthe internal map of keys to values for the following components:- ChannelCounter- ChannelProcessorCounter- SinkCounter- SinkProcessorCounter- SourceCounter Sets the stopTime for the component as the current time in milliseconds Prints out a message indicating that this component has been stopped Retrieve the type for this counter group Print out the startTime for this component Print out the stopTime for this component Retrieve and sort counter group map keys Cycle through and print out all the key value pairs in counterMap Retrieves the value from the original counterMap.Returns when this component was first startedReturns when this component was stoppedRetrieves the current value for this keyThe key for this metricThe current value for this keySets the value for this key to the given valueThe new value for this keyAtomically adds the delta to the current value for this keyThe updated value for this keyAtomically increments the current value for this key by oneComponent Enum ConstantsUsed by each component's constructor to distinguish which type thecomponent is./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/MonitoringType.java? extends MonitorServiceClass<? extends MonitorService>MonitoringTypeMonitoringType(java.lang.Class)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/MonitoringType.classmonitoringClassClass<HTTPMetricsServer>Class<PrometheusHTTPMetricsServer>klassEnum for Monitoring types./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/SinkCounter.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/SinkCounter.classCOUNTER_CHANNEL_READ_FAILCOUNTER_EVENT_WRITE_FAILCOUNTER_EVENT_DRAIN_SUCCESSCOUNTER_EVENT_DRAIN_ATTEMPTCOUNTER_BATCH_COMPLETECOUNTER_BATCH_UNDERFLOWCOUNTER_BATCH_EMPTYCOUNTER_CONNECTION_FAILEDCOUNTER_CONNECTION_CLOSEDCOUNTER_CONNECTION_CREATED"sink.connection.creation.count"sink.connection.creation.count"sink.connection.closed.count"sink.connection.closed.count"sink.connection.failed.count"sink.connection.failed.count"sink.batch.empty"sink.batch.empty"sink.batch.underflow"sink.batch.underflow"sink.batch.complete"sink.batch.complete"sink.event.drain.attempt"sink.event.drain.attempt"sink.event.drain.sucess"sink.event.drain.sucess"sink.event.write.fail"sink.event.write.fail"sink.channel.read.fail"sink.channel.read.fail/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/SinkCounterMBean.javaThis interface represents a sink counter mbean. Any class implementing{@linkplain org.apache.flume.instrumentation.SinkCounter} class./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/SinkProcessorCounter.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/SourceCounter.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/SourceCounter.classCOUNTER_CHANNEL_WRITE_FAILCOUNTER_GENERIC_PROCESSING_FAILCOUNTER_EVENT_READ_FAILCOUNTER_OPEN_CONNECTION_COUNTCOUNTER_APPEND_BATCH_ACCEPTEDCOUNTER_APPEND_BATCH_RECEIVEDCOUNTER_APPEND_ACCEPTEDCOUNTER_APPEND_RECEIVEDCOUNTER_EVENTS_ACCEPTEDCOUNTER_EVENTS_RECEIVED"src.events.received"src.events.received"src.events.accepted"src.events.accepted"src.append.received"src.append.received"src.append.accepted"src.append.accepted"src.append-batch.received"src.append-batch.received"src.append-batch.accepted"src.append-batch.accepted"src.open-connection.count"src.open-connection.count"src.event.read.fail"src.event.read.fail"src.generic.processing.fail"src.generic.processing.fail"src.channel.write.fail"src.channel.write.failopenConnectionCount/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/SourceCounterMBean.javaThis interface represents a source counter mbean. Any class implementing{@linkplain org.apache.flume.instrumentation.SourceCounter} class./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/http/HTTPMetricsServer.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/httporg.apache.flume.instrumentation.httpDEFAULT_PORT/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/http/HTTPMetricsServer.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/httpjettyServer41414httpConfigurationconnectorConnectionFactory[]"Error starting Jetty. JSON Metrics may not be available."Error starting Jetty. JSON Metrics may not be available."Error stopping Jetty. JSON Metrics may not be available."Error stopping Jetty. JSON Metrics may not be available./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/http/HTTPMetricsServer$HTTPMetricsHandler.classmapTypeHTTPMetricsHandlerHTTPMetricsHandler()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/http/HTTPMetricsServer$HTTPMetricsHandler$1.classcom.google.gson.reflectTypeToken<Map<String,Map<String,String>>>/Users/burakyetistiren/.m2/repository/com/google/code/gson/gson/2.9.1/gson-2.9.1.jar/Users/burakyetistiren/.m2/repository/com/google/code/gson/gson/2.9.1/Users/burakyetistiren/.m2/repository/com/google/code/gson/gson/Users/burakyetistiren/.m2/repository/com/google/code/gson/Users/burakyetistiren/.m2/repository/com/google/code/Users/burakyetistiren/.m2/repository/com/google/code/gson/gson/2.9.1/gson-2.9.1.jar/com/Users/burakyetistiren/.m2/repository/com/google/code/gson/gson/2.9.1/gson-2.9.1.jar/com/google/Users/burakyetistiren/.m2/repository/com/google/code/gson/gson/2.9.1/gson-2.9.1.jar/com/google/gson/Users/burakyetistiren/.m2/repository/com/google/code/gson/gson/2.9.1/gson-2.9.1.jar/com/google/gson/reflect/Users/burakyetistiren/.m2/repository/com/google/code/gson/gson/2.9.1/gson-2.9.1.jar/com/google/gson/reflect/TypeToken.classTypeToken<>TypeToken<?>getArraygetArray(java.lang.reflect.Type)getParameterizedgetParameterized(java.lang.reflect.Type,java.lang.reflect.Type[])TypeToken<T>get(java.lang.Class)get(java.lang.reflect.Type)isAssignableFrom(com.google.gson.reflect.TypeToken)isAssignableFrom(java.lang.reflect.Type)getRawTypegetRawType()? super Map<String,Map<String,String>>Class<? super Map<String,Map<String,String>>>TypeTokenTypeToken()TypeToken<Map<String,Map<String,String>>>()r1requestresponse"TRACE"TRACE"OPTIONS"OPTIONS"/""text/html;charset=utf-8"text/html;charset=utf-8"For Flume metrics please click"
                + " <a href = \"./metrics\"> here</a>."For Flume metrics please click <a href = "./metrics"> here</a>."/metrics"/metricsjson"application/json;charset=utf-8"application/json;charset=utf-8A Monitor service implementation that runs a web server on a configurableport and returns the metrics for components in JSON format. <p> Optionalparameters: <p> <tt>port</tt> : The port on which the server should listento.<p> Returns metrics in the following format: <p>{<p> "componentName1":{"metric1" : "metricValue1","metric2":"metricValue2"}<p> "componentName1":{"metric3" : "metricValue3","metric4":"metricValue4"}<p> }We can use Contexts etc if we have many urls to handle. For one url,specifying a handler directly is the most efficient. /metrics is the only place to pull metrics.If we want to use any other url for something else, we should make surethat for metrics only /metrics is used to prevent backwardcompatibility issues.Not handling the request returns a Not found error page./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/http/PrometheusHTTPMetricsServer.javarequests/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/http/PrometheusHTTPMetricsServer.classmbeanServerPROM_DEFAULT_PREFIX"Flume_"Flume_List<MetricFamilySamples>SequencedCollection<MetricFamilySamples>Collection<MetricFamilySamples>Iterable<MetricFamilySamples>Map<String,MetricFamilySamples>createGaugeIfNotExistscreateGaugeIfNotExists(java.util.List,java.util.Map,java.lang.String,java.util.List)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/http/PrometheusHTTPMetricsServer$FlumePrometheusCollector.classcreateCounterIfNotExistscreateCounterIfNotExists(java.util.List,java.util.Map,java.lang.String)makeStringPromSafemakeStringPromSafe(java.lang.String)Map<Object,Map<String,MetricFamilySamples>>processKafkaMetricprocessKafkaMetric(java.util.Map,java.util.List,javax.management.ObjectInstance)processFlumeMetricprocessFlumeMetric(java.util.Map,java.util.List,javax.management.ObjectInstance)FlumePrometheusCollectorFlumePrometheusCollector()counterMetricMapHashMap<Object,Map<String,MetricFamilySamples>>AbstractMap<Object,Map<String,MetricFamilySamples>>HashMap<Object,Map<String,MetricFamilySamples>>()? super Map<String,MetricFamilySamples>? extends Map<String,MetricFamilySamples>BiFunction<? super Map<String,MetricFamilySamples>,? super Map<String,MetricFamilySamples>,? extends Map<String,MetricFamilySamples>>merge(java.lang.Object,java.util.Map,java.util.function.BiFunction)? super ObjectBiFunction<? super Object,? super Map<String,MetricFamilySamples>,? extends Map<String,MetricFamilySamples>>Function<? super Object,? extends Map<String,MetricFamilySamples>>replace(java.lang.Object,java.util.Map)replace(java.lang.Object,java.util.Map,java.util.Map)putIfAbsent(java.lang.Object,java.util.Map)BiConsumer<? super Object,? super Map<String,MetricFamilySamples>>Entry<Object,Map<String,MetricFamilySamples>>Set<Entry<Object,Map<String,MetricFamilySamples>>>Collection<Entry<Object,Map<String,MetricFamilySamples>>>Iterable<Entry<Object,Map<String,MetricFamilySamples>>>Collection<Map<String,MetricFamilySamples>>Iterable<Map<String,MetricFamilySamples>>Map<? extends Object,? extends Map<String,MetricFamilySamples>>put(java.lang.Object,java.util.Map)AbstractMap<Object,Map<String,MetricFamilySamples>>()Node<Object,Map<String,MetricFamilySamples>>TreeNode<Object,Map<String,MetricFamilySamples>>newTreeNode(int,java.lang.Object,java.util.Map,java.util.HashMap.Node)newNode(int,java.lang.Object,java.util.Map,java.util.HashMap.Node)Node<Object,Map<String,MetricFamilySamples>>[]putVal(int,java.lang.Object,java.util.Map,boolean,boolean)HashMap<Object,Map<String,MetricFamilySamples>>(java.util.Map)HashMap<Object,Map<String,MetricFamilySamples>>(int)HashMap<Object,Map<String,MetricFamilySamples>>(int,float)mfsArrayList<MetricFamilySamples>AbstractList<MetricFamilySamples>AbstractCollection<MetricFamilySamples>ArrayList<MetricFamilySamples>()Spliterator<MetricFamilySamples>? super MetricFamilySamplesConsumer<? super MetricFamilySamples>Iterator<MetricFamilySamples>Stream<MetricFamilySamples>BaseStream<MetricFamilySamples,Stream<MetricFamilySamples>>Predicate<? super MetricFamilySamples>? extends MetricFamilySamplesCollection<? extends MetricFamilySamples>Iterable<? extends MetricFamilySamples>add(io.prometheus.client.Collector.MetricFamilySamples)AbstractCollection<MetricFamilySamples>()addLast(io.prometheus.client.Collector.MetricFamilySamples)addFirst(io.prometheus.client.Collector.MetricFamilySamples)ListIterator<MetricFamilySamples>add(int,io.prometheus.client.Collector.MetricFamilySamples)set(int,io.prometheus.client.Collector.MetricFamilySamples)Comparator<? super MetricFamilySamples>UnaryOperator<MetricFamilySamples>Function<MetricFamilySamples,MetricFamilySamples>AbstractList<MetricFamilySamples>()ArrayList<MetricFamilySamples>(java.util.Collection)ArrayList<MetricFamilySamples>(int)Set<ObjectInstance>Collection<ObjectInstance>Iterable<ObjectInstance>queryMBeans"org.apache.flume""kafka.consumer"kafka.consumer"kafka.producer"kafka.producer"metrics"metrics"Unable to poll JMX for metrics."Unable to poll JMX for metrics."Could not get Mbeans for monitoring"Could not get Mbeans for monitoringmbeanClassMBeanAttributeInfo[]strAttsattrList'='HashMap<String,MetricFamilySamples>AbstractMap<String,MetricFamilySamples>HashMap<String,MetricFamilySamples>()BiFunction<? super MetricFamilySamples,? super MetricFamilySamples,? extends MetricFamilySamples>merge(java.lang.String,io.prometheus.client.Collector.MetricFamilySamples,java.util.function.BiFunction)BiFunction<? super String,? super MetricFamilySamples,? extends MetricFamilySamples>Function<? super String,? extends MetricFamilySamples>replace(java.lang.String,io.prometheus.client.Collector.MetricFamilySamples)replace(java.lang.String,io.prometheus.client.Collector.MetricFamilySamples,io.prometheus.client.Collector.MetricFamilySamples)putIfAbsent(java.lang.String,io.prometheus.client.Collector.MetricFamilySamples)BiConsumer<? super String,? super MetricFamilySamples>getOrDefault(java.lang.Object,io.prometheus.client.Collector.MetricFamilySamples)Entry<String,MetricFamilySamples>Set<Entry<String,MetricFamilySamples>>Collection<Entry<String,MetricFamilySamples>>Iterable<Entry<String,MetricFamilySamples>>Map<? extends String,? extends MetricFamilySamples>put(java.lang.String,io.prometheus.client.Collector.MetricFamilySamples)AbstractMap<String,MetricFamilySamples>()Node<String,MetricFamilySamples>TreeNode<String,MetricFamilySamples>newTreeNode(int,java.lang.String,io.prometheus.client.Collector.MetricFamilySamples,java.util.HashMap.Node)newNode(int,java.lang.String,io.prometheus.client.Collector.MetricFamilySamples,java.util.HashMap.Node)Node<String,MetricFamilySamples>[]putVal(int,java.lang.String,io.prometheus.client.Collector.MetricFamilySamples,boolean,boolean)HashMap<String,MetricFamilySamples>(java.util.Map)HashMap<String,MetricFamilySamples>(int)HashMap<String,MetricFamilySamples>(int,float)methodmethodName"increment"increment"addTo"addTo"set""component"attrlocalAttrsamplesobjectNamequalifiedType"_"_TreeMap<String,String>/modules/java.base/java/util/TreeMap.classNavigableMap<String,String>SortedMap<String,String>SequencedMap<String,String>TreeMap<String,String>()SequencedSet<Entry<String,String>>SequencedCollection<Entry<String,String>>SequencedSet<String>putLast(java.lang.String,java.lang.String)putFirst(java.lang.String,java.lang.String)lastKeylastKey()firstKeyfirstKey()tailMaptailMap(java.lang.Object)tailMap(java.lang.String)headMapheadMap(java.lang.Object)headMap(java.lang.String)subMapsubMap(java.lang.Object,java.lang.Object)subMap(java.lang.String,java.lang.String)tailMap(java.lang.Object,boolean)tailMap(java.lang.String,boolean)headMap(java.lang.Object,boolean)headMap(java.lang.String,boolean)subMap(java.lang.Object,boolean,java.lang.Object,boolean)subMap(java.lang.String,boolean,java.lang.String,boolean)descendingKeySetdescendingKeySet()NavigableSet<String>SortedSet<String>navigableKeySetnavigableKeySet()descendingMapdescendingMap()higherKeyhigherKey(java.lang.Object)higherKey(java.lang.String)higherEntryhigherEntry(java.lang.Object)higherEntry(java.lang.String)ceilingKeyceilingKey(java.lang.Object)ceilingKey(java.lang.String)ceilingEntryceilingEntry(java.lang.Object)ceilingEntry(java.lang.String)floorKeyfloorKey(java.lang.Object)floorKey(java.lang.String)floorEntryfloorEntry(java.lang.Object)floorEntry(java.lang.String)lowerKeylowerKey(java.lang.Object)lowerKey(java.lang.String)lowerEntrylowerEntry(java.lang.Object)lowerEntry(java.lang.String)descendingKeySpliteratordescendingKeySpliterator()keySpliteratorForkeySpliteratorFor(java.util.NavigableMap)NavigableMap<K,?>SortedMap<K,?>SequencedMap<K,?>Map<K,?>SortedSet<? extends K>Set<? extends K>Collection<? extends K>Iterable<? extends K>SequencedSet<? extends K>SequencedCollection<? extends K>addAllForTreeSetaddAllForTreeSet(java.util.SortedSet,java.lang.Object)SortedSet<? extends String>Set<? extends String>SequencedSet<? extends String>SequencedCollection<? extends String>addAllForTreeSet(java.util.SortedSet,java.lang.String)readTreeSetreadTreeSet(int,java.io.ObjectInputStream,java.lang.Object)readTreeSet(int,java.io.ObjectInputStream,java.lang.String)/modules/java.base/java/util/TreeMap$Entry.classpredecessorpredecessor(java.util.TreeMap.Entry)successorsuccessor(java.util.TreeMap.Entry)getLastEntrygetLastEntry()getFirstEntrygetFirstEntry()key(java.util.TreeMap.Entry)Entry<K,?>keyOrNullkeyOrNull(java.util.TreeMap.Entry)exportEntryexportEntry(java.util.TreeMap.Entry)valEqualsvalEquals(java.lang.Object,java.lang.Object)descendingKeyIteratordescendingKeyIterator()getLowerEntrygetLowerEntry(java.lang.Object)getLowerEntry(java.lang.String)getHigherEntrygetHigherEntry(java.lang.Object)getHigherEntry(java.lang.String)getFloorEntrygetFloorEntry(java.lang.Object)getFloorEntry(java.lang.String)getCeilingEntrygetCeilingEntry(java.lang.Object)getCeilingEntry(java.lang.String)getEntryUsingComparatorgetEntryUsingComparator(java.lang.Object)getEntrygetEntry(java.lang.Object)SortedMap<K,? extends V>SequencedMap<K,? extends V>Map<K,? extends V>TreeMapTreeMap(java.util.SortedMap)SortedMap<String,? extends String>SequencedMap<String,? extends String>Map<String,? extends String>TreeMap<String,String>(java.util.SortedMap)TreeMap(java.util.Map)TreeMap<String,String>(java.util.Map)TreeMap(java.util.Comparator)TreeMap<String,String>(java.util.Comparator)TreeMap()metricKey? extends CharSequenceIterable<? extends CharSequence>Hashtable<String,String>/modules/java.base/java/util/Hashtable.classDictionary<String,String>/modules/java.base/java/util/Dictionary.classEnumeration<>/modules/java.base/java/util/Enumeration.classEnumeration<V>elements()Enumeration<String>Enumeration<K>DictionaryDictionary()Dictionary<String,String>()readHashtablereadHashtable(java.io.ObjectInputStream)defaultWriteHashtabledefaultWriteHashtable(java.io.ObjectOutputStream,int,float)writeHashtablewriteHashtable(java.io.ObjectOutputStream)Hashtable<>Dictionary<>Hashtable<?,?>Dictionary<?,?>Map<?,?>cloneHashtablecloneHashtable()rehashrehash()HashtableHashtable(java.lang.Void)Hashtable<String,String>(java.lang.Void)Hashtable(java.util.Map)Hashtable<String,String>(java.util.Map)Hashtable()Hashtable<String,String>()Hashtable(int)Hashtable<String,String>(int)Hashtable(int,float)Hashtable<String,String>(int,float)"Metric {} could not be monitored"Metric {} could not be monitored"[.\\-]"[.\-]labeledCountergaugeNamelabelNameslabelledGauge"Error stopping Jetty. Prometheus Metrics may not be available."Error stopping Jetty. Prometheus Metrics may not be available. We create a unique name for the metric based on the metric that came from Kafka, plus all of the properties. Unfortunately Kafka does not have unique metric names and therefore you can end up with metrics with differing property lists (which you can't have. Get the attribute list now as we'll need it to create the gauge We pre-create each metric (once) before populating it once for each matching mbeanPrometeus is really unhappy with metrics with , or - in, so replace them/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/kafka/KafkaChannelCounter.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/kafkaorg.apache.flume.instrumentation.kafka/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/kafka/KafkaChannelCounter.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/kafkaCOUNT_ROLLBACKTIMER_KAFKA_COMMITTIMER_KAFKA_EVENT_SENDTIMER_KAFKA_EVENT_GET"channel.kafka.event.get.time"channel.kafka.event.get.time"channel.kafka.event.send.time"channel.kafka.event.send.time"channel.kafka.commit.time"channel.kafka.commit.time"channel.rollback.count"channel.rollback.count/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/kafka/KafkaChannelCounterMBean.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/kafka/KafkaSinkCounter.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/kafka/KafkaSinkCounter.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/kafka/KafkaSinkCounterMBean.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/kafka/KafkaSourceCounter.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/kafka/KafkaSourceCounter.classCOUNTER_KAFKA_EMPTY"source.kafka.event.get.time"source.kafka.event.get.time"source.kafka.commit.time"source.kafka.commit.time"source.kafka.empty.count"source.kafka.empty.count/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/kafka/KafkaSourceCounterMBean.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/util/JMXPollUtil.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/instrumentation/utilorg.apache.flume.instrumentation.util/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/util/JMXPollUtil.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/instrumentation/utilClass<JMXPollUtil>mbeanMapHashMap<String,Map<String,String>>AbstractMap<String,Map<String,String>>attrMap/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/interceptor/HostInterceptor.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/interceptororg.apache.flume.interceptorHostInterceptorHostInterceptor(boolean,boolean,java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptor/HostInterceptor.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptorpreserveExistingClass<HostInterceptor>useIP"Could not get local host address. Exception follows."Could not get local host address. Exception follows./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptor/HostInterceptor$Builder.class"preserveExisting""useIP""hostHeader"hostHeaderSimple Interceptor class that sets the host name or IP on all eventsthat are intercepted.<p>The host header is named <code>host</code> and its format is either the FQDNor IP of the host on which this interceptor is run.Properties:<p>preserveExisting: Whether to preserve an existing value for 'host'(default is false)<p>useIP: Whether to use IP address or fully-qualified hostname for 'host'header value (default is true)<p>hostHeader: Specify the key to be used in the event header map for thehost name. (default is "host") <p>Sample config:<p>agent.sources.r1.channels = c1<p>agent.sources.r1.type = SEQ<p>agent.sources.r1.interceptors = i1<p>agent.sources.r1.interceptors.i1.type = host<p>agent.sources.r1.interceptors.i1.preserveExisting = true<p>agent.sources.r1.interceptors.i1.useIP = false<p>agent.sources.r1.interceptors.i1.hostHeader = hostname<p>Only {@link HostInterceptor.Builder} can build meModifies events in-place.Delegates to {@link #intercept(Event)} in a loop.Builder which builds new instances of the HostInterceptor./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/interceptor/Interceptor.javaAny initialization / startup needed by the Interceptor.Interception of a single {@link Event}.Event to be interceptedOriginal or modified event, or {@code null} if the Eventis to be dropped (i.e. filtered out).Interception of a batch of {@linkplain Event events}.Input list of eventsOutput list of events. The size of output list MUST NOT BE GREATERthan the size of the input list (i.e. transformation and removal ONLY).Also, this method MUST NOT return {@code null}. If all events are dropped,then an empty List is returned.Perform any closing / shutdown needed by the Interceptor./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/interceptor/InterceptorBuilderFactory.javalookuplookup(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptor/InterceptorBuilderFactory.classFactory used to register instances of Interceptors & their builders,as well as to instantiate the builders.Instantiate specified class, either alias or fully-qualified class name./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/interceptor/InterceptorChain.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptor/InterceptorChain.classinterceptor"Event list returned null from interceptor %s"Event list returned null from interceptor %sImplementation of Interceptor that calls a list of other Interceptorsserially. list of interceptors that will be traversed, in order/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/interceptor/InterceptorType.javaInterceptorTypeInterceptorType(java.lang.Class)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptor/InterceptorType.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/interceptor/RegexExtractorInterceptor.javaList<NameAndSerializer>SequencedCollection<NameAndSerializer>Collection<NameAndSerializer>Iterable<NameAndSerializer>RegexExtractorInterceptorRegexExtractorInterceptor(java.util.regex.Pattern,java.util.List)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptor/RegexExtractorInterceptor.classserializersregexSERIALIZERSREGEX"regex""serializers"Class<RegexExtractorInterceptor>groupgroupIndexserializerSpliterator<NameAndSerializer>? super NameAndSerializerConsumer<? super NameAndSerializer>Iterator<NameAndSerializer>Stream<NameAndSerializer>BaseStream<NameAndSerializer,Stream<NameAndSerializer>>Predicate<? super NameAndSerializer>? extends NameAndSerializerCollection<? extends NameAndSerializer>Iterable<? extends NameAndSerializer>add(org.apache.flume.interceptor.RegexExtractorInterceptor.NameAndSerializer)addLast(org.apache.flume.interceptor.RegexExtractorInterceptor.NameAndSerializer)addFirst(org.apache.flume.interceptor.RegexExtractorInterceptor.NameAndSerializer)ListIterator<NameAndSerializer>add(int,org.apache.flume.interceptor.RegexExtractorInterceptor.NameAndSerializer)set(int,org.apache.flume.interceptor.RegexExtractorInterceptor.NameAndSerializer)Comparator<? super NameAndSerializer>UnaryOperator<NameAndSerializer>Function<NameAndSerializer,NameAndSerializer>"Skipping group {} to {} due to missing serializer"Skipping group {} to {} due to missing serializer"Serializing {} using {}"Serializing {} using {}interceptedinterceptedEventgetCustomSerializergetCustomSerializer(java.lang.String,org.apache.flume.Context)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptor/RegexExtractorInterceptor$Builder.classconfigureSerializersconfigureSerializers(org.apache.flume.Context)defaultSerializerserializerListregexString"Must supply a valid regex string"Must supply a valid regex stringserializerListStrserializerNamesserializerContexts"Must supply at least one name and serializer"Must supply at least one name and serializerArrayList<NameAndSerializer>AbstractList<NameAndSerializer>AbstractCollection<NameAndSerializer>serializerNameserializerContext"DEFAULT"DEFAULT"Supplied name cannot be empty."Supplied name cannot be empty.clazzNameMap<String,?>TypeVariable<Class<?>>TypeVariable<Class<?>>[]"Could not instantiate event serializer."Could not instantiate event serializer."Regex pattern was misconfigured"Regex pattern was misconfigured"Must supply a valid group match id list"Must supply a valid group match id list/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptor/RegexExtractorInterceptor$NameAndSerializer.classInterceptor that extracts matches using a specified regular expression andappends the matches to the event headers using the specified serializers</p>Note that all regular expression matching occurs through Java's built injava.util.regex package</p>. Properties:regex: The regex to useserializers: Specifies the group the serializer will be applied to, and thename of the header that will be added. If no serializer is specified for agroup the default {@link RegexExtractorInterceptorPassThroughSerializer} willbe usedSample config:agent.sources.r1.channels = c1agent.sources.r1.type = SEQagent.sources.r1.interceptors = i1agent.sources.r1.interceptors.i1.type = REGEX_EXTRACTORagent.sources.r1.interceptors.i1.regex = (WARNING)|(ERROR)|(FATAL)agent.sources.r1.interceptors.i1.serializers = s1 s2agent.sources.r1.interceptors.i1.serializers.s1.type = com.blah.SomeSerializeragent.sources.r1.interceptors.i1.serializers.s1.name = warningagent.sources.r1.interceptors.i1.serializers.s2.type =org.apache.flume.interceptor.RegexExtractorInterceptorTimestampSerializeragent.sources.r1.interceptors.i1.serializers.s2.name = erroragent.sources.r1.interceptors.i1.serializers.s2.dateFormat = yyyy-MM-ddExample 1:EventBody: 1:2:3.4foobar5</p> Configuration:agent.sources.r1.interceptors.i1.regex = (\\d):(\\d):(\\d)agent.sources.r1.interceptors.i1.serializers = s1 s2 s3agent.sources.r1.interceptors.i1.serializers.s1.name = oneagent.sources.r1.interceptors.i1.serializers.s2.name = twoagent.sources.r1.interceptors.i1.serializers.s3.name = threeresults in an event with the the followingbody: 1:2:3.4foobar5 headers: one=>1, two=>2, three=3Example 2:EventBody: 1:2:3.4foobar5Configuration: agent.sources.r1.interceptors.i1.regex = (\\d):(\\d):(\\d)body: 1:2:3.4foobar5 headers: one=>1, two=>2 NO-OP.../Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/interceptor/RegexExtractorInterceptorMillisSerializer.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptor/RegexExtractorInterceptorMillisSerializer.class"pattern""Must configure with a valid pattern"Must configure with a valid patterndateTimeSerializer that converts the passed in value into milliseconds using thespecified formatting pattern/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/interceptor/RegexExtractorInterceptorPassThroughSerializer.javaSerializer that simply returns the passed in value/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/interceptor/RegexExtractorInterceptorSerializer.javaSerializer for serializing groups matched by the{@link RegexExtractorInterceptor}The value extracted by the {@link RegexExtractorInterceptor}The serialized version of the specified value/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/interceptor/RegexFilteringInterceptor.javaDEFAULT_EXCLUDE_EVENTSDEFAULT_REGEXEXCLUDE_EVENTSRegexFilteringInterceptorRegexFilteringInterceptor(java.util.regex.Pattern,boolean)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptor/RegexFilteringInterceptor.classexcludeEventsClass<RegexFilteringInterceptor>outEvent/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptor/RegexFilteringInterceptor$Builder.class"Creating RegexFilteringInterceptor: regex=%s,excludeEvents=%s"Creating RegexFilteringInterceptor: regex=%s,excludeEvents=%s".*""excludeEvents"Interceptor that filters events selectively based on a configured regularexpression matching against the event body.This supports either include- or exclude-based filtering. A giveninterceptor can only perform one of these functions, but multipleinterceptor can be chained together to create more complexinclusion/exclusion patterns. If include-based filtering is configured, thenall events matching the supplied regular expression will be passed throughand all events not matching will be ignored. If exclude-based filtering isconfigured, than all events matching will be ignored, and all other eventswill pass through.java.util.regex package.regex: Regular expression for matching excluded events.(default is ".*")<p>excludeEvents: If true, a regex match determines events to exclude,otherwise a regex determines events to includeagent.sources.r1.interceptors.i1.type = REGEX<p>agent.sources.r1.interceptors.i1.regex = (WARNING)|(ERROR)|(FATAL)<p>Only {@link RegexFilteringInterceptor.Builder} can build meReturns the event if it passes the regular expression filter and nullotherwise. We've already ensured here that at most one of includeRegex and excludeRegex are defined.Returns the set of events which pass filters, according to{@link #intercept(Event)}.Builder which builds new instance of the RegexFilteringInterceptor./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/interceptor/RemoveHeaderInterceptor.javaRemoveHeaderInterceptorRemoveHeaderInterceptor(java.lang.String,java.lang.String,java.lang.String,java.util.regex.Pattern)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptor/RemoveHeaderInterceptor.classmatchRegexfromListwithNameMATCH_REGEXLIST_SEPARATOR_DEFAULTLIST_SEPARATORFROM_LISTWITH_NAME"withName""fromList""fromListSeparator"fromListSeparator"\\s*,\\s*"\s*,\s*"matching"matchingClass<RemoveHeaderInterceptor>listSeparator"Default value used otherwise"Default value used otherwise"Missing Flume event while intercepting"Missing Flume event while intercepting"Removed header \"{}\" for event: {}"Removed header "{}" for event: {}headerIteratorremovedHeadersLinkedList<String>AbstractSequentialList<String>Deque<String>Queue<String>LinkedList<String>()AbstractSequentialList<String>()offer(java.lang.String)push(java.lang.String)offerLast(java.lang.String)offerFirst(java.lang.String)Node<String>linkBefore(java.lang.String,java.util.LinkedList.Node)linkLast(java.lang.String)LinkedList<String>(java.util.Collection)currentHeader"Removed headers \"{}\" for event: {}"Removed headers "{}" for event: {}"Failed to process event "Failed to process event /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptor/RemoveHeaderInterceptor$Builder.class"Creating RemoveHeaderInterceptor with: withName={}, fromList={}, " +
            "listSeparator={}, matchRegex={}"Creating RemoveHeaderInterceptor with: withName={}, fromList={}, listSeparator={}, matchRegex={}matchRegexStrThis interceptor manipulates Flume event headers, by removing one or manyheaders. It can remove a statically defined header, headers based on aregular expression or headers in a list. If none of these is defined, or ifno header matches the criteria, the Flume events are not modified.<br />Note that if only one header needs to be removed, specifying it by nameprovides performance benefits over the other two methods.<br /><br />Properties:<br />- .withName (optional): name of the header to remove<br />- .fromList (optional): list of headers to remove, separated with theseparator specified with .from.list.separator<br />- .fromListSeparator (optional): regular expression used to separatemultiple header names in the list specified using .from.list<br />- .matching (optional): All the headers which names match this regular expression areOnly {@link RemoveHeaderInterceptor.Builder} can build meorg.apache.flume.interceptor.Interceptor#initialize() Nothing to doorg.apache.flume.interceptor.Interceptor#close()org.apache.flume.interceptor.Interceptor#intercept(java.util.List)org.apache.flume.interceptor.Interceptor#intercept(org.apache.flume.Event) If withName matches, removing it directly Also, we need to go through the listBuilder which builds new instances of the {@link RemoveHeaderInterceptor}.org.apache.flume.interceptor.Interceptor.Builder#build()org.apache.flume.conf.Configurable#configure(org.apache.flume.Context)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/interceptor/SearchAndReplaceInterceptor.javaSearchAndReplaceInterceptorSearchAndReplaceInterceptor(java.util.regex.Pattern,java.lang.String,java.nio.charset.Charset)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptor/SearchAndReplaceInterceptor.classreplaceStringsearchPatternClass<SearchAndReplaceInterceptor>origBodynewBody/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptor/SearchAndReplaceInterceptor$Builder.classsearchRegexREPLACE_STRING_KEYSEARCH_PAT_KEY"searchPattern""replaceString""Must supply a valid search pattern "Must supply a valid search pattern " (may not be empty)" (may not be empty)"Regular expression search pattern required"Regular expression search pattern required"Replacement string required"Replacement string requiredInterceptor that allows search-and-replace of event body strings usingregular expressions. This only works with event bodies that are validstrings. The charset is configurable.Usage:agent.source-1.interceptors.search-replace.searchPattern = ^INFO:agent.source-1.interceptors.search-replace.replaceString = Log msg:Any regular expression search pattern and replacement pattern that can beused with {@link java.util.regex.Matcher#replaceAll(String)} may be used,including backtracking and grouping. Empty replacement String value or if the property itself is not present assign empty string as replacement May throw IllegalArgumentException for unsupported charsets./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/interceptor/StaticInterceptor.javaStaticInterceptorStaticInterceptor(boolean,java.lang.String,java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptor/StaticInterceptor.classClass<StaticInterceptor>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptor/StaticInterceptor$Builder.class"Creating StaticInterceptor: preserveExisting=%s,key=%s,value=%s"Creating StaticInterceptor: preserveExisting=%s,key=%s,value=%s"value"Interceptor class that appends a static, pre-configured header to all events.key: Key to use in static header insertion.(default is "key")<p>value: Value to use in static header insertion.(default is "value")<p>preserveExisting: Whether to preserve an existing value for 'key'(default is true)<p>agent.sources.r1.interceptors.i1.type = static<p>agent.sources.r1.interceptors.i1.preserveExisting = false<p>agent.sources.r1.interceptors.i1.key = datacenter<p>agent.sources.r1.interceptors.i1.value= NYC_01<p>Builder which builds new instance of the StaticInterceptor./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/interceptor/TimestampInterceptor.javaTimestampInterceptorTimestampInterceptor(boolean,java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptor/TimestampInterceptor.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/interceptor/TimestampInterceptor$Builder.class"headerName"Simple Interceptor class that sets the current system timestamp on all eventsthat are intercepted.By convention, this timestamp header is named "timestamp" by default and its formatis a "stringified" long timestamp in milliseconds since the UNIX epoch.The name of the header can be changed through the configuration using theconfig key "header".Only {@link TimestampInterceptor.Builder} can build me we must preserve the existing timestampBuilder which builds new instances of the TimestampInterceptor./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/lifecycle/LifecycleAware.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/lifecycleAn interface implemented by any class that has a defined, stateful,lifecycle.Implementations of {@link LifecycleAware} conform to a standard method ofstarting, stopping, and reporting their current state. Additionally, thisinterface creates a standard method of communicating failure to perform alifecycle operation to the caller (i.e. via {@link LifecycleException}). Itis never considered valid to call {@link #start()} or{@link #stop()} more than once or to call them in the wrong order.While this is not strictly enforced, it may be in the future.Example services may include Flume nodes and the master, but also lower levelcomponents that can be controlled in a similar manner.Example usage{@codepublic class MyService implements LifecycleAware {private LifecycleState lifecycleState;public MyService() {lifecycleState = LifecycleState.IDLE;@Overridepublic void start(Context context) throws LifecycleException, InterruptedException {// ...your code does something.lifecycleState = LifecycleState.START;public void stop(Context context) throws LifecycleException, InterruptedException {// ...you stop services here.} catch (SomethingException) {lifecycleState = LifecycleState.ERROR;lifecycleState = LifecycleState.STOP;public LifecycleState getLifecycleState() {return lifecycleState;Starts a service or component.Implementations should determine the result of any start logic and effectthe return value of {@link #getLifecycleState()} accordingly.LifecycleExceptionStops a service or component.Implementations should determine the result of any stop logic and effectReturn the current state of the service or component./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/lifecycle/LifecycleController.javamaxNumberOfChecks/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/lifecycle/LifecycleController.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/lifecycleshortestSleepDurationClass<LifecycleController>delegateLifecycleState[]statessleepIntervaldeadLine"Waiting for state {} for delegate:{} up to {}ms"Waiting for state {} for delegate:{} up to {}msEnum<LifecycleState>Comparable<LifecycleState>compareTo(org.apache.flume.lifecycle.LifecycleState)EnumDesc<LifecycleState>DynamicConstantDesc<LifecycleState>Optional<EnumDesc<LifecycleState>>Class<LifecycleState>Enum<LifecycleState>(java.lang.String,int)"Didn't see {} state(s) within timeout of {}ms"Didn't see {} state(s) within timeout of {}msList<LifecycleAware>SequencedCollection<LifecycleAware>Collection<LifecycleAware>Iterable<LifecycleAware>services/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/lifecycle/LifecycleException.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/lifecycle/LifecycleException.class4689000562519155240L4689000562519155240/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/lifecycle/LifecycleState.javaLifecycleStateLifecycleState()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/lifecycle/LifecycleState.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/lifecycle/LifecycleSupervisor.javaneedToPurge/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/lifecycle/LifecycleSupervisor.classpurgermonitorServiceMap<LifecycleAware,ScheduledFuture<?>>monitorFuturesMap<LifecycleAware,Supervisoree>supervisedProcessesClass<LifecycleSupervisor>HashMap<LifecycleAware,Supervisoree>AbstractMap<LifecycleAware,Supervisoree>HashMap<LifecycleAware,Supervisoree>()? super Supervisoree? extends SupervisoreeBiFunction<? super Supervisoree,? super Supervisoree,? extends Supervisoree>merge(org.apache.flume.lifecycle.LifecycleAware,org.apache.flume.lifecycle.LifecycleSupervisor.Supervisoree,java.util.function.BiFunction)? super LifecycleAwareBiFunction<? super LifecycleAware,? super Supervisoree,? extends Supervisoree>compute(org.apache.flume.lifecycle.LifecycleAware,java.util.function.BiFunction)computeIfPresent(org.apache.flume.lifecycle.LifecycleAware,java.util.function.BiFunction)Function<? super LifecycleAware,? extends Supervisoree>computeIfAbsent(org.apache.flume.lifecycle.LifecycleAware,java.util.function.Function)replace(org.apache.flume.lifecycle.LifecycleAware,org.apache.flume.lifecycle.LifecycleSupervisor.Supervisoree)replace(org.apache.flume.lifecycle.LifecycleAware,org.apache.flume.lifecycle.LifecycleSupervisor.Supervisoree,org.apache.flume.lifecycle.LifecycleSupervisor.Supervisoree)putIfAbsent(org.apache.flume.lifecycle.LifecycleAware,org.apache.flume.lifecycle.LifecycleSupervisor.Supervisoree)BiConsumer<? super LifecycleAware,? super Supervisoree>getOrDefault(java.lang.Object,org.apache.flume.lifecycle.LifecycleSupervisor.Supervisoree)Entry<LifecycleAware,Supervisoree>Set<Entry<LifecycleAware,Supervisoree>>Collection<Entry<LifecycleAware,Supervisoree>>Iterable<Entry<LifecycleAware,Supervisoree>>Collection<Supervisoree>Iterable<Supervisoree>Set<LifecycleAware>? extends LifecycleAwareMap<? extends LifecycleAware,? extends Supervisoree>put(org.apache.flume.lifecycle.LifecycleAware,org.apache.flume.lifecycle.LifecycleSupervisor.Supervisoree)AbstractMap<LifecycleAware,Supervisoree>()Node<LifecycleAware,Supervisoree>TreeNode<LifecycleAware,Supervisoree>newTreeNode(int,org.apache.flume.lifecycle.LifecycleAware,org.apache.flume.lifecycle.LifecycleSupervisor.Supervisoree,java.util.HashMap.Node)newNode(int,org.apache.flume.lifecycle.LifecycleAware,org.apache.flume.lifecycle.LifecycleSupervisor.Supervisoree,java.util.HashMap.Node)Node<LifecycleAware,Supervisoree>[]putVal(int,org.apache.flume.lifecycle.LifecycleAware,org.apache.flume.lifecycle.LifecycleSupervisor.Supervisoree,boolean,boolean)HashMap<LifecycleAware,Supervisoree>(java.util.Map)HashMap<LifecycleAware,Supervisoree>(int)HashMap<LifecycleAware,Supervisoree>(int,float)HashMap<LifecycleAware,ScheduledFuture<?>>AbstractMap<LifecycleAware,ScheduledFuture<?>>HashMap<LifecycleAware,ScheduledFuture<?>>()? super ScheduledFuture<?>? extends ScheduledFuture<?>BiFunction<? super ScheduledFuture<?>,? super ScheduledFuture<?>,? extends ScheduledFuture<?>>merge(org.apache.flume.lifecycle.LifecycleAware,java.util.concurrent.ScheduledFuture,java.util.function.BiFunction)BiFunction<? super LifecycleAware,? super ScheduledFuture<?>,? extends ScheduledFuture<?>>Function<? super LifecycleAware,? extends ScheduledFuture<?>>replace(org.apache.flume.lifecycle.LifecycleAware,java.util.concurrent.ScheduledFuture)replace(org.apache.flume.lifecycle.LifecycleAware,java.util.concurrent.ScheduledFuture,java.util.concurrent.ScheduledFuture)putIfAbsent(org.apache.flume.lifecycle.LifecycleAware,java.util.concurrent.ScheduledFuture)BiConsumer<? super LifecycleAware,? super ScheduledFuture<?>>getOrDefault(java.lang.Object,java.util.concurrent.ScheduledFuture)Entry<LifecycleAware,ScheduledFuture<?>>Set<Entry<LifecycleAware,ScheduledFuture<?>>>Collection<Entry<LifecycleAware,ScheduledFuture<?>>>Iterable<Entry<LifecycleAware,ScheduledFuture<?>>>Collection<ScheduledFuture<?>>Iterable<ScheduledFuture<?>>Map<? extends LifecycleAware,? extends ScheduledFuture<?>>put(org.apache.flume.lifecycle.LifecycleAware,java.util.concurrent.ScheduledFuture)AbstractMap<LifecycleAware,ScheduledFuture<?>>()Node<LifecycleAware,ScheduledFuture<?>>TreeNode<LifecycleAware,ScheduledFuture<?>>newTreeNode(int,org.apache.flume.lifecycle.LifecycleAware,java.util.concurrent.ScheduledFuture,java.util.HashMap.Node)newNode(int,org.apache.flume.lifecycle.LifecycleAware,java.util.concurrent.ScheduledFuture,java.util.HashMap.Node)Node<LifecycleAware,ScheduledFuture<?>>[]putVal(int,org.apache.flume.lifecycle.LifecycleAware,java.util.concurrent.ScheduledFuture,boolean,boolean)HashMap<LifecycleAware,ScheduledFuture<?>>(java.util.Map)HashMap<LifecycleAware,ScheduledFuture<?>>(int)HashMap<LifecycleAware,ScheduledFuture<?>>(int,float)"lifecycleSupervisor-"lifecycleSupervisor-"-%d"-%d"Starting lifecycle supervisor {}"Starting lifecycle supervisor {}"Lifecycle supervisor started"Lifecycle supervisor started"Stopping lifecycle supervisor {}"Stopping lifecycle supervisor {}"Interrupted while waiting for monitor service to stop"Interrupted while waiting for monitor service to stopsetValue(org.apache.flume.lifecycle.LifecycleSupervisor.Supervisoree)"Lifecycle supervisor stopped"Lifecycle supervisor stoppedlifecycleAwaredesiredStateprocessmonitorRunnablefuture"Supervise called on "Supervise called on " " +
          "after shutdown has been initiated. " after shutdown has been initiated. " will not" +
          " be started" will not be started"Refusing to supervise "Refusing to supervise " more than once" more than once"Supervising service:{} policy:{} desiredState:{}"Supervising service:{} policy:{} desiredState:{}"Unaware of "Unaware of " - can not unsupervise" - can not unsupervise"Unsupervising service:{}"Unsupervising service:{}supervisoree"Stopping component: {}"Stopping component: {}cancelcancel(boolean)state()exceptionNowexceptionNow()resultNowresultNow()get(long,java.util.concurrent.TimeUnit)isDoneisDone()isCancelledisCancelled()" - can not set desired state to " - can not set desired state to "Setting desiredState:{} on service:{}"Setting desiredState:{} on service:{}"checking process:{} supervisoree:{}"checking process:{} supervisoree:{}"first time seeing {}"first time seeing {}"Component has already been stopped {}"Component has already been stopped {}"Component {} is in error state, and Flume will not"
                + "attempt to change its state"Component {} is in error state, and Flume will notattempt to change its state"Want to transition {} from {} to {} (failures:{})"Want to transition {} from {} to {} (failures:{})"Unable to start "Unable to start " - Exception follows." - Exception follows."Component {} stopped, since it could not be"
                          + "successfully started due to missing dependencies"Component {} stopped, since it could not besuccessfully started due to missing dependenciese1"Unsuccessful attempt to "
                          + "shutdown component: {} due to missing dependencies."
                          + " Please shutdown the agent"
                          + "or disable this component, or the agent will be"
                          + "in an undefined state."Unsuccessful attempt to shutdown component: {} due to missing dependencies. Please shutdown the agentor disable this component, or the agent will bein an undefined state."Unable to stop "Unable to stop "I refuse to acknowledge {} as a desired state"I refuse to acknowledge {} as a desired state"Policy {} of {} has been violated - supervisor should exit!"Policy {} of {} has been violated - supervisor should exit!"Status check complete"Status check completePurgerPurger()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/lifecycle/LifecycleSupervisor$Purger.class"{ lastSeen:"{ lastSeen:" lastSeenState:" lastSeenState:" desiredState:" desiredState:" firstSeen:" firstSeen:" failures:" failures:" discard:" discard:" error:" error:isValid(org.apache.flume.lifecycle.LifecycleAware,org.apache.flume.lifecycle.LifecycleSupervisor.Status)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/lifecycle/LifecycleSupervisor$SupervisorPolicy.classobject/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/lifecycle/LifecycleSupervisor$SupervisorPolicy$AlwaysRestartPolicy.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/lifecycle/LifecycleSupervisor$SupervisorPolicy$OnceOnlyPolicy.classSupervisoreeSupervisoree()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/lifecycle/LifecycleSupervisor$Supervisoree.class"{ status:"{ status:" policy:" policy:If we've failed, preserve the error state.We need to do this because a reconfiguration simply unsupervises oldcomponents and supervises new ones.purges are expensive, so it is done only once every 2 hours. Unsupervise has already been called on this. This component can never recover, shut it down. Set the state to stop, so that the conf poller can proceed./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/netty/filter/PatternRule.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/netty/filter/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/nettyorg.apache.flume.netty.filterparseparse(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/netty/filter/PatternRule.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/netty/filter/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/nettyisLocalhostisLocalhost(java.net.InetAddress)addRuleaddRule(java.lang.String,java.lang.String)namePatternipPatternruleTypeLOCALHOSTClass<PatternRule>"127.0.0.1"127.0.0.1rule"|"|"\\."\."\\\\."\\."\\*"\*"\\?"\?'('')'address"error getting ip of localhost"error getting ip of localhostInetAddress[]addrsinetSocketAddressinetAddressacls"n:localhost"n:localhost"n:"n:"i:"i:contributor license agreements. See the NOTICE file distributed withthis work for additional information regarding copyright ownership.The ASF licenses this file to You under the Apache license, Version 2.0(the "License"); you may not use this file except in compliance withthe License. You may obtain a copy of the License atSee the license for the specific language governing permissions andlimitations under the license.The Class PatternRule represents an IP filter rule using string patterns.<br>Rule Syntax:Rule ::= [n|i]:address          n stands for computer name, i for ip addressaddress ::= &lt;regex&gt; | localhostregex is a regular expression with '*' as multi character and '?' as single character wild cardExample: allow localhost:new PatternRule(true, "n:localhost")Example: allow local lan:new PatternRule(true, "i:192.168.0.*")Example: block allnew PatternRule(false, "n:*")For some reason Netty 4 didn't copy this from Netty 3. The code was copied from the Netty 3 PatternRuleand modifed as required to match the new version of IpFilterRule.Construct the IpFilterRule from a pattern.The RuleType (accept or deny)The pattern./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/AbstractAvroEventSerializer.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serializationorg.apache.flume.serializationorg.apache.avro.fileDataFileWriter<T>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/file/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/file/DataFileWriter.classdataFileWriter/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/serialization/AbstractAvroEventSerializer.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/serializationDatumWriter<T>AbstractAvroEventSerializer<>Class<AbstractAvroEventSerializer<>>syncIntervalBytescompressionCodecReflectDatumWriter<T>SpecificDatumWriter<T>GenericDatumWriter<T>ReflectDatumWriter<T>(org.apache.avro.Schema)GenericDatumWriter<T>(org.apache.avro.Schema,org.apache.avro.generic.GenericData)GenericDatumWriter<T>(org.apache.avro.Schema)GenericDatumWriter<T>(org.apache.avro.generic.GenericData)GenericDatumWriter<T>()SpecificDatumWriter<T>(org.apache.avro.specific.SpecificData)SpecificDatumWriter<T>(org.apache.avro.Schema,org.apache.avro.specific.SpecificData)SpecificDatumWriter<T>(org.apache.avro.Schema)SpecificDatumWriter<T>(java.lang.Class)SpecificDatumWriter<T>()ReflectDatumWriter<T>(org.apache.avro.reflect.ReflectData)ReflectDatumWriter<T>(org.apache.avro.Schema,org.apache.avro.reflect.ReflectData)ReflectDatumWriter<T>(java.lang.Class,org.apache.avro.reflect.ReflectData)ReflectDatumWriter<T>(java.lang.Class)ReflectDatumWriter<T>()DatumWriter<D>DataFileWriter<T>(org.apache.avro.io.DatumWriter)fSyncfSync()flushflush()DataFileStream<D>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/file/DataFileStream.classIterator<D>Iterable<D>appendAllFromappendAllFrom(org.apache.avro.file.DataFileStream,boolean)DataFileStream<T>appendEncodedappendEncoded(java.nio.ByteBuffer)appendappend(java.lang.Object)DataFileWriter<>setMetasetMeta(java.lang.String,long)setMeta(java.lang.String,java.lang.String)isReservedMetaisReservedMeta(java.lang.String)setMeta(java.lang.String,byte[])appendToappendTo(org.apache.avro.file.SeekableInput,java.io.OutputStream)appendTo(java.io.File)isFlushOnEveryBlockisFlushOnEveryBlock()setFlushOnEveryBlocksetFlushOnEveryBlock(boolean)create(org.apache.avro.Schema,java.io.OutputStream,byte[])create(org.apache.avro.Schema,java.io.OutputStream)create(org.apache.avro.Schema,java.io.File)setSyncIntervalsetSyncInterval(int)setCodecsetCodec(org.apache.avro.file.CodecFactory)DataFileWriterDataFileWriter(org.apache.avro.io.DatumWriter)codecFactory"Unable to instantiate avro codec with name ("Unable to instantiate avro codec with name ("). Compression disabled. Exception follows."). Compression disabled. Exception follows."Avro API doesn't support append"Avro API doesn't support appenddestTypeThis is a helper class provided to make it straightforward to serializeFlume {@linkplain Event events} into Avro data.Data type that can be written in the Schema given below.Returns the stream to serialize data into.Returns the parsed Avro schema corresponding to the data being writtenand the parameterized type specified.Simple conversion routine used to convert an Event to a type of yourchoosing. That type must correspond to the Avro schema given by{@link #getSchema()}. write the AVRO container format header impossible to initialize DataFileWriter without writing the schema?/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/AvroEventDeserializer.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/serialization/AvroEventDeserializer.classAvroEventDeserializerAvroEventDeserializer(org.apache.flume.Context,org.apache.flume.serialization.ResettableInputStream)GenericDatumWriter<>DatumWriter<>datumWriterDataFileReader<GenericRecord>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/file/DataFileReader.classDataFileStream<GenericRecord>Iterator<GenericRecord>Iterable<GenericRecord>FileReader<GenericRecord>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/file/FileReader.classfileReaderschemaHashStringschemaHashrisschemaTypeClass<AvroEventDeserializer>"schemaType""flume.avro.schema.hash"flume.avro.schema.hashAvroSchemaTypeAvroSchemaType()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/serialization/AvroEventDeserializer$AvroSchemaType.classEnum<AvroSchemaType>Comparable<AvroSchemaType>compareTo(org.apache.flume.serialization.AvroEventDeserializer.AvroSchemaType)EnumDesc<AvroSchemaType>DynamicConstantDesc<AvroSchemaType>Optional<EnumDesc<AvroSchemaType>>Class<AvroSchemaType>Enum<AvroSchemaType>(java.lang.String,int)" set to " set to ", so storing full Avro " +
          "schema in the header of each event, which may be inefficient. " +
          "Consider using the hash of the schema " +
          "instead of the literal schema.", so storing full Avro schema in the header of each event, which may be inefficient. Consider using the hash of the schema instead of the literal schema.posGenericDatumReader<GenericRecord>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/generic/GenericDatumReader.classDatumReader<GenericRecord>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/io/DatumReader.classDatumReader<D>DataFileReader<GenericRecord>(org.apache.avro.file.SeekableInput,org.apache.avro.io.DatumReader)? super GenericRecordConsumer<? super GenericRecord>Spliterator<GenericRecord>nextRawBlocknextRawBlock(org.apache.avro.file.DataFileStream.DataBlock)hasNextBlockhasNextBlock()blockFinishedblockFinished()getBlockSizegetBlockSize()getBlockCountgetBlockCount()nextBlocknextBlock()next(java.lang.Object)next(org.apache.avro.generic.GenericRecord)getMetaLonggetMetaLong(java.lang.String)getMetaStringgetMetaString(java.lang.String)getMetagetMeta(java.lang.String)getMetaKeysgetMetaKeys()getSchemagetSchema()getHeadergetHeader()resolveCodecresolveCodec()initialize(org.apache.avro.file.DataFileStream.Header)initialize(java.io.InputStream,byte[])validateMagicvalidateMagic(byte[])readMagicreadMagic()DataFileStreamDataFileStream(org.apache.avro.io.DatumReader)DataFileStream<GenericRecord>(org.apache.avro.io.DatumReader)DataFileStream(java.io.InputStream,org.apache.avro.io.DatumReader)DataFileStream<GenericRecord>(java.io.InputStream,org.apache.avro.io.DatumReader)syncBufferblockRemainingblockCountblockBufferdatumInvintelltell()pastSyncpastSync(long)sync(long)previousSyncpreviousSync()seekseek(long)DataFileReaderDataFileReader(org.apache.avro.file.SeekableInput,org.apache.avro.io.DatumReader,org.apache.avro.file.DataFileStream.Header)DataFileReader<GenericRecord>(org.apache.avro.file.SeekableInput,org.apache.avro.io.DatumReader,org.apache.avro.file.DataFileStream.Header)DataFileReader(org.apache.avro.file.SeekableInput,org.apache.avro.io.DatumReader,boolean,byte[])DataFileReader<GenericRecord>(org.apache.avro.file.SeekableInput,org.apache.avro.io.DatumReader,boolean,byte[])DataFileReader(org.apache.avro.file.SeekableInput,org.apache.avro.io.DatumReader)DataFileReader(java.io.File,org.apache.avro.io.DatumReader)DataFileReader<GenericRecord>(java.io.File,org.apache.avro.io.DatumReader)DatumReader<>DataFileReader<>DataFileStream<>FileReader<>DataFileReader<D>FileReader<D>openReaderopenReader(org.apache.avro.file.SeekableInput,org.apache.avro.io.DatumReader,org.apache.avro.file.DataFileStream.Header,boolean)openReader(org.apache.avro.file.SeekableInput,org.apache.avro.io.DatumReader)openReader(java.io.File,org.apache.avro.io.DatumReader)GenericDatumReader<GenericRecord>()read(java.lang.Object,org.apache.avro.io.Decoder)read(org.apache.avro.generic.GenericRecord,org.apache.avro.io.Decoder)skip(org.apache.avro.Schema,org.apache.avro.io.Decoder)createBytescreateBytes(byte[])readIntreadInt(java.lang.Object,org.apache.avro.Schema,org.apache.avro.io.Decoder)readBytesreadBytes(java.lang.Object,org.apache.avro.io.Decoder)readBytes(java.lang.Object,org.apache.avro.Schema,org.apache.avro.io.Decoder)newInstanceFromStringnewInstanceFromString(java.lang.Class,java.lang.String)getReaderCachegetReaderCache()findStringClassfindStringClass(org.apache.avro.Schema)createStringcreateString(java.lang.String)readStringreadString(java.lang.Object,org.apache.avro.io.Decoder)readString(java.lang.Object,org.apache.avro.Schema,org.apache.avro.io.Decoder)newMapnewMap(java.lang.Object,int)newArraynewArray(java.lang.Object,int,org.apache.avro.Schema)newRecordnewRecord(java.lang.Object,org.apache.avro.Schema)createFixedcreateFixed(java.lang.Object,byte[],org.apache.avro.Schema)createFixed(java.lang.Object,org.apache.avro.Schema)readFixedreadFixed(java.lang.Object,org.apache.avro.Schema,org.apache.avro.io.Decoder)addToMapaddToMap(java.lang.Object,java.lang.Object,java.lang.Object)readMapKeyreadMapKey(java.lang.Object,org.apache.avro.Schema,org.apache.avro.io.Decoder)readMapreadMap(java.lang.Object,org.apache.avro.Schema,org.apache.avro.io.ResolvingDecoder)addToArrayaddToArray(java.lang.Object,long,java.lang.Object)peekArraypeekArray(java.lang.Object)readArrayreadArray(java.lang.Object,org.apache.avro.Schema,org.apache.avro.io.ResolvingDecoder)createEnumcreateEnum(java.lang.String,org.apache.avro.Schema)readEnumreadEnum(org.apache.avro.Schema,org.apache.avro.io.Decoder)readFieldreadField(java.lang.Object,org.apache.avro.Schema.Field,java.lang.Object,org.apache.avro.io.ResolvingDecoder,java.lang.Object)readRecordreadRecord(java.lang.Object,org.apache.avro.Schema,org.apache.avro.io.ResolvingDecoder)Conversion<?>convert(java.lang.Object,org.apache.avro.Schema,org.apache.avro.LogicalType,org.apache.avro.Conversion)readWithoutConversionreadWithoutConversion(java.lang.Object,org.apache.avro.Schema,org.apache.avro.io.ResolvingDecoder)readWithConversionreadWithConversion(java.lang.Object,org.apache.avro.Schema,org.apache.avro.LogicalType,org.apache.avro.Conversion,org.apache.avro.io.ResolvingDecoder)read(java.lang.Object,org.apache.avro.Schema,org.apache.avro.io.ResolvingDecoder)getResolvergetResolver(org.apache.avro.Schema,org.apache.avro.Schema)setExpectedsetExpected(org.apache.avro.Schema)getExpectedgetExpected()GenericDatumReaderGenericDatumReader(org.apache.avro.generic.GenericData)GenericDatumReader<GenericRecord>(org.apache.avro.generic.GenericData)GenericDatumReader(org.apache.avro.Schema,org.apache.avro.Schema,org.apache.avro.generic.GenericData)GenericDatumReader<GenericRecord>(org.apache.avro.Schema,org.apache.avro.Schema,org.apache.avro.generic.GenericData)GenericDatumReader(org.apache.avro.Schema,org.apache.avro.Schema)GenericDatumReader<GenericRecord>(org.apache.avro.Schema,org.apache.avro.Schema)GenericDatumReader(org.apache.avro.Schema)GenericDatumReader<GenericRecord>(org.apache.avro.Schema)GenericDatumReader()GenericDatumWriter<>(org.apache.avro.Schema)GenericDatumWriter<>(org.apache.avro.Schema,org.apache.avro.generic.GenericData)GenericDatumWriter<>(org.apache.avro.generic.GenericData)GenericDatumWriter<>()"CRC-64-AVRO"CRC-64-AVRO"Cannot use this deserializer " +
            "without a RemoteMarkable input stream"Cannot use this deserializer without a RemoteMarkable input stream"Cannot instantiate deserializer"Cannot instantiate deserializer/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/serialization/AvroEventDeserializer$SeekableResettableInputBridge.classoffA deserializer that parses Avro container files, generating one Flume eventper record in the Avro file, and storing binary avro-encoded records inthe Flume event body. annotate header with 64-bit schema CRC hash in hex FIXME: Avro doesn't seem to complain about this, but probably not a great idea.../Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/AvroEventSerializerConfigurationConstants.java"syncIntervalBytes"2048000"compressionCodec""schemaURL"schemaURLAvro sync interval, in approximate bytesAvro compression codec. For supported codecs, see Avro's{@link CodecFactory} class. no codecAvro static Schema URL/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/BodyTextEventSerializer.javaBodyTextEventSerializerBodyTextEventSerializer(java.io.OutputStream,org.apache.flume.Context)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/serialization/BodyTextEventSerializer.classappendNewlineAPPEND_NEWLINE_DFLTAPPEND_NEWLINE"appendNewline"Class<BodyTextEventSerializer>ctx'\n'This class simply writes the body of the event to the output streamand appends a newline after each event. for legacy reasons, by default, append a newline to each event written out noop/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/DecodeErrorPolicy.javaDecodeErrorPolicyDecodeErrorPolicy()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/serialization/DecodeErrorPolicy.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/DurablePositionTracker.javainitReaderinitReader()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/serialization/DurablePositionTracker.classDurablePositionTrackerDurablePositionTracker(java.io.File,java.lang.String)isOpenmetaCacheDataFileReader<TransferStateFileMeta>DataFileStream<TransferStateFileMeta>Iterator<TransferStateFileMeta>Iterable<TransferStateFileMeta>FileReader<TransferStateFileMeta>DataFileWriter<TransferStateFileMeta>trackerFileoldTrackerexistingTargettargetPositiontmpMetatmpTrackernewTracker"Unable to delete existing meta file "Unable to delete existing meta file DatumWriter<TransferStateFileMeta>doutSpecificDatumWriter<TransferStateFileMeta>GenericDatumWriter<TransferStateFileMeta>SpecificDatumWriter<TransferStateFileMeta>(org.apache.avro.Schema)write(org.apache.flume.serialization.TransferStateFileMeta,org.apache.avro.io.Encoder)GenericDatumWriter<TransferStateFileMeta>(org.apache.avro.Schema,org.apache.avro.generic.GenericData)GenericDatumWriter<TransferStateFileMeta>(org.apache.avro.Schema)GenericDatumWriter<TransferStateFileMeta>(org.apache.avro.generic.GenericData)GenericDatumWriter<TransferStateFileMeta>()SpecificDatumWriter<TransferStateFileMeta>(org.apache.avro.specific.SpecificData)SpecificDatumWriter<TransferStateFileMeta>(org.apache.avro.Schema,org.apache.avro.specific.SpecificData)Class<TransferStateFileMeta>SpecificDatumWriter<TransferStateFileMeta>(java.lang.Class)SpecificDatumWriter<TransferStateFileMeta>()DatumReader<TransferStateFileMeta>dinSpecificDatumReader<TransferStateFileMeta>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/specific/SpecificDatumReader.classGenericDatumReader<TransferStateFileMeta>SpecificDatumReader<TransferStateFileMeta>(org.apache.avro.Schema)read(org.apache.flume.serialization.TransferStateFileMeta,org.apache.avro.io.Decoder)GenericDatumReader<TransferStateFileMeta>(org.apache.avro.generic.GenericData)GenericDatumReader<TransferStateFileMeta>(org.apache.avro.Schema,org.apache.avro.Schema,org.apache.avro.generic.GenericData)GenericDatumReader<TransferStateFileMeta>(org.apache.avro.Schema,org.apache.avro.Schema)GenericDatumReader<TransferStateFileMeta>(org.apache.avro.Schema)GenericDatumReader<TransferStateFileMeta>()SpecificDatumReaderSpecificDatumReader(org.apache.avro.specific.SpecificData)SpecificDatumReader<TransferStateFileMeta>(org.apache.avro.specific.SpecificData)SpecificDatumReader(org.apache.avro.Schema,org.apache.avro.Schema,org.apache.avro.specific.SpecificData)SpecificDatumReader<TransferStateFileMeta>(org.apache.avro.Schema,org.apache.avro.Schema,org.apache.avro.specific.SpecificData)SpecificDatumReader(org.apache.avro.Schema,org.apache.avro.Schema)SpecificDatumReader<TransferStateFileMeta>(org.apache.avro.Schema,org.apache.avro.Schema)SpecificDatumReader(org.apache.avro.Schema)SpecificDatumReader(java.lang.Class)SpecificDatumReader<TransferStateFileMeta>(java.lang.Class)SpecificDatumReader()SpecificDatumReader<TransferStateFileMeta>()"trackerFile must not be null"trackerFile must not be null"target must not be null"target must not be nullDataFileWriter<TransferStateFileMeta>(org.apache.avro.io.DatumWriter)append(org.apache.flume.serialization.TransferStateFileMeta)DataFileReader<TransferStateFileMeta>(java.io.File,org.apache.avro.io.DatumReader)? super TransferStateFileMetaConsumer<? super TransferStateFileMeta>Spliterator<TransferStateFileMeta>next(org.apache.flume.serialization.TransferStateFileMeta)DataFileStream<TransferStateFileMeta>(org.apache.avro.io.DatumReader)DataFileStream<TransferStateFileMeta>(java.io.InputStream,org.apache.avro.io.DatumReader)DataFileReader<TransferStateFileMeta>(org.apache.avro.file.SeekableInput,org.apache.avro.io.DatumReader,org.apache.avro.file.DataFileStream.Header)DataFileReader<TransferStateFileMeta>(org.apache.avro.file.SeekableInput,org.apache.avro.io.DatumReader,boolean,byte[])DataFileReader<TransferStateFileMeta>(org.apache.avro.file.SeekableInput,org.apache.avro.io.DatumReader)"file"syncPos256L<p/>Class that stores object state in an avro container file.The file is only ever appended to.At construction time, the object reads data from the end of the file andcaches that data for use by a client application. After construction, readsnever go to disk.Writes always flush to disk.<p/>Note: This class is not thread-safe.If the file exists at startup, then read it, roll it, and open a new one.We go through this to avoid issues with partial reads at the end of thefile from a previous crash. If we append to a bad record,our writes may never be visible. exists On windows, things get messy with renames... FIXME: This is not atomic. Consider implementing a recovery procedure so that if it does not exist at startup, check for a rolled version before creating a new file from scratch. rename tmp file to meta return a new known-good version that is open for appendIf the file exists, read it and open it for append. open it for append create the file initialize @ line = 0;Read the last record in the file./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/EventDeserializer.javaEstablishes a contract for reading events stored in arbitrary formats fromreliable, resettable streams.Read a single event from the underlying stream.Deserialized event or {@code null} if no events could be read.#mark()#reset()Read a batch of events from the underlying stream.Maximum number of events to return.List of read events, or empty list if no events could be read.Marks the underlying input stream, indicating that the events previouslyreturned by this EventDeserializer have been successfully committed.Resets the underlying input stream to the last known mark (or beginningof the stream if {@link #mark()} was never previously called. This shouldbe done in the case of inability to commit previously-deserialized events.Calls {@link #reset()} on the stream and then closes it.In the case of successful completion of event consumption,{@link #mark()} MUST be called before {@code close()}.Knows how to construct this deserializer.<br/>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/EventDeserializerFactory.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/serialization/EventDeserializerFactory.classClass<EventDeserializerFactory>"serializer type must not be null"serializer type must not be null/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/EventDeserializerType.javaEventDeserializerTypeEventDeserializerType(java.lang.Class)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/serialization/EventDeserializerType.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/EventSerDe.java"future public api"future public api<p/>A class that is able to both serialize and deserialize events.<p/>Implementing this interface does not simply mean that a class has bothserialization and deserialization capability. By implementing thisinterface, implementations guarantee that they can convert a serializedevent back to a deserialized event, and back to a serialized event againwithout any data loss.<p/>That guarantee allows Flume to "replay" partial file writes and restoreoutput files that may have been damaged in a system crash. At the time ofthis writing, support for this functionality is still lacking.Knows how to construct this serde.<br/>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/EventSerializer.java"serializer."serializer.This interface provides callbacks for important serialization-related events.This allows generic implementations of serializers to be plugged in, allowingimplementations of this interface to do arbitrary header and messageformatting, as well as file and message framing.The following general semantics should be used by drivers that call thisinterface:// open file (for example... or otherwise create some new stream)OutputStream out = new FileOutputStream(file); // open for create// build serializer using builder interfaceEventSerializer serializer = builder.build(ctx, out);// hook to write header (since in this case we opened the file for create)serializer.afterCreate();// write one or more eventsserializer.write(event1);serializer.write(event2);serializer.write(event3);// periodically flush any internal buffers from EventSerializer.write()serializer.flush();// The driver responsible for specifying and implementing its durability// semantics (if any) for flushing or syncing the underlying stream.out.flush();// when closing the file...// make sure we got all buffered events flushed from the serializer// write trailer before closing fileserializer.beforeClose();// Driver is responsible for flushing the underlying stream, if needed,// before closing it.out.close();Hook to write a header after file is opened for the first time.Hook to handle any framing needed when file is re-opened (for write).<br/>Could have been named {@code afterOpenForAppend()}.Serialize and write the given event.Event to write to the underlying stream.Hook to flush any internal write buffers to the underlying stream.It is NOT necessary for an implementation to then call flush() / sync()on the underlying stream itself, since those semantics would be providedby the driver that calls this API.Hook to write a trailer before the stream is closed.Implementations must not buffer data in this call sinceEventSerializer.flush() is not guaranteed to be called after beforeClose().Specify whether this output format supports reopening files for append.For example, this method should return {@code false} if{@link #beforeClose()} writes a trailer that "finalizes" the file(this type of behavior is file format-specific).<br/>Could have been named {@code supportsAppend()}.Knows how to construct this event serializer.<br/>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/EventSerializerFactory.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/serialization/EventSerializerFactory.classClass<EventSerializerFactory>serializerType/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/EventSerializerType.javaEventSerializerTypeEventSerializerType(java.lang.Class)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/serialization/EventSerializerType.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/FlumeEventAvroEventSerializer.javaFlumeEventAvroEventSerializerFlumeEventAvroEventSerializer(java.io.OutputStream)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/serialization/FlumeEventAvroEventSerializer.classSCHEMAAbstractAvroEventSerializer<Event>"{ \"type\":\"record\", \"name\": \"Event\", \"fields\": [" +
      " {\"name\": \"headers\", \"type\": { \"type\": \"map\", \"values\": \"string\" } }, " +
      " {\"name\": \"body\", \"type\": \"bytes\" } ] }"{ "type":"record", "name": "Event", "fields": [ {"name": "headers", "type": { "type": "map", "values": "string" } },  {"name": "body", "type": "bytes" } ] }configureconfigure(org.apache.flume.Context)supportsReopensupportsReopen()beforeClosebeforeClose()write(org.apache.flume.Event)afterReopenafterReopen()afterCreateafterCreate()convert(org.apache.flume.Event)getOutputStreamgetOutputStream()AbstractAvroEventSerializerAbstractAvroEventSerializer()AbstractAvroEventSerializer<Event>()A no-op for this simple, special-case implementation/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/HeaderAndBodyTextEventSerializer.javaHeaderAndBodyTextEventSerializerHeaderAndBodyTextEventSerializer(java.io.OutputStream,org.apache.flume.Context)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/serialization/HeaderAndBodyTextEventSerializer.classClass<HeaderAndBodyTextEventSerializer>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/LengthMeasurable.javareturns the total length of the stream or file/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/LineDeserializer.javareadLinereadLine()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/serialization/LineDeserializer.classensureOpenensureOpen()LineDeserializerLineDeserializer(org.apache.flume.Context,org.apache.flume.serialization.ResettableInputStream)maxLineLengthoutputCharsetClass<LineDeserializer>"outputCharset""maxLineLength"2048"Serializer has been closed"Serializer has been closedreadChars"Line length exceeds max ({}), truncating line!"Line length exceeds max ({}), truncating line!A deserializer that parses text lines from a file.Reads a line from a file and returns an eventEvent containing parsed lineBatch line readList of events containing read lines TODO: consider not returning a final character that is a high surrogate when truncating FIXME: support \r\n/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/PositionTracker.javaDefines an interface for tracking the offset position in a target file./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/RemoteMarkable.javaAllows for calling mark() without a seek()Indicate that the specified position should be returned to in the case of{@link Resettable#reset()} being called.Return the saved mark position without moving the mark pointer./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/Resettable.javaDefines an API for objects that can be mark()ed and reset() on arbitraryboundaries. Any implementation that has a limited buffer for the mark(),like {@link java.io.InputStream}, must not implement Resettable.Indicate that the current position should be returned to in the case of{@link #reset()} being called.Return to the last marked position, or the beginning of the stream if{@link #mark()} has never been called./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/ResettableFileInputStream.javaincrPositionincrPosition(int,boolean)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/serialization/ResettableFileInputStream.classrefillBufrefillBuf()lowSurrogatehasLowSurrogatemaxCharWidthsyncPositionfileSizebyteBufcharBufchanClass<ResettableFileInputStream>16384bufSizeerrorAction"UTF-16"UTF-16"UTF-32"UTF-32"Unexpected value for decode error policy: "Unexpected value for decode error policy: rem"read(buf, {}, {})"read(buf, {}, {})isEndOfInputhighSurrogate"Decoded a pair of chars, but it does not seem to be a surrogate pair: {} {}"Decoded a pair of chars, but it does not seem to be a surrogate pair: {} {}"Tell position: {}"Tell position: {}newPosrelativeChangenewBufPos"Seek to position: {}"Seek to position: {}incrupdateSyncPosition<p>This class makes the following assumptions:</p><ol><li>The underlying file is not changing while it is being read</li></ol><p>The ability to {@link #reset()} is dependent on the underlying {@linkPositionTracker} instance's durability semantics.</p><p><strong>A note on surrogate pairs:</strong></p><p>The logic for decoding surrogate pairs is as follows:If no character has been decoded by a "normal" pass, and the buffer still has remaining bytes,then an attempt is made to read 2 characters in one pass.If it succeeds, then the first char (high surrogate) is returned;the second char (low surrogate) is recorded internally,and is returned at the next call to {@link #readChar()}.If it fails, then it is assumed that EOF has been reached.</p><p>Impacts on position, mark and reset: when a surrogate pair is decoded, the positionis incremented by the amount of bytes taken to decode the <em>entire</em> pair (usually, 4).This is the most reasonable choice since it would not be advisableto reset a stream to a position pointing to the second char in a pair of surrogates:such a dangling surrogate would not be properly decoded without its counterpart.</p><p>Thus the behaviour of mark and reset is as follows:</p><li>If {@link #mark()} is called after a high surrogate pair has been returned by{@link #readChar()}, the marked position will be that of the character <em>following</em>the low surrogate, <em>not</em> that of the low surrogate itself.</li><li>If {@link #reset()} is called after a high surrogate pair has been returned by{@link #readChar()}, the low surrogate is always returned by the next call to{@link #readChar()}, <em>before</em> the stream is actually reset to the last markedposition.</li><p>This ensures that no dangling high surrogate could ever be read as long asthe same instance is used to read the whole pair. <strong>However, if {@link #reset()}is called after a high surrogate pair has been returned by {@link #readChar()},and a new instance of ResettableFileInputStream is used to resume reading,then the low surrogate char will be lost,resulting in a corrupted sequence of characters (dangling high surrogate).</strong>This situation is hopefully extremely unlikely to happen in real life.The minimum acceptable buffer size to store bytes readfrom the underlying file. A minimum size of 8 ensures that thebuffer always has enough space to contain multi-byte characters,including special sequences such as surrogate pairs, Byte Order Marks, etc.Whether this instance holds a low surrogate character.A low surrogate character read from a surrogate pair.When a surrogate pair is found, the high (first) surrogate pairis returned upon a call to {@link #read()},while the low (second) surrogate remains stored in memory,to be returned at the next call to {@link #read()}.File to readPositionTracker implementation to make offset position durableFileNotFoundExceptionIf the file to read does not existIf the position reported by the tracker cannot be soughtSize of the underlying buffer used for input. If lesser than {@link #MIN_BUF_SIZE},a buffer of length {@link #MIN_BUF_SIZE} will be created instead.Character set used for decoding text, as necessaryA {@link DecodeErrorPolicy} instance to determine howthe decoder should behave in case of malformed input and/orunmappable character. single byte two chars for surrogate pairs some JDKs wrongly report 3 bytes max UTF_16BE and UTF_16LE wrongly report 2 bytes max UTF_32BE and UTF_32LE wrongly report 4 bytes max len == 0 should never happen Check whether we are in the middle of a surrogate pair, in which case, return the last (low surrogate) char of the pair. The decoder can have issues with multi-byte characters. This check ensures that there are at least maxCharWidth bytes in the buffer before reaching EOF. Found a single char Found nothing, but the byte buffer has not been entirely consumed. This situation denotes the presence of a surrogate pair that can only be decoded if we have a 2-char buffer. increase the limit to 2 decode 2 chars in one pass Check if we successfully decoded 2 chars save second (low surrogate) char for later consumption Check if we really have a surrogate pair This should only happen in case of bad sequences (dangling surrogate, etc.) consider the pair as a single unit and increment position normally return the first (high surrogate) char of the pair end of file ensure we read from the proper offset check to see if we can seek within our existing buffer seek to current pos => no-op we can reuse the read buffer otherwise, we have to invalidate the read buffer clear decoder state perform underlying file seek reset position pointers/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/ResettableInputStream.java<p> This abstract class defines an interface for whichthe stream may be <code>mark</code>ed and <code>reset</code> with no limit tothe number of bytes which may have been read between the calls.<p> Any implementation of this interface guarantees that the mark positionwill not be invalidated by reading any number of bytes.<p> Warning: We reserve the right to add public methods to this class inthe future. Third-party subclasses beware.Read a single byte of data from the stream.the next byte of data, or {@code -1} if the end of the stream hasbeen reached.Read multiple bytes of data from the stream.the buffer into which the data is read.Offset into the array {@code b} at which the data is written.the maximum number of bytes to read.the total number of bytes read into the buffer, or {@code -1} ifthe end of the stream has been reached.<p>Read a single character.<p>Note that this may lead to returning only one character in a 2-charsurrogate pair sequence. When this happens, the underlying implementationshould never persist a mark between two chars of a two-char surrogatepair sequence.The character read, as an integer in the range 0 to 65535(0x00-0xffff), or -1 if the end of the stream has been reachedMarks the current position in this input stream. A subsequent call to the<code>reset</code> method repositions this stream at the last markedposition so that subsequent reads re-read the same bytes.<p> Marking a closed stream should not have any effect on the stream.If there is an error while setting the mark position.java.io.InputStream#mark(int)java.io.InputStream#reset()Reset stream position to that set by {@link #mark()}Seek to the specified byte position in the stream.Absolute byte offset to seek toTell the current byte position.the current absolute byte position in the stream/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/serialization/Seekable.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/sink/AbstractRpcSink.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/sinkorg.apache.flume.sinkgetUnderlyingClientgetUnderlyingClient()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sink/AbstractRpcSink.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sinkverifyConnectionverifyConnection()destroyConnectiondestroyConnection()resetConnectionresetConnection()createConnectioncreateConnection()cxnResetExecutorDEFAULT_CXN_RESET_INTERVALresetConnectionFlagcxnResetIntervalsinkCounterclientProps"Rpc Sink Reset Thread"Rpc Sink Reset ThreadClass<AbstractRpcSink>"hostname""No hostname specified"No hostname specified"No port specified"No port specified"reset-connection-interval"reset-connection-interval"Connection reset is set to "Connection reset is set to ". Will not reset connection to next hop". Will not reset connection to next hop"Rpc sink {}: Building RpcClient with hostname: {}, " +
          "port: {}"Rpc sink {}: Building RpcClient with hostname: {}, port: {}"Rpc Client could not be " +
            "initialized. "Rpc Client could not be initialized. " could not be started" could not be started/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sink/AbstractRpcSink$1.class"Rpc sink {}: Created RpcClient: {}"Rpc sink {}: Created RpcClient: {}"Error while trying to expire connection"Error while trying to expire connection"Rpc sink {} closing Rpc client: {}"Rpc sink {} closing Rpc client: {}"Rpc sink "Rpc sink ": Attempt to close Rpc " +
            "client failed. Exception follows.": Attempt to close Rpc client failed. Exception follows."Unable to create Rpc client using hostname: "Unable to create Rpc client using hostname: ", port: ", port: "Rpc sink {} started."Rpc sink {} started."Rpc sink {} stopping..."Rpc sink {} stopping..."Interrupted while waiting for connection reset executor to shut down"Interrupted while waiting for connection reset executor to shut down"Rpc sink {} stopped. Metrics: {}"Rpc sink {} stopped. Metrics: {}"RpcSink "RpcSink " { host: " { host: "Rpc Sink "Rpc Sink ": Unable to get event from" +
            " channel ": Unable to get event from channel ". Exception follows.". Exception follows."Failed to send events"Failed to send eventsThis sink provides the basic RPC functionality for Flume. This sink takesseveral arguments which are used in RPC.This sink forms one half of Flume's tiered collection support. Events sent tothis sink are transported over the network to the hostname / port pair usingthe RPC implementation encapsulated in {@link RpcClient}.The destination is an instance of Flume's {@link org.apache.flume.source.AvroSource} or {@link org.apache.flume.source.ThriftSource} (based onwhich implementation of this class is used), whichallows Flume agents to forward to other Flume agents, forming a tieredcollection infrastructure. Of course, nothing prevents one from using thissink to speak to other custom built infrastructure that implements the sameRPC protocol.Events are taken from the configured {@link Channel} in batches of theconfigured <tt>batch-size</tt>. The batch size has no theoretical limitsalthough all events in the batch <b>must</b> fit in memory. Generally, largerbatches are far more efficient, but introduce a slight delay (measured inmillis) in delivery. The batch behavior is such that underruns (i.e. batchessmaller than the configured batch size) are possible. This is a compromisemade to maintain low latency of event delivery. If the channel returns a nullevent, meaning it is empty, the batch is immediately sent, regardless ofsize. Batch underruns are tracked in the metrics. Empty batches do not incuran RPC roundtrip.<th>Unit (data type)</th><td><tt>hostname</tt></td><td>The hostname to which events should be sent.</td><td>Hostname or IP (String)</td><td>none (required)</td><td><tt>port</tt></td><td>The port to which events should be sent on <tt>hostname</tt>.</td><td>TCP port (int)</td><td><tt>batch-size</tt></td><td>The maximum number of events to send per RPC.</td><td>events (int)</td><td>100</td><td><tt>connect-timeout</tt></td><td>Maximum time to wait for the first Avro handshake and RPC request</td><td>milliseconds (long)</td><td>20000</td><td><tt>request-timeout</tt></td><td>Maximum time to wait RPC requests after the first</td><td><tt>compression-type</tt></td><td>Select compression type. Default is "none" and the only compression type availableis "deflate"</td><td>compression type</td><td>none</td><td><tt>compression-level</tt></td><td>In the case compression type is "deflate" this value can be between 0-9.0 being no compression and 1-9 is compression.The higher the number the better the compression. 6 is the default.</td><td>compression level</td><td>6</td><strong>Implementation Notes:</strong> Any implementation of this classmust override the {@linkplain #initializeRpcClient(Properties)} method.This method will be called whenever this sink needs to create a newconnection to the source. batchSize is used in the clients, here it is only used for config validation before the client is configuredReturns a new {@linkplain RpcClient} instance configured using the given{@linkplain Properties} object. This method is called whenever a newconnection needs to be created to the next hop.If this function is called successively without calling{@see #destroyConnection()}, only the first call has any effect.org.apache.flume.FlumeExceptionif an RPC client connection could not be opened Don't rethrow, else this runnable won't get scheduled again.Ensure the connection exists and is active.If the connection is not active, destroy it and recreate it.If there are errors closing or opening the RPCconnection.The start() of RpcSink is more of an optimization that allows connectionto be created before the process() loop is started. In case it so happensthat the start failed, the process() loop will itself attempt to reconnectas necessary. This is the expected behavior since it is possible that thedownstream source becomes unavailable in the middle of the process loopand the sink will have to retry the connection again.Try to prevent leaking resources. if the time to reset is long and the timeout is short this may cancel the next reset request this should however not be an issue/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/sink/AbstractSingleSinkProcessor.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sink/AbstractSingleSinkProcessor.class"DefaultSinkProcessor sink not set"DefaultSinkProcessor sink not setSpliterator<Sink>? super SinkConsumer<? super Sink>Iterator<Sink>Stream<Sink>BaseStream<Sink,Stream<Sink>>Predicate<? super Sink>Collection<? extends Sink>Iterable<? extends Sink>add(org.apache.flume.Sink)addLast(org.apache.flume.Sink)addFirst(org.apache.flume.Sink)ListIterator<Sink>add(int,org.apache.flume.Sink)set(int,org.apache.flume.Sink)Comparator<? super Sink>UnaryOperator<Sink>Function<Sink,Sink>"DefaultSinkPolicy can "
        + "only handle one sink, "
        + "try using a policy that supports multiple sinks"DefaultSinkPolicy can only handle one sink, try using a policy that supports multiple sinksA Sink Processor that only accesses a single Sink./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/sink/AbstractSink.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sink/AbstractSink.class"No channel configured"No channel configured? extends AbstractSinkClass<? extends AbstractSink>Map<String,? extends AbstractSink>AbstractSink[]Constructor<? extends AbstractSink>TypeVariable<Class<? extends AbstractSink>>TypeVariable<Class<? extends AbstractSink>>[]"{name:"{name:", channel:", channel:/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/sink/AbstractSinkProcessor.javasinkList/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sink/AbstractSinkProcessor.classlistArrayList<Sink>AbstractList<Sink>AbstractCollection<Sink>ArrayList<Sink>()AbstractCollection<Sink>()AbstractList<Sink>()ArrayList<Sink>(java.util.Collection)ArrayList<Sink>(int)A convenience base class for sink processors. List of sinks as specified/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/sink/AbstractSinkSelector.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sink/AbstractSinkSelector.classtimeOut"maxTimeOut"maxTimeOutfailedSink/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/sink/AvroSink.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sink/AvroSink.classClass<AvroSink>"Attempting to create Avro Rpc client."Attempting to create Avro Rpc client.A {@link Sink} implementation that can send events to an RPC server (such asFlume's {@link AvroSource}).The destination is an instance of Flume's {@link AvroSource}, which/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/sink/DefaultSinkFactory.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sink/DefaultSinkFactory.classClass<DefaultSinkFactory>sinkClass"Creating instance of sink: {}, type: {}"Creating instance of sink: {}, type: {}sinkMap<String,? extends Sink>Sink[]Constructor<? extends Sink>TypeVariable<Class<? extends Sink>>TypeVariable<Class<? extends Sink>>[]"Unable to create sink: "Unable to create sink: "Sink type {} is a custom type"Sink type {} is a custom type"Unable to load sink type: "Unable to load sink type: /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/sink/DefaultSinkProcessor.javaDefault sink processor that only accepts a single sink, passing on processresults without any additional handling. Suitable for all sinks that aren'tassigned to a group./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/sink/FailoverSinkProcessor.javamoveActiveToDeadAndGetNextmoveActiveToDeadAndGetNext()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sink/FailoverSinkProcessor.classmaxPenaltyQueue<FailedSink>Collection<FailedSink>Iterable<FailedSink>failedSinksSortedMap<Integer,Sink>SequencedMap<Integer,Sink>Map<Integer,Sink>liveSinksactiveSinkMap<String,Sink>MAX_PENALTY_PREFIXPRIORITY_PREFIXDEFAULT_MAX_PENALTYFAILURE_PENALTYClass<FailoverSinkProcessor>"priority."priority."maxpenalty"maxpenaltyadjustRefreshadjustRefresh()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sink/FailoverSinkProcessor$FailedSink.classsequentialFailurespriorityrefreshComparable<FailedSink>seqFailures"Sink {} failed again, new refresh is at {}, current time {}"Sink {} failed again, new refresh is at {}, current time {}nextPriomaxPenaltyStrTreeMap<Integer,Sink>AbstractMap<Integer,Sink>NavigableMap<Integer,Sink>TreeMap<Integer,Sink>()BiFunction<? super Sink,? super Sink,? extends Sink>merge(java.lang.Integer,org.apache.flume.Sink,java.util.function.BiFunction)BiFunction<? super Integer,? super Sink,? extends Sink>Function<? super Integer,? extends Sink>replace(java.lang.Integer,org.apache.flume.Sink)replace(java.lang.Integer,org.apache.flume.Sink,org.apache.flume.Sink)putIfAbsent(java.lang.Integer,org.apache.flume.Sink)BiConsumer<? super Integer,? super Sink>getOrDefault(java.lang.Object,org.apache.flume.Sink)Entry<Integer,Sink>Set<Entry<Integer,Sink>>Collection<Entry<Integer,Sink>>Iterable<Entry<Integer,Sink>>Map<? extends Integer,? extends Sink>put(java.lang.Integer,org.apache.flume.Sink)AbstractMap<Integer,Sink>()SequencedSet<Entry<Integer,Sink>>SequencedCollection<Entry<Integer,Sink>>putLast(java.lang.Integer,org.apache.flume.Sink)putFirst(java.lang.Integer,org.apache.flume.Sink)tailMap(java.lang.Integer)headMap(java.lang.Integer)subMap(java.lang.Integer,java.lang.Integer)tailMap(java.lang.Integer,boolean)headMap(java.lang.Integer,boolean)subMap(java.lang.Integer,boolean,java.lang.Integer,boolean)higherKey(java.lang.Integer)higherEntry(java.lang.Integer)ceilingKey(java.lang.Integer)ceilingEntry(java.lang.Integer)floorKey(java.lang.Integer)floorEntry(java.lang.Integer)lowerKey(java.lang.Integer)lowerEntry(java.lang.Integer)SortedSet<? extends Integer>Set<? extends Integer>SequencedSet<? extends Integer>SequencedCollection<? extends Integer>addAllForTreeSet(java.util.SortedSet,org.apache.flume.Sink)readTreeSet(int,java.io.ObjectInputStream,org.apache.flume.Sink)getLowerEntry(java.lang.Integer)getHigherEntry(java.lang.Integer)getFloorEntry(java.lang.Integer)getCeilingEntry(java.lang.Integer)SortedMap<Integer,? extends Sink>SequencedMap<Integer,? extends Sink>Map<Integer,? extends Sink>TreeMap<Integer,Sink>(java.util.SortedMap)TreeMap<Integer,Sink>(java.util.Map)TreeMap<Integer,Sink>(java.util.Comparator)PriorityQueue<FailedSink>AbstractQueue<FailedSink>AbstractCollection<FailedSink>PriorityQueue<FailedSink>()Spliterator<FailedSink>? super FailedSinkConsumer<? super FailedSink>Iterator<FailedSink>Stream<FailedSink>BaseStream<FailedSink,Stream<FailedSink>>Predicate<? super FailedSink>? extends FailedSinkCollection<? extends FailedSink>Iterable<? extends FailedSink>add(org.apache.flume.sink.FailoverSinkProcessor.FailedSink)AbstractCollection<FailedSink>()offer(org.apache.flume.sink.FailoverSinkProcessor.FailedSink)AbstractQueue<FailedSink>()Comparator<? super FailedSink>SortedSet<? extends FailedSink>Set<? extends FailedSink>SequencedSet<? extends FailedSink>SequencedCollection<? extends FailedSink>PriorityQueue<FailedSink>(java.util.SortedSet)PriorityQueue<? extends FailedSink>AbstractQueue<? extends FailedSink>AbstractCollection<? extends FailedSink>Queue<? extends FailedSink>PriorityQueue<FailedSink>(java.util.PriorityQueue)PriorityQueue<FailedSink>(java.util.Collection)PriorityQueue<FailedSink>(int,java.util.Comparator)PriorityQueue<FailedSink>(java.util.Comparator)PriorityQueue<FailedSink>(int)"{} is not a valid value for {}"{} is not a valid value for {}Entry<String,Sink>Set<Entry<String,Sink>>Collection<Entry<String,Sink>>Iterable<Entry<String,Sink>>merge(java.lang.String,org.apache.flume.Sink,java.util.function.BiFunction)BiFunction<? super String,? super Sink,? extends Sink>Function<? super String,? extends Sink>replace(java.lang.String,org.apache.flume.Sink)replace(java.lang.String,org.apache.flume.Sink,org.apache.flume.Sink)putIfAbsent(java.lang.String,org.apache.flume.Sink)BiConsumer<? super String,? super Sink>Map<? extends String,? extends Sink>put(java.lang.String,org.apache.flume.Sink)priStrsetValue(org.apache.flume.Sink)"Sink {} not added to FailverSinkProcessor as priority" +
            "duplicates that of sink {}"Sink {} not added to FailverSinkProcessor as priorityduplicates that of sink {}retcur"Sink {} was recovered from the fail list"Sink {} was recovered from the fail list"Sink {} failed and has been sent to failover list"Sink {} failed and has been sent to failover list"All sinks failed to process, " +
        "nothing left to failover to"All sinks failed to process, nothing left to failover toHashMap<String,Sink>AbstractMap<String,Sink>HashMap<String,Sink>()AbstractMap<String,Sink>()Node<String,Sink>TreeNode<String,Sink>newTreeNode(int,java.lang.String,org.apache.flume.Sink,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.flume.Sink,java.util.HashMap.Node)Node<String,Sink>[]putVal(int,java.lang.String,org.apache.flume.Sink,boolean,boolean)HashMap<String,Sink>(java.util.Map)HashMap<String,Sink>(int)HashMap<String,Sink>(int,float)FailoverSinkProcessor maintains a prioritized list of sinks,guarranteeing that so long as one is available events will be processed.The failover mechanism works by relegating failed sinks to a poolwhere they are assigned a cooldown period, increasing with sequentialfailures before they are retried. Once a sink successfully sends anevent it is restored to the live pool.FailoverSinkProcessor is in no way thread safe and expects to be run viaSinkRunner Additionally, setSinks must be called before configure, andadditional sinks cannot be added while runningTo configure, set a sink groups processor to "failover" and set prioritiesfor individual sinks, all priorities must be unique. Furthermore, anupper limit to failover time can be set(in miliseconds) using maxpenaltyEx)host1.sinkgroups = group1host1.sinkgroups.group1.sinks = sink1 sink2host1.sinkgroups.group1.processor.type = failoverhost1.sinkgroups.group1.processor.priority.sink1 = 5host1.sinkgroups.group1.processor.priority.sink2 = 10host1.sinkgroups.group1.processor.maxpenalty = 10000 Retry any failed sinks that have gone through their "cooldown" period if it's a backoff it needn't be penalized. needed to implement the start/stop functionality/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/sink/LoadBalancingSinkProcessor.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sink/LoadBalancingSinkProcessor.class"selector""backoff"backoff"ROUND_ROBIN""RANDOM""ROUND_ROBIN_BACKOFF"ROUND_ROBIN_BACKOFF"RANDOM_BACKOFF"RANDOM_BACKOFFClass<LoadBalancingSinkProcessor>selectorTypeNameshouldBackOff"The LoadBalancingSinkProcessor cannot be used for a single sink. "
        + "Please configure more than one sinks and try again."The LoadBalancingSinkProcessor cannot be used for a single sink. Please configure more than one sinks and try again.? extends SinkSelectorClass<? extends SinkSelector>Map<String,? extends SinkSelector>SinkSelector[]Constructor<? extends SinkSelector>TypeVariable<Class<? extends SinkSelector>>TypeVariable<Class<? extends SinkSelector>>[]"Unable to instantiate sink selector: "Unable to instantiate sink selector: "Sink selector: "Sink selector: " initialized" initializedsinkIterator"Sink failed to consume event. "
            + "Attempting next sink if available."Sink failed to consume event. Attempting next sink if available."All configured sinks have failed"All configured sinks have failedRoundRobinSinkSelectorRoundRobinSinkSelector(boolean)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sink/LoadBalancingSinkProcessor$RoundRobinSinkSelector.classorg.apache.flume.utilOrderSelector<Sink>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/util/OrderSelector.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/util/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdkRoundRobinOrderSelector<Sink>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/util/RoundRobinOrderSelector.classRoundRobinOrderSelector<Sink>(boolean)getMaxTimeOutgetMaxTimeOut()setMaxTimeOutsetMaxTimeOut(long)isShouldBackOffisShouldBackOff()List<Integer>getIndexListgetIndexList()informFailureinformFailure(java.lang.Object)informFailure(org.apache.flume.Sink)createIteratorcreateIterator()List<T>SequencedCollection<T>getObjectsgetObjects()setObjectssetObjects(java.util.List)OrderSelectorOrderSelector(boolean)OrderSelector<Sink>(boolean)RoundRobinOrderSelectorRoundRobinOrderSelector(boolean)RandomOrderSinkSelectorRandomOrderSinkSelector(boolean)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sink/LoadBalancingSinkProcessor$RandomOrderSinkSelector.classRandomOrderSelector<Sink>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/util/RandomOrderSelector.classRandomOrderSelector<Sink>(boolean)RandomOrderSelectorRandomOrderSelector(boolean)<p>Provides the ability to load-balance flow over multiple sinks.</p><p>The <tt>LoadBalancingSinkProcessor</tt> maintains an indexed list ofactive sinks on which the load must be distributed. This implementation<p>When invoked, this selector picks the next sink using its configuredselection mechanism and invokes it. In case the selected sink fails withan exception, the processor picks the next available sink via its configuredselection mechanism. This implementation does not blacklist the failingsink and instead continues to optimistically attempt every available sink.If all sinks invocations result in failure, the selector propagates thefailure to the sink runner.</p>Sample configuration:host1.sinkgroups.group1.processor.type = load_balancehost1.sinkgroups.group1.processor.selector = <selector type>host1.sinkgroups.group1.processor.selector.selector_property = <value>The value of processor.selector could be either <tt>round_robin</tt> forround-robin scheme of load-balancing or <tt>random</tt> for randomselection. Alternatively you can specify your own implementation of theselection algorithm by implementing the <tt>LoadBalancingSelector</tt>interface. If no selector mechanism is specified, the round-robin selectoris used by default.This implementation is not thread safe at this timeLoadBalancingSinkProcessor.SinkSelectorAn interface that allows the LoadBalancingSinkProcessor to usea load-balancing strategy such as round-robin, random distribution etc.Implementations of this class can be plugged into the system viaprocessor configuration and are used to select a sink on every invocation.An instance of the configured sink selector is create during the processorconfiguration, its {@linkplain #setSinks(List)} method is invoked followingwhich it is configured via a subcontext. Once configured, the lifecycle ofthis selector is tied to the lifecycle of the sink processor.At runtime, the processor invokes the {@link #createSinkIterator()}method for every <tt>process</tt> call to create an iteration order overthe available sinks. The processor then loops through this iteration orderuntil one of the sinks succeeds in processing the event. If the iteratoris exhausted and none of the sinks succeed, the processor will raisean <tt>EventDeliveryException</tt>.<p>A sink selector that implements the round-robin sink selection policy.This implementation is not MT safe.</p><p>Unfortunately both implementations need to override the base implementationin AbstractSinkSelector class, because any custom sink selectorswill break if this stuff is moved to that class.</p>A sink selector that implements a random sink selection policy. Thisimplementation is not thread safe./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/sink/LoggerSink.javamaxBytesToLog/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sink/LoggerSink.classClass<LoggerSink>"maxBytesToLog"strMaxBytes"Unable to convert %s to integer, using default value(%d) for maxByteToDump"Unable to convert %s to integer, using default value(%d) for maxByteToDump"Event: "Event: "Failed to log event: "Failed to log event: A {@link org.apache.flume.Sink} implementation that logs all events received at the INFO levelto the <tt>org.apache.flume.sink.LoggerSink</tt> logger.<b>WARNING:</b> Logging events can potentially introduce performancedegradation.<i>This sink has no configuration parameters.</i> Default Max bytes to dump Max number of bytes to be dumped No event found, request back-off semantics from the sink runner/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/sink/NullSink.javalogEveryNEvents/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sink/NullSink.classDFLT_LOG_EVERY_N_EVENTSDFLT_BATCH_SIZEClass<NullSink>"batchSize"" " +
        "batch size set to " batch size set to "Batch size must be > 0"Batch size must be > 0"logEveryNEvents"" " +
        "log event N events set to " log event N events set to "logEveryNEvents must be > 0"logEveryNEvents must be > 0eventCounter"events.success"events.success"Null sink {} successful processed {} events."Null sink {} successful processed {} events."transaction.success"transaction.success"transaction.failed"transaction.failed"Failed to deliver event. Exception follows."Failed to deliver event. Exception follows."Failed to deliver event: "Failed to deliver event: "Null sink {} started."Null sink {} started."Null sink {} stopping..."Null sink {} stopping..."Null sink {} stopped. Event metrics: {}"Null sink {} stopped. Event metrics: {}"NullSink "NullSink " { batchSize: " { batchSize: A {@link Sink} implementation that simply discards all events it receives. A<tt>/dev/null</tt> for Flume./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/sink/RollingFileSink.javashouldRotate/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sink/RollingFileSink.classpathControllerrollServicerollIntervaldefaultBatchSizedefaultRollIntervalClass<RollingFileSink>pathManagerType"sink.pathManager"sink.pathManager"sink.directory"sink.directory"sink.rollInterval"sink.rollIntervalpathManagerContext"sink."sink."sink.serializer"sink.serializer"TEXT""Directory may not be null"Directory may not be null"Serializer type is undefined"Serializer type is undefined"sink.batchSize"sink.batchSize"rollingFileSink-roller-"rollingFileSink-roller-/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sink/RollingFileSink$1.class"Marking time to rotate file {}"Marking time to rotate file {}"RollInterval is not valid, file rolling will not happen."RollInterval is not valid, file rolling will not happen."RollingFileSink {} started."RollingFileSink {} started."Time to rotate {}"Time to rotate {}"Closing file {}"Closing file {}"Unable to rotate file "Unable to rotate file " while delivering event" while delivering event"Opening output stream for file {}"Opening output stream for file {}"Failed to open file "Failed to open file eventAttemptCounter"Failed to process transaction"Failed to process transaction"RollingFile sink {} stopping..."RollingFile sink {} stopping..."Unable to close output stream. Exception follows."Unable to close output stream. Exception follows."Interrupted while waiting for roll service to stop. " +
                       "Please report this."Interrupted while waiting for roll service to stop. Please report this."RollingFile sink {} stopped. Event metrics: {}"RollingFile sink {} stopped. Event metrics: {}Every N seconds, mark that it's time to rotate. We purposefully do NOTtouch anything other than the indicator flag to avoid error handlingissues (e.g. IO exceptions occuring in two different threads.Resist the urge to actually perform rotation in a separate thread!FIXME: Feature: Rotate on size and time by checking bytes written andsetting shouldRotate = true if we're past a threshold.FIXME: Feature: Control flush interval based on time or number ofevents. For now, we're super-conservative and flush on each write. No events found, request back-off semantics from runner/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/sink/SinkGroup.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sink/SinkGroup.class"sinkgrp"sinkgrp"Invalid Configuration!"Invalid Configuration!<p>Configuration concept for handling multiple sinks working together.</p>org.apache.flume.conf.properties.PropertiesFileConfigurationProvider/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/sink/SinkProcessorFactory.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sink/SinkProcessorFactory.classClass<SinkProcessorFactory>typeStrprocessorClass"Sink Processor type {} is a custom type"Sink Processor type {} is a custom type"Creating instance of sink processor type {}, class {}"Creating instance of sink processor type {}, class {}"Unable to load sink processor type: "Unable to load sink processor type: "Unable to create sink processor, type: "Unable to create sink processor, type: "Sink type {} does not exist, using default"Sink type {} does not exist, using default"Unable to create processor, type: "Unable to create processor, type: Creates a sink processor and configures it using the provided contextContext limited to that of the processor. Should include typeand any settings relevant to that processor type. Referer to javadoc forspecific sinkA non-null, non-empty list of sinks to be assigned to theA configured SinkProcessororg.apache.flume.SinkProcessorTypeRuntime exception thrown in the case of an invalidprocessor configuration/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/sink/ThriftSink.javaenableKerberos"false"A {@link org.apache.flume.Sink} implementation that can send events to an RPC server (such asFlume's {@link org.apache.flume.source.ThriftSource}).The destination is an instance of Flume's{@link org.apache.flume.source.ThriftSource}, whichEvents are taken from the configured {@link org.apache.flume.Channel} in batches of the Only one thread is enough, since only one sink thread processes transactions at any given time. Each sink owns its own Rpc client./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/AbstractEventDrivenSource.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/sourceorg.apache.flume.sourceBase class which ensures sub-classes will inherit all the propertiesof BasicSourceSemantics. Adds no additional functionality and is providedfor completeness sake./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/AbstractPollableSource.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/AbstractPollableSource.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/sourceexception"Source had error configuring or starting"Source had error configuring or starting"Source is not started.  It is in '"Source is not started.  It is in '"' state"' stateof BasicSourceSemantics in addition to:<li>Ensuring when configure/start throw an exception process will notbe called</li><li>Ensure that process will not be called unless configure and starthave successfully been called</li>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/AbstractSource.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/AbstractSource.class"No channel processor configured"No channel processor configuredcp? extends AbstractSourceClass<? extends AbstractSource>Map<String,? extends AbstractSource>AbstractSource[]Constructor<? extends AbstractSource>TypeVariable<Class<? extends AbstractSource>>TypeVariable<Class<? extends AbstractSource>>[]",state:",state:/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/AvroSource.javagenerateRulegenerateRule(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/AvroSource.classMap<CharSequence,CharSequence>toStringMaptoStringMap(java.util.Map)List<IpFilterRule>SequencedCollection<IpFilterRule>Collection<IpFilterRule>Iterable<IpFilterRule>rulesconnectionCountUpdatermaxThreadspatternRuleConfigDefinitionenableIpFiltercompressionTypebindAddressIP_FILTER_RULES_KEYIP_FILTER_KEYCOMPRESSION_TYPEBIND_KEYPORT_KEYTHREADS"threads"threadsClass<AvroSource>"bind"bind"compression-type"compression-type"ipFilter"ipFilter"ipFilterRules"ipFilterRules"none"none"AVRO source\'s \"threads\" property must specify an integer value."AVRO source's "threads" property must specify an integer value.patternRuleDefinitions"ipFilter is configured with true but ipFilterRules is not defined:" +
            " "ipFilter is configured with true but ipFilterRules is not defined: ArrayList<IpFilterRule>AbstractList<IpFilterRule>AbstractCollection<IpFilterRule>ArrayList<IpFilterRule>(int)Spliterator<IpFilterRule>? super IpFilterRuleConsumer<? super IpFilterRule>Iterator<IpFilterRule>Stream<IpFilterRule>BaseStream<IpFilterRule,Stream<IpFilterRule>>Predicate<? super IpFilterRule>? extends IpFilterRuleCollection<? extends IpFilterRule>Iterable<? extends IpFilterRule>add(io.netty.handler.ipfilter.IpFilterRule)AbstractCollection<IpFilterRule>()addLast(io.netty.handler.ipfilter.IpFilterRule)addFirst(io.netty.handler.ipfilter.IpFilterRule)ListIterator<IpFilterRule>add(int,io.netty.handler.ipfilter.IpFilterRule)set(int,io.netty.handler.ipfilter.IpFilterRule)Comparator<? super IpFilterRule>UnaryOperator<IpFilterRule>Function<IpFilterRule,IpFilterRule>AbstractList<IpFilterRule>()ArrayList<IpFilterRule>(java.util.Collection)ArrayList<IpFilterRule>()patternRuleDefinitionsrvresponderClass<AvroSourceProtocol>enableCompression"deflate"deflateConsumer<SocketChannel>Consumer<>andThenandThen(java.util.function.Consumer)? super SocketChannelConsumer<? super SocketChannel>acceptaccept(java.lang.Object)accept(io.netty.channel.socket.SocketChannel)pipelineOptional<SSLEngine>engine"deflater"deflater"inflater"inflater? super SSLEngineConsumer<? super SSLEngine>ifPresentifPresent(java.util.function.Consumer)XorElseThroworElseThrow(java.util.function.Supplier)? extends XSupplier<? extends X>orElseThrow()orElseGetorElseGet(java.util.function.Supplier)? extends SSLEngineSupplier<? extends SSLEngine>orElseorElse(java.lang.Object)orElse(javax.net.ssl.SSLEngine)Stream<T>BaseStream<T,Stream<T>>Stream<SSLEngine>BaseStream<SSLEngine,Stream<SSLEngine>>Supplier<? extends Optional<? extends T>>or(java.util.function.Supplier)Optional<? extends SSLEngine>? extends Optional<? extends SSLEngine>Supplier<? extends Optional<? extends SSLEngine>>Optional<U>flatMapflatMap(java.util.function.Function)Optional<? extends U>? extends Optional<? extends U>Function<? super T,? extends Optional<? extends U>>Function<? super SSLEngine,? extends Optional<? extends U>>map(java.util.function.Function)Function<? super SSLEngine,? extends U>Predicate<? super T>filterfilter(java.util.function.Predicate)Predicate<? super SSLEngine>ifPresentOrElseifPresentOrElse(java.util.function.Consumer,java.lang.Runnable)ofNullableofNullable(java.lang.Object)empty()Consumer<SSLEngine>accept(javax.net.ssl.SSLEngine)sslEngine"ssl"sslIpFilterRule[]"Setting up ipFilter with the following rule definition: "Setting up ipFilter with the following rule definition: "Adding ipFilter with "Adding ipFilter with " rules" rulesnce"Avro source {} startup failed. Cannot initialize Netty server"Avro source {} startup failed. Cannot initialize Netty server"Failed to set up server socket"Failed to set up server socket"Avro source {} started."Avro source {} started."Avro source {} stopping: {}"Avro source {} stopping: {}"Avro source "Avro source ": Interrupted while waiting " +
                "for Avro server to stop. Exiting. Exception follows.": Interrupted while waiting for Avro server to stop. Exiting. Exception follows."Avro source {} stopped. Metrics: {}"Avro source {} stopped. Metrics: {}": { bindAddress: ": { bindAddress: charSeqMapstringMapEntry<CharSequence,CharSequence>Set<Entry<CharSequence,CharSequence>>Collection<Entry<CharSequence,CharSequence>>Iterable<Entry<CharSequence,CharSequence>>? super CharSequenceBiFunction<? super CharSequence,? super CharSequence,? extends CharSequence>merge(java.lang.CharSequence,java.lang.CharSequence,java.util.function.BiFunction)compute(java.lang.CharSequence,java.util.function.BiFunction)computeIfPresent(java.lang.CharSequence,java.util.function.BiFunction)Function<? super CharSequence,? extends CharSequence>computeIfAbsent(java.lang.CharSequence,java.util.function.Function)replace(java.lang.CharSequence,java.lang.CharSequence)replace(java.lang.CharSequence,java.lang.CharSequence,java.lang.CharSequence)putIfAbsent(java.lang.CharSequence,java.lang.CharSequence)BiConsumer<? super CharSequence,? super CharSequence>getOrDefault(java.lang.Object,java.lang.CharSequence)Collection<CharSequence>Iterable<CharSequence>Set<CharSequence>Map<? extends CharSequence,? extends CharSequence>put(java.lang.CharSequence,java.lang.CharSequence)setValue(java.lang.CharSequence)avroEvent"Avro source {}: Received avro event: {}"Avro source {}: Received avro event: {}"Avro source {}: Received avro event"Avro source {}: Received avro event": Unable to process event. " +
          "Exception follows.": Unable to process event. Exception follows.List<AvroFlumeEvent>SequencedCollection<AvroFlumeEvent>Collection<AvroFlumeEvent>Iterable<AvroFlumeEvent>"Avro source {}: Received avro event batch of {} events."Avro source {}: Received avro event batch of {} events.Spliterator<AvroFlumeEvent>? super AvroFlumeEventConsumer<? super AvroFlumeEvent>Iterator<AvroFlumeEvent>Stream<AvroFlumeEvent>BaseStream<AvroFlumeEvent,Stream<AvroFlumeEvent>>Predicate<? super AvroFlumeEvent>? extends AvroFlumeEventCollection<? extends AvroFlumeEvent>Iterable<? extends AvroFlumeEvent>add(org.apache.flume.source.avro.AvroFlumeEvent)addLast(org.apache.flume.source.avro.AvroFlumeEvent)addFirst(org.apache.flume.source.avro.AvroFlumeEvent)ListIterator<AvroFlumeEvent>add(int,org.apache.flume.source.avro.AvroFlumeEvent)set(int,org.apache.flume.source.avro.AvroFlumeEvent)Comparator<? super AvroFlumeEvent>UnaryOperator<AvroFlumeEvent>Function<AvroFlumeEvent,AvroFlumeEvent>": Unable to process event " +
          "batch. Exception follows.": Unable to process event batch. Exception follows.firstColonIndex"Invalid ipFilter patternRule '"Invalid ipFilter patternRule '"' should look like <'allow'  or 'deny'>:<'ip' or " +
          "'name'>:<pattern>"' should look like <'allow'  or 'deny'>:<'ip' or 'name'>:<pattern>ruleAccessFlagsecondColonIndexpatternTypeFlagisAllow"allow"allowpatternRuleString"i""n""deny"deny"' should look like <'allow'  or 'deny'>:<'ip' or " +
            "'name'>:<pattern>""Adding ipFilter PatternRule: {} {}"Adding ipFilter PatternRule: {} {}A {@link Source} implementation that receives Avro events from clients thatimplement {@link AvroSourceProtocol}.This source forms one half of Flume's tiered collection support. Internally,this source uses Avro's <tt>NettyTransceiver</tt> to listen for, and handleevents. It can be paired with the builtin <tt>AvroSink</tt> to create tieredcollection topologies. Of course, nothing prevents one from using this sourceto receive data from other custom built infrastructure that uses the sameAvro protocol (specifically {@link AvroSourceProtocol}).Events may be received from the client either singly or in batches.Generally,larger batches are far more efficient, but introduce a slight delay (measuredin millis) in delivery. A batch submitted to the configured {@link Channel}atomically (i.e. either all events make it into the channel or none).<td><tt>bind</tt></td><td>The hostname or IP to which the source will bind.</td><td>Hostname or IP / String</td><td>The port to which the source will bind and listen for events.</td><td>TCP port / int</td><td><tt>threads</tt></td><td>Max number of threads assigned to thread pool, 0 being unlimited</td><td>Count / int</td><td>0(optional)</td>Helper function to convert a map of CharSequence to a map of String.first validate the format/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/BasicSourceSemantics.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/BasicSourceSemantics.classClass<BasicSourceSemantics>"Configure called when started"Configure called when started"Cannot start due to error: name = %s"Cannot start due to error: name = %s"Unexpected error performing start: name = %s"Unexpected error performing start: name = %s"Unexpected error performing stop: name = %s"Unexpected error performing stop: name = %s? extends BasicSourceSemanticsClass<? extends BasicSourceSemantics>Map<String,? extends BasicSourceSemantics>BasicSourceSemantics[]Constructor<? extends BasicSourceSemantics>TypeVariable<Class<? extends BasicSourceSemantics>>TypeVariable<Class<? extends BasicSourceSemantics>>[]Alternative to AbstractSource, which:<li>Ensure configure cannot be called while started</li><li>Exceptions thrown during configure, start, stop put source in ERROR state</li><li>Exceptions thrown during start, stop will be logged but not re-thrown.</li><li>Exception in configure disables starting</li> causes source to be removed by configuration codeException thrown during configure() or start()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/DefaultSourceFactory.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/DefaultSourceFactory.classClass<DefaultSourceFactory>sourceClass"Creating instance of source {}, type {}"Creating instance of source {}, type {}Map<String,? extends Source>Source[]Constructor<? extends Source>TypeVariable<Class<? extends Source>>TypeVariable<Class<? extends Source>>[]"Unable to create source: "Unable to create source: "Source type {} is a custom type"Source type {} is a custom type"Unable to load source type: "Unable to load source type: /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/EventDrivenSourceRunner.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/EventDrivenSourceRunner.class"EventDrivenSourceRunner: { source:"EventDrivenSourceRunner: { source:Starts, stops, and manages{@linkplain EventDrivenSource event-driven sources}./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/ExecSource.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/ExecSource.classbatchTimeoutbufferCountlogStderrrestartrestartThrottlerunnerFutureexecutorshellClass<ExecSource>"Exec source starting with command: {}"Exec source starting with command: {}"Exec source started"Exec source started"Stopping exec source with command: {}"Stopping exec source with command: {}"Stopping exec runner"Stopping exec runner"Exec runner stopped"Exec runner stopped"Waiting for exec executor service to stop"Waiting for exec executor service to stop"Interrupted while waiting for exec executor service "
            + "to stop. Just exiting."Interrupted while waiting for exec executor service to stop. Just exiting."Exec source with command:{} stopped. Metrics:{}"Exec source with command:{} stopped. Metrics:{}"The parameter command must be specified"The parameter command must be specifiedformulateShellCommandformulateShellCommand(java.lang.String,java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/ExecSource$ExecRunnable.classtimeout()flushEventBatchflushEventBatch(java.util.List)timedFlushServicelastPushToChannelsystemClockexitCode"unknown"unknowneventList"timedFlushExecService"timedFlushExecServicestderrReadercommandArgs"StderrReader-["StderrReader-[/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/ExecSource$ExecRunnable$1.class"Exception occurred when processing event batch"Exception occurred when processing event batch"Failed while running command: "Failed while running command: "Failed to close reader for exec source"Failed to close reader for exec source"Restarting in {}ms, exit code {}"Restarting in {}ms, exit code {}"Command ["Command ["] exited with "] exited with shellArgsexitValue"Interrupted while waiting for exec executor service "
                      + "to stop. Just exiting."/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/ExecSource$StderrReader.class"StderrLogger[{}] = '{}'"StderrLogger[{}] = '{}'"StderrLogger exiting"StderrLogger exiting"Failed to close stderr reader for exec source"Failed to close stderr reader for exec sourceA {@link Source} implementation that executes a Unix process and turns eachline of text into an event.This source runs a given Unix command on start up and expects that process tocontinuously produce data on standard out (stderr ignored by default). Unlesstold to restart, if the process exits for any reason, the source also exits andwill produce no further data. This means configurations such as <tt>cat [named pipe]</tt>or <tt>tail -F [file]</tt> are going to produce the desired results where as<tt>date</tt> will probably not - the former two commands produce streams ofdata where as the latter produces a single event and exits.The <tt>ExecSource</tt> is meant for situations where one must integrate withexisting systems without modifying code. It is a compatibility gateway builtto allow simple, stop-gap integration and doesn't necessarily offer all ofthe benefits or guarantees of native integration with Flume. If one has theoption of using the <tt>AvroSource</tt>, for instance, that would be greatlypreferred to this source as it (and similarly implemented sources) canmaintain the transactional guarantees that exec can not.<i>Why doesn't <tt>ExecSource</tt> offer transactional guarantees?</i>The problem with <tt>ExecSource</tt> and other asynchronous sources is thatthe source can not guarantee that if there is a failure to put the event intothe {@link Channel} the client knows about it. As a for instance, one of themost commonly requested features is the <tt>tail -F [file]</tt>-like use casewhere an application writes to a log file on disk and Flume tails the file,sending each line as an event. While this is possible, there's an obviousproblem; what happens if the channel fills up and Flume can't send an event?Flume has no way of indicating to the application writing the log file thatit needs to retain the log or that the event hasn't been sent, for somereason. If this doesn't make sense, you need only know this: <b>Yourapplication can never guarantee data has been received when using aunidirectional asynchronous interface such as ExecSource!</b> As an extensionof this warning - and to be completely clear - there is absolutely zeroguarantee of event delivery when using this source. You have been warned.<td><tt>command</tt></td><td>The command to execute</td><td>String</td><td><tt>restart</tt></td><td>Whether to restart the command when it exits</td><td>Boolean</td><td>false</td><td><tt>restartThrottle</tt></td><td>How long in milliseconds to wait before restarting the command</td><td>Long</td><td>10000</td><td><tt>logStderr</tt></td><td>Whether to log or discard the standard error stream of the command</td><td><tt>batchSize</tt></td><td>The number of events to commit to channel at a time.</td><td>integer</td><td>20</td><td><tt>batchTimeout</tt></td><td>Amount of time (in milliseconds) to wait, if the buffer size was not reached,before data is pushed downstream.</td><td>long</td><td>3000</td> Start the counter before starting any threads that may access it. Start the runner thread. Mark the Source as RUNNING. StderrLogger dies as soon as the input stream is invalid Stop the Thread that flushes periodically There is no need to read 'line' with a charset as we do not to propagate it. It is in UTF-16 and would be printed in UTF-8 format./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/ExecSourceConfigurationConstants.java"restart""restartThrottle"10000L"logStdErr"logStdErr"batchTimeout"3000L3000"shell"Should the exec'ed command restarted if it dies: : default falseAmount of time to wait before attempting a restart: : default 10000 msShould stderr from the command be logged: default falseNumber of lines to read at a timeAmount of time to wait, if the buffer size was not reached, beforeto data is pushed downstream: : default 3000 msCharset for reading inputOptional shell/command processor used to run command/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/MultiportSyslogTCPSource.javakeepFields/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/MultiportSyslogTCPSource.classdefaultDecoderdefaultCharsetclientHostnameHeaderclientIPHeaderportHeaderreadBufferSizemaxEventSizenumProcessorsacceptorportsConcurrentMap<Integer,ThreadSafeDecoder>Map<Integer,ThreadSafeDecoder>portCharsetsArrayList<Integer>AbstractList<Integer>Class<MultiportSyslogTCPSource>ConcurrentHashMap<Integer,ThreadSafeDecoder>/modules/java.base/java/util/concurrent/ConcurrentHashMap.classAbstractMap<Integer,ThreadSafeDecoder>ConcurrentHashMap<Integer,ThreadSafeDecoder>()? super ThreadSafeDecoder? extends ThreadSafeDecoderBiFunction<? super ThreadSafeDecoder,? super ThreadSafeDecoder,? extends ThreadSafeDecoder>merge(java.lang.Integer,org.apache.flume.source.MultiportSyslogTCPSource.ThreadSafeDecoder,java.util.function.BiFunction)BiFunction<? super Integer,? super ThreadSafeDecoder,? extends ThreadSafeDecoder>Function<? super Integer,? extends ThreadSafeDecoder>replace(java.lang.Integer,org.apache.flume.source.MultiportSyslogTCPSource.ThreadSafeDecoder)replace(java.lang.Integer,org.apache.flume.source.MultiportSyslogTCPSource.ThreadSafeDecoder,org.apache.flume.source.MultiportSyslogTCPSource.ThreadSafeDecoder)putIfAbsent(java.lang.Integer,org.apache.flume.source.MultiportSyslogTCPSource.ThreadSafeDecoder)BiConsumer<? super Integer,? super ThreadSafeDecoder>getOrDefault(java.lang.Object,org.apache.flume.source.MultiportSyslogTCPSource.ThreadSafeDecoder)Entry<Integer,ThreadSafeDecoder>Set<Entry<Integer,ThreadSafeDecoder>>Collection<Entry<Integer,ThreadSafeDecoder>>Iterable<Entry<Integer,ThreadSafeDecoder>>Collection<ThreadSafeDecoder>Iterable<ThreadSafeDecoder>Map<? extends Integer,? extends ThreadSafeDecoder>put(java.lang.Integer,org.apache.flume.source.MultiportSyslogTCPSource.ThreadSafeDecoder)AbstractMap<Integer,ThreadSafeDecoder>()ToIntFunction<Entry<K,V>>reduceEntriesToIntreduceEntriesToInt(long,java.util.function.ToIntFunction,int,java.util.function.IntBinaryOperator)ToIntFunction<Entry<Integer,ThreadSafeDecoder>>ToLongFunction<Entry<K,V>>reduceEntriesToLongreduceEntriesToLong(long,java.util.function.ToLongFunction,long,java.util.function.LongBinaryOperator)ToLongFunction<Entry<Integer,ThreadSafeDecoder>>ToDoubleFunction<Entry<K,V>>reduceEntriesToDoublereduceEntriesToDouble(long,java.util.function.ToDoubleFunction,double,java.util.function.DoubleBinaryOperator)ToDoubleFunction<Entry<Integer,ThreadSafeDecoder>>reduceEntriesreduceEntries(long,java.util.function.Function,java.util.function.BiFunction)Function<Entry<K,V>,? extends U>BiFunction<? super U,? super U,? extends U>Function<Entry<Integer,ThreadSafeDecoder>,? extends U>? extends Entry<K,V>BiFunction<Entry<K,V>,Entry<K,V>,? extends Entry<K,V>>reduceEntries(long,java.util.function.BiFunction)? extends Entry<Integer,ThreadSafeDecoder>BiFunction<Entry<Integer,ThreadSafeDecoder>,Entry<Integer,ThreadSafeDecoder>,? extends Entry<Integer,ThreadSafeDecoder>>searchEntriessearchEntries(long,java.util.function.Function)forEachEntryforEachEntry(long,java.util.function.Function,java.util.function.Consumer)Consumer<? super U>? super Entry<K,V>Consumer<? super Entry<K,V>>forEachEntry(long,java.util.function.Consumer)? super Entry<Integer,ThreadSafeDecoder>Consumer<? super Entry<Integer,ThreadSafeDecoder>>ToIntFunction<? super V>reduceValuesToIntreduceValuesToInt(long,java.util.function.ToIntFunction,int,java.util.function.IntBinaryOperator)ToIntFunction<? super ThreadSafeDecoder>ToLongFunction<? super V>reduceValuesToLongreduceValuesToLong(long,java.util.function.ToLongFunction,long,java.util.function.LongBinaryOperator)ToLongFunction<? super ThreadSafeDecoder>ToDoubleFunction<? super V>reduceValuesToDoublereduceValuesToDouble(long,java.util.function.ToDoubleFunction,double,java.util.function.DoubleBinaryOperator)ToDoubleFunction<? super ThreadSafeDecoder>reduceValuesreduceValues(long,java.util.function.Function,java.util.function.BiFunction)Function<? super V,? extends U>Function<? super ThreadSafeDecoder,? extends U>reduceValues(long,java.util.function.BiFunction)searchValuessearchValues(long,java.util.function.Function)forEachValueforEachValue(long,java.util.function.Function,java.util.function.Consumer)Consumer<? super V>forEachValue(long,java.util.function.Consumer)Consumer<? super ThreadSafeDecoder>ToIntFunction<? super K>reduceKeysToIntreduceKeysToInt(long,java.util.function.ToIntFunction,int,java.util.function.IntBinaryOperator)ToIntFunction<? super Integer>ToLongFunction<? super K>reduceKeysToLongreduceKeysToLong(long,java.util.function.ToLongFunction,long,java.util.function.LongBinaryOperator)ToLongFunction<? super Integer>ToDoubleFunction<? super K>reduceKeysToDoublereduceKeysToDouble(long,java.util.function.ToDoubleFunction,double,java.util.function.DoubleBinaryOperator)ToDoubleFunction<? super Integer>reduceKeysreduceKeys(long,java.util.function.Function,java.util.function.BiFunction)Function<? super K,? extends U>Function<? super Integer,? extends U>BiFunction<? super K,? super K,? extends K>reduceKeys(long,java.util.function.BiFunction)BiFunction<? super Integer,? super Integer,? extends Integer>searchKeyssearchKeys(long,java.util.function.Function)forEachKeyforEachKey(long,java.util.function.Function,java.util.function.Consumer)Consumer<? super K>forEachKey(long,java.util.function.Consumer)ToIntBiFunction<? super K,? super V>/modules/java.base/java/util/function/ToIntBiFunction.classreduceToIntreduceToInt(long,java.util.function.ToIntBiFunction,int,java.util.function.IntBinaryOperator)ToIntBiFunction<? super Integer,? super ThreadSafeDecoder>ToLongBiFunction<? super K,? super V>/modules/java.base/java/util/function/ToLongBiFunction.classreduceToLongreduceToLong(long,java.util.function.ToLongBiFunction,long,java.util.function.LongBinaryOperator)ToLongBiFunction<? super Integer,? super ThreadSafeDecoder>ToDoubleBiFunction<? super K,? super V>/modules/java.base/java/util/function/ToDoubleBiFunction.classreduceToDoublereduceToDouble(long,java.util.function.ToDoubleBiFunction,double,java.util.function.DoubleBinaryOperator)ToDoubleBiFunction<? super Integer,? super ThreadSafeDecoder>reducereduce(long,java.util.function.BiFunction,java.util.function.BiFunction)BiFunction<? super K,? super V,? extends U>BiFunction<? super Integer,? super ThreadSafeDecoder,? extends U>searchsearch(long,java.util.function.BiFunction)forEach(long,java.util.function.BiFunction,java.util.function.Consumer)forEach(long,java.util.function.BiConsumer)batchForbatchFor(long)/modules/java.base/java/util/concurrent/ConcurrentHashMap$Node.classuntreeifyuntreeify(java.util.concurrent.ConcurrentHashMap.Node)sumCountsumCount()helpTransferhelpTransfer(java.util.concurrent.ConcurrentHashMap.Node[],java.util.concurrent.ConcurrentHashMap.Node)Node<Integer,ThreadSafeDecoder>Node<Integer,ThreadSafeDecoder>[]resizeStampresizeStamp(int)KeySetView<>/modules/java.base/java/util/concurrent/ConcurrentHashMap$KeySetView.classCollectionView<>/modules/java.base/java/util/concurrent/ConcurrentHashMap$CollectionView.classKeySetView<K,V>CollectionView<K,V,K>keySet(java.lang.Object)KeySetView<Integer,ThreadSafeDecoder>CollectionView<Integer,ThreadSafeDecoder,Integer>keySet(org.apache.flume.source.MultiportSyslogTCPSource.ThreadSafeDecoder)KeySetView<K,Boolean>CollectionView<K,Boolean,K>newKeySetnewKeySet(int)newKeySet()mappingCountmappingCount()Enumeration<ThreadSafeDecoder>Enumeration<Integer>Predicate<? super V>removeValueIfremoveValueIf(java.util.function.Predicate)Predicate<? super ThreadSafeDecoder>Predicate<? super Entry<K,V>>removeEntryIfremoveEntryIf(java.util.function.Predicate)Predicate<? super Entry<Integer,ThreadSafeDecoder>>replaceNodereplaceNode(java.lang.Object,java.lang.Object,java.lang.Object)replaceNode(java.lang.Object,org.apache.flume.source.MultiportSyslogTCPSource.ThreadSafeDecoder,java.lang.Object)putVal(java.lang.Object,java.lang.Object,boolean)putVal(java.lang.Integer,org.apache.flume.source.MultiportSyslogTCPSource.ThreadSafeDecoder,boolean)ConcurrentHashMapConcurrentHashMap(int,float,int)ConcurrentHashMap<Integer,ThreadSafeDecoder>(int,float,int)ConcurrentHashMap(int,float)ConcurrentHashMap<Integer,ThreadSafeDecoder>(int,float)ConcurrentHashMap(java.util.Map)ConcurrentHashMap<Integer,ThreadSafeDecoder>(java.util.Map)ConcurrentHashMap(int)ConcurrentHashMap<Integer,ThreadSafeDecoder>(int)ConcurrentHashMap()setTabAtsetTabAt(java.util.concurrent.ConcurrentHashMap.Node[],int,java.util.concurrent.ConcurrentHashMap.Node)casTabAtcasTabAt(java.util.concurrent.ConcurrentHashMap.Node[],int,java.util.concurrent.ConcurrentHashMap.Node,java.util.concurrent.ConcurrentHashMap.Node)tabAttabAt(java.util.concurrent.ConcurrentHashMap.Node[],int)spreadspread(int)NCPUHASH_BITSRESERVEDTREEBINMOVEDMAX_ARRAY_SIZEportsStrdefaultCharsetStr"Must define config "
            + "parameter for MultiportSyslogTCPSource: ports"Must define config parameter for MultiportSyslogTCPSource: portsportStrListIterator<Integer>add(int,java.lang.Integer)set(int,java.lang.Integer)UnaryOperator<Integer>Function<Integer,Integer>"Unable to parse charset "
          + "string ("Unable to parse charset string (") from port configuration.") from port configuration.portCharsetCfgcharsetStr"Invalid port number in config"Invalid port number in config"Unable to parse charset " +
              "string ("Optional<SSLContext>? super SSLContextConsumer<? super SSLContext>? extends SSLContextSupplier<? extends SSLContext>orElse(javax.net.ssl.SSLContext)Stream<SSLContext>BaseStream<SSLContext,Stream<SSLContext>>Optional<? extends SSLContext>? extends Optional<? extends SSLContext>Supplier<? extends Optional<? extends SSLContext>>Function<? super SSLContext,? extends Optional<? extends U>>Function<? super SSLContext,? extends U>Predicate<? super SSLContext>Consumer<SSLContext>Supplier<Optional<SSLContext>>accept(javax.net.ssl.SSLContext)sslContextsslParametersorg.apache.mina.core.pollingAbstractPollingIoAcceptor<NioSession,ServerSocketChannel>/Users/burakyetistiren/.m2/repository/org/apache/mina/mina-core/2.1.5/mina-core-2.1.5.jar/Users/burakyetistiren/.m2/repository/org/apache/mina/mina-core/2.1.5/Users/burakyetistiren/.m2/repository/org/apache/mina/mina-core/Users/burakyetistiren/.m2/repository/org/apache/mina/Users/burakyetistiren/.m2/repository/org/apache/mina/mina-core/2.1.5/mina-core-2.1.5.jar/org/Users/burakyetistiren/.m2/repository/org/apache/mina/mina-core/2.1.5/mina-core-2.1.5.jar/org/apache/Users/burakyetistiren/.m2/repository/org/apache/mina/mina-core/2.1.5/mina-core-2.1.5.jar/org/apache/mina/Users/burakyetistiren/.m2/repository/org/apache/mina/mina-core/2.1.5/mina-core-2.1.5.jar/org/apache/mina/core/Users/burakyetistiren/.m2/repository/org/apache/mina/mina-core/2.1.5/mina-core-2.1.5.jar/org/apache/mina/core/polling/Users/burakyetistiren/.m2/repository/org/apache/mina/mina-core/2.1.5/mina-core-2.1.5.jar/org/apache/mina/core/polling/AbstractPollingIoAcceptor.classsetReuseAddresssetReuseAddress(boolean)getSessionConfiggetSessionConfig()isReuseAddressisReuseAddress()setBacklogsetBacklog(int)getBackloggetBacklog()newSessionnewSession(java.net.SocketAddress,java.net.SocketAddress)List<? extends SocketAddress>SequencedCollection<? extends SocketAddress>unbind0unbind0(java.util.List)Set<SocketAddress>bindInternalbindInternal(java.util.List)dispose0dispose0()close(java.lang.Object)close(java.nio.channels.ServerSocketChannel)org.apache.mina.core.serviceIoProcessor<S>/Users/burakyetistiren/.m2/repository/org/apache/mina/mina-core/2.1.5/mina-core-2.1.5.jar/org/apache/mina/core/service/Users/burakyetistiren/.m2/repository/org/apache/mina/mina-core/2.1.5/mina-core-2.1.5.jar/org/apache/mina/core/service/IoProcessor.classaccept(org.apache.mina.core.service.IoProcessor,java.lang.Object)IoProcessor<NioSession>accept(org.apache.mina.core.service.IoProcessor,java.nio.channels.ServerSocketChannel)localAddresslocalAddress(java.lang.Object)localAddress(java.nio.channels.ServerSocketChannel)open(java.net.SocketAddress)Iterator<H>selectedHandlesselectedHandles()Iterator<ServerSocketChannel>wakeupwakeup()selectselect()destroydestroy()initinit(java.nio.channels.spi.SelectorProvider)init()AbstractPollingIoAcceptorAbstractPollingIoAcceptor(org.apache.mina.core.session.IoSessionConfig,java.util.concurrent.Executor,org.apache.mina.core.service.IoProcessor)AbstractPollingIoAcceptor<NioSession,ServerSocketChannel>(org.apache.mina.core.session.IoSessionConfig,java.util.concurrent.Executor,org.apache.mina.core.service.IoProcessor)AbstractPollingIoAcceptor(org.apache.mina.core.session.IoSessionConfig,org.apache.mina.core.service.IoProcessor)AbstractPollingIoAcceptor<NioSession,ServerSocketChannel>(org.apache.mina.core.session.IoSessionConfig,org.apache.mina.core.service.IoProcessor)? extends IoProcessor<S>Class<? extends IoProcessor<S>>AbstractPollingIoAcceptor(org.apache.mina.core.session.IoSessionConfig,java.lang.Class,int,java.nio.channels.spi.SelectorProvider)? extends IoProcessor<NioSession>Class<? extends IoProcessor<NioSession>>AbstractPollingIoAcceptor<NioSession,ServerSocketChannel>(org.apache.mina.core.session.IoSessionConfig,java.lang.Class,int,java.nio.channels.spi.SelectorProvider)AbstractPollingIoAcceptor(org.apache.mina.core.session.IoSessionConfig,java.lang.Class,int)AbstractPollingIoAcceptor<NioSession,ServerSocketChannel>(org.apache.mina.core.session.IoSessionConfig,java.lang.Class,int)AbstractPollingIoAcceptor(org.apache.mina.core.session.IoSessionConfig,java.lang.Class)AbstractPollingIoAcceptor<NioSession,ServerSocketChannel>(org.apache.mina.core.session.IoSessionConfig,java.lang.Class)backlogreuseAddress"Could not bind to address: "Could not bind to address: "{} started."{} started."{} stopped. Metrics: {}"{} stopped. Metrics: {}"Multiport Syslog TCP source "Multiport Syslog TCP source parseEventparseEvent(org.apache.flume.source.MultiportSyslogTCPSource.ParsedBuffer,java.nio.charset.CharsetDecoder)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/MultiportSyslogTCPSource$MultiportSyslogHandler.classlineSplittersyslogParserSAVED_BUF"savedBuffer"savedBufferctrsession"Error in syslog message handler"Error in syslog message handler"Session created: {}"Session created: {}"Session opened: {}"Session opened: {}"Session closed: {}"Session closed: {}savedBufparsedLinenum"Parsed null event"Parsed null event"Empty set!"Empty set!"Error writing to channel, event dropped"Error writing to channel, event droppedparsedBuf"Error decoding line with charset ("Error decoding line with charset ("Seen raw event: {}"Seen raw event: {}"Seen raw event."Seen raw event."Error parsing syslog event"Error parsing syslog event/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/MultiportSyslogTCPSource$LineSplitter.classNEWLINEcurBytemsgPosseenNewlineend"Event size larger than specified event size: {}. "
              + "Consider increasing the max event size."Event size larger than specified event size: {}. Consider increasing the max event size."unexpected buffer state: " +
              "msgPos="unexpected buffer state: msgPos=", buf.hasRemaining=", buf.hasRemaining=", savedBuf.hasRemaining=", savedBuf.hasRemaining=", seenNewline=", seenNewline=", maxLen=", maxLen=ParsedBufferParsedBuffer()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/MultiportSyslogTCPSource$ParsedBuffer.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/MultiportSyslogTCPSource$ThreadSafeDecoder.class clear any previous charset configuration and reconfigure it allow user to specify number of processors to use for thread poolNot using the one that takes an array because we won't want one binderror affecting the next. Allocate saved buffer when session is created. This allows us to parse an incomplete message and use it on the next request. debug level so it isn't too spammy together w/ sessionCreated() the character set can be specified per-port while the buffer is not empty take number of events no greater than batchSize don't try to write anything if we didn't get any events somehow write the events to the downstream channelDecodes a syslog-formatted ParsedLine into a Flume Event.Buffer containing characters to be parsedCharacter set is configurable on a per-port basis. fall back to byte arrayThis class is designed to parse lines up to a maximum length. If the lineexceeds the given length, it is cut off at that mark and an overflow flagis set for the line. If less than the specified length is parsed, and anewline is not found, then the parsed data is saved in a buffer providedfor that purpose so that it can be used in the next round of parsing.Parse a line from the IoBuffer {@code buf} and store it into{@code parsedBuf} except for the trailing newline character. If a lineis successfully parsed, returns {@code true}.<p/>If no newline is found, andthe number of bytes traversed is less than {@code maxLineLength}, thenthe data read from {@code buf} is stored in {@code savedBuf} and thismethod returns {@code false}.<p/>If the number of characters traversedequals {@code maxLineLength}, but a newline was not found, then the{@code parsedBuf} variable will be populated, the {@code overflow} flagwill be set in the {@code ParsedBuffer} object, and this function willreturn {@code true}. clear out passed-in ParsedBuffer object carry on from previous buffer we are looking for newline delimiters between events hit a newline? complete the saved buffer throw away newline we either emptied our buffer or hit max msg size exceeded max message size no newline found message fragmentation; save in buffer for later this should never happenPrivate struct to represent a simple text line parsed from a message.The parsed line of text, without the newline character.The incomplete flag is set if the source line length exceeds the maximumallowed line length. In that case, the returned line will have lengthequal to the maximum line length.Package private only for unit testing/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/NetcatSource.javahandlerService/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/NetcatSource.classacceptThreadacceptThreadShouldStopserverSocketsourceEncodingackEveryEventClass<NetcatSource>hostKeyportKeyackEventKeyacceptRunnable"Source starting"Source starting"open.attempts"open.attemptsbindPoint"Created serverSocket:{}"Created serverSocket:{}"open.errors"open.errors"Unable to bind to socket. Exception follows."Unable to bind to socket. Exception follows."netcat-handler-%d"netcat-handler-%d"Source started"Source started"Source stopping"Source stopping"Stopping accept handler thread"Stopping accept handler thread"Waiting for accept handler to finish"Waiting for accept handler to finish"Interrupted while waiting for accept handler to finish"Interrupted while waiting for accept handler to finish"Stopped accept handler thread"Stopped accept handler thread"Unable to close socket. Exception follows."Unable to close socket. Exception follows."Waiting for handler service to stop"Waiting for handler service to stop"Interrupted while waiting for netcat handler service to stop"Interrupted while waiting for netcat handler service to stop"Handler service stopped"Handler service stopped"Source stopped. Event metrics:{}"Source stopped. Event metrics:{}/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/NetcatSource$AcceptHandler.class"Starting accept handler"Starting accept handlersocketChannel"accept.succeeded"accept.succeeded"Unable to accept connection. Exception follows."Unable to accept connection. Exception follows."accept.failed"accept.failed"Accept handler exiting"Accept handler exitingfillfill(java.nio.CharBuffer,java.io.Reader)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/NetcatSource$NetcatSocketHandler.classprocessEventsprocessEvents(java.nio.CharBuffer,java.io.Writer)"Starting connection handler"Starting connection handlercharsReadeventsProcessed"Chars read = {}"Chars read = {}"Events processed = {}"Events processed = {}"Client sent event exceeding the maximum length"Client sent event exceeding the maximum length"events.failed"events.failed"FAILED: Event exceeds the maximum length ("FAILED: Event exceeds the maximum length (" chars, including newline)\n" chars, including newline)
"sessions.completed"sessions.completed"sessions.broken"sessions.broken"Unable to close socket channel. Exception follows."Unable to close socket channel. Exception follows."Connection handler exiting"Connection handler exitingnumProcessedfoundNewLinelimitchEx"events.processed"events.processed"OK\n"OK
"Error processing event. Exception follows."Error processing event. Exception follows."FAILED: "FAILED: "characters.received"characters.receivedA netcat-like source that listens on a given port and turns each line of textinto an event.This source, primarily built for testing and exceedingly simple systems, actslike <tt>nc -k -l [host] [port]</tt>. In other words, it opens a specifiedport and listens for data. The expectation is that the supplied data isnewline separated text. Each line of text is turned into a Flume event andsent via the connected channel.Most testing has been done by using the <tt>nc</tt> client but other,similarly implemented, clients should work just fine.<td><tt>max-line-length</tt></td><td>The maximum # of chars a line can be per event (including newline).</td><td>Number of UTF-8 characters / int</td><td>512</td> wait 500ms for threads to stop Parent is canceling us. flip() so fill() sees buffer as initially empty this method blocks until new data is available in the socket attempt to process all the events in the buffer if we received EOF before last event processing attempt, then we have done everything we can If we get here it means: 1. Last time we called fill(), no new chars were buffered 2. After that, we failed to process any events => no newlines 3. The unread data in the buffer == the size of the buffer Therefore, we are stuck because the client sent a line longer than the size of the buffer. Response: Drop the connection.<p>Consume some number of events from the buffer into the system.</p>Invariants (pre- and post-conditions): <br/>buffer should have position @ beginning of unprocessed data. <br/>buffer should have limit @ end of unprocessed data. <br/>The buffer containing data to processThe channel back to the clientnumber of events successfully processed parse event body bytes out of CharBuffer temporary limit restore limit build event object process event advance position after data is consumed skip newline<p>Refill the buffer read from the socket.</p>Preconditions: <br/>Postconditions: <br/>buffer should have position @ beginning of buffer (pos=0). <br/>Note: this method blocks on new data arriving.The buffer to fillThe Reader to read the data fromnumber of characters read move existing data to the front of the buffer pull in as much data as we can from the socket flip so the data can be consumed/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/NetcatSourceConfigurationConstants.java"ack-every-event"ack-every-event"max-line-length"max-line-length"encoding"encoding"utf-8"utf-8Hostname to bind to.Port to bind to.Ack every event received with an "OK" back to the senderMaximum line length per event.Encoding for the netcat source/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/NetcatUdpSource.javaCONFIG_HOST/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/NetcatUdpSource.classCONFIG_PORTREMOTE_ADDRESS_HEADERremoteHostHeadernettyChannel"REMOTE_ADDRESS"REMOTE_ADDRESSClass<NetcatUdpSource>"remoteAddress"remoteAddressextractEventextractEvent(io.netty.buffer.ByteBuf,java.net.SocketAddress)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/NetcatUdpSource$NetcatHandler.classio.netty.channelSimpleChannelInboundHandler<DatagramPacket>/Users/burakyetistiren/.m2/repository/io/netty/netty-transport/4.1.86.Final/netty-transport-4.1.86.Final.jar/Users/burakyetistiren/.m2/repository/io/netty/netty-transport/4.1.86.Final/Users/burakyetistiren/.m2/repository/io/netty/netty-transport/Users/burakyetistiren/.m2/repository/io/netty/Users/burakyetistiren/.m2/repository/io/Users/burakyetistiren/.m2/repository/io/netty/netty-transport/4.1.86.Final/netty-transport-4.1.86.Final.jar/io/Users/burakyetistiren/.m2/repository/io/netty/netty-transport/4.1.86.Final/netty-transport-4.1.86.Final.jar/io/netty/Users/burakyetistiren/.m2/repository/io/netty/netty-transport/4.1.86.Final/netty-transport-4.1.86.Final.jar/io/netty/channel/Users/burakyetistiren/.m2/repository/io/netty/netty-transport/4.1.86.Final/netty-transport-4.1.86.Final.jar/io/netty/channel/SimpleChannelInboundHandler.classbaosdoneReadingDefaultAddressedEnvelope<ByteBuf,InetSocketAddress>/Users/burakyetistiren/.m2/repository/io/netty/netty-transport/4.1.86.Final/netty-transport-4.1.86.Final.jar/io/netty/channel/DefaultAddressedEnvelope.classAddressedEnvelope<ByteBuf,InetSocketAddress>/Users/burakyetistiren/.m2/repository/io/netty/netty-transport/4.1.86.Final/netty-transport-4.1.86.Final.jar/io/netty/channel/AddressedEnvelope.classcontentcontent()AddressedEnvelope<>touchtouch(java.lang.Object)touch()retainretain(int)retain()recipientrecipient()sendersender()AddressedEnvelope<M,A>release(int)release()refCntrefCnt()DefaultAddressedEnvelopeDefaultAddressedEnvelope(java.lang.Object,java.net.SocketAddress)DefaultAddressedEnvelope<ByteBuf,InetSocketAddress>(io.netty.buffer.ByteBuf,java.net.InetSocketAddress)DefaultAddressedEnvelope(java.lang.Object,java.net.SocketAddress,java.net.SocketAddress)DefaultAddressedEnvelope<ByteBuf,InetSocketAddress>(io.netty.buffer.ByteBuf,java.net.InetSocketAddress,java.net.InetSocketAddress)"events.dropped"events.dropped"Error writing to channel"Error writing to channel"Error retrieving event from udp stream, event dropped"Error retrieving event from udp stream, event droppedio.netty.bootstrapAbstractBootstrap<Bootstrap,Channel>/Users/burakyetistiren/.m2/repository/io/netty/netty-transport/4.1.86.Final/netty-transport-4.1.86.Final.jar/io/netty/bootstrap/Users/burakyetistiren/.m2/repository/io/netty/netty-transport/4.1.86.Final/netty-transport-4.1.86.Final.jar/io/netty/bootstrap/AbstractBootstrap.classAbstractBootstrap<>handlerhandler(io.netty.channel.ChannelHandler)ChannelOption<?>/Users/burakyetistiren/.m2/repository/io/netty/netty-transport/4.1.86.Final/netty-transport-4.1.86.Final.jar/io/netty/channel/ChannelOption.classio.netty.utilAbstractConstant<ChannelOption<?>>/Users/burakyetistiren/.m2/repository/io/netty/netty-common/4.1.86.Final/netty-common-4.1.86.Final.jar/Users/burakyetistiren/.m2/repository/io/netty/netty-common/4.1.86.Final/Users/burakyetistiren/.m2/repository/io/netty/netty-common/Users/burakyetistiren/.m2/repository/io/netty/netty-common/4.1.86.Final/netty-common-4.1.86.Final.jar/io/Users/burakyetistiren/.m2/repository/io/netty/netty-common/4.1.86.Final/netty-common-4.1.86.Final.jar/io/netty/Users/burakyetistiren/.m2/repository/io/netty/netty-common/4.1.86.Final/netty-common-4.1.86.Final.jar/io/netty/util/Users/burakyetistiren/.m2/repository/io/netty/netty-common/4.1.86.Final/netty-common-4.1.86.Final.jar/io/netty/util/AbstractConstant.classConstant<ChannelOption<?>>/Users/burakyetistiren/.m2/repository/io/netty/netty-common/4.1.86.Final/netty-common-4.1.86.Final.jar/io/netty/util/Constant.classComparable<ChannelOption<?>>Entry<ChannelOption<?>,Object>Entry<ChannelOption<?>,Object>[]setChannelOptionssetChannelOptions(io.netty.channel.Channel,java.util.Map.Entry[],io.netty.util.internal.logging.InternalLogger)AttributeKey<?>/Users/burakyetistiren/.m2/repository/io/netty/netty-common/4.1.86.Final/netty-common-4.1.86.Final.jar/io/netty/util/AttributeKey.classAbstractConstant<AttributeKey<?>>Constant<AttributeKey<?>>Comparable<AttributeKey<?>>Entry<AttributeKey<?>,Object>Entry<AttributeKey<?>,Object>[]setAttributessetAttributes(io.netty.channel.Channel,java.util.Map.Entry[])copiedMapcopiedMap(java.util.Map)Map<AttributeKey<?>,Object>attrs()Map<ChannelOption<?>,Object>options()handler()ChannelFactory<>/Users/burakyetistiren/.m2/repository/io/netty/netty-transport/4.1.86.Final/netty-transport-4.1.86.Final.jar/io/netty/bootstrap/ChannelFactory.class? extends CChannelFactory<? extends C>channelFactorychannelFactory()ChannelFactory<? extends Channel>localAddress()attrs0attrs0()options0options0()newAttributesArraynewAttributesArray(java.util.Map)newAttributesArray()newOptionsArraynewOptionsArray(java.util.Map)newOptionsArray()AbstractBootstrapConfig<>/Users/burakyetistiren/.m2/repository/io/netty/netty-transport/4.1.86.Final/netty-transport-4.1.86.Final.jar/io/netty/bootstrap/AbstractBootstrapConfig.classAbstractBootstrapConfig<B,C>config()AbstractBootstrapConfig<Bootstrap,Channel>group()init(io.netty.channel.Channel)initAndRegisterinitAndRegister()bind(java.net.SocketAddress)bind(java.net.InetAddress,int)bind(java.lang.String,int)bind(int)bind()validatevalidate()AttributeKey<>AbstractConstant<>Constant<>attr(io.netty.util.AttributeKey,java.lang.Object)AttributeKey<T>AbstractConstant<AttributeKey<T>>Constant<AttributeKey<T>>Comparable<AttributeKey<T>>ChannelOption<>option(io.netty.channel.ChannelOption,java.lang.Object)ChannelOption<T>AbstractConstant<ChannelOption<T>>Constant<ChannelOption<T>>Comparable<ChannelOption<T>>localAddress(java.net.InetAddress,int)localAddress(java.lang.String,int)localAddress(int)localAddress(java.net.SocketAddress)/Users/burakyetistiren/.m2/repository/io/netty/netty-transport/4.1.86.Final/netty-transport-4.1.86.Final.jar/io/netty/channel/ChannelFactory.classchannelFactory(io.netty.channel.ChannelFactory)channelFactory(io.netty.bootstrap.ChannelFactory)Class<? extends C>channel(java.lang.Class)group(io.netty.channel.EventLoopGroup)AbstractBootstrapAbstractBootstrap(io.netty.bootstrap.AbstractBootstrap)AbstractBootstrap<Bootstrap,Channel>(io.netty.bootstrap.AbstractBootstrap)AbstractBootstrap()AbstractBootstrap<Bootstrap,Channel>()ChannelOption<Boolean>AbstractConstant<ChannelOption<Boolean>>Constant<ChannelOption<Boolean>>Comparable<ChannelOption<Boolean>>Class<NioDatagramChannel>"netty server startup was interrupted"netty server startup was interrupted"Netcat UDP Source stopping..."Netcat UDP Source stopping..."Metrics:{}"Metrics:{}io.netty.util.concurrent/Users/burakyetistiren/.m2/repository/io/netty/netty-common/4.1.86.Final/netty-common-4.1.86.Final.jar/io/netty/util/concurrent/Users/burakyetistiren/.m2/repository/io/netty/netty-common/4.1.86.Final/netty-common-4.1.86.Final.jar/io/netty/util/concurrent/Future.class extract line for building Flume event Entries are separated by '\n' setup Netty server/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/PollableSourceConstants.java"backoffSleepIncrement""maxBackoffSleep"/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/PollableSourceRunner.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/PollableSourceRunner.classClass<PollableSourceRunner>? extends PollableSourceRunnerClass<? extends PollableSourceRunner>Map<String,? extends PollableSourceRunner>PollableSourceRunner[]Constructor<? extends PollableSourceRunner>TypeVariable<Class<? extends PollableSourceRunner>>TypeVariable<Class<? extends PollableSourceRunner>>[]? extends PollableSourceClass<? extends PollableSource>Map<String,? extends PollableSource>PollableSource[]Constructor<? extends PollableSource>TypeVariable<Class<? extends PollableSource>>TypeVariable<Class<? extends PollableSource>>[]"Interrupted while waiting for polling runner to stop. Please report this."Interrupted while waiting for polling runner to stop. Please report this."PollableSourceRunner: { source:"PollableSourceRunner: { source:/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/PollableSourceRunner$PollingRunner.class"Polling runner starting. Source:{}"Polling runner starting. Source:{}"runner.polls"runner.pollscompareTo(org.apache.flume.PollableSource.Status)"Source runner interrupted. Exiting"Source runner interrupted. Exiting"Unhandled exception, logging and sleeping for "Unhandled exception, logging and sleeping for "ms"msAn implementation of {@link SourceRunner} that can drive a{@link PollableSource}.A {@link PollableSourceRunner} wraps a {@link PollableSource} in the requiredrun loop in order for it to operate. Internally, metrics and counters arekept such that a source that returns a {@link PollableSource.Status} of{@code BACKOFF} causes the run loop to do exactly that. There's a maximumbackoff period of 500ms. A source that returns {@code READY} is immediatelyinvoked. Note that {@code BACKOFF} is merely a hint to the runner; it neednot be strictly adhered to./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/SequenceGeneratorSource.javaeventsSent/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/SequenceGeneratorSource.classtotalEventsClass<SequenceGeneratorSource>"totalEvents""batchSize was %s but expected positive"batchSize was %s but expected positiveeventsSentTXbatchArrayList" source could not write to channel." source could not write to channel."Sequence generator source do starting"Sequence generator source do starting"Sequence generator source do started"Sequence generator source do started"Sequence generator source do stopping"Sequence generator source do stopping"Sequence generator source do stopped. Metrics:{}"Sequence generator source do stopped. Metrics:{}<li>batchSize = type int that defines the size of event batches/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/SpoolDirectorySource.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/SpoolDirectorySource.classpollDelayhitChannelFullExceptionhitChannelExceptionhasFatalErrorbasenameHeaderKeybasenameHeaderfileHeaderKeyfileHeaderClass<SpoolDirectorySource>"SpoolDirectorySource source starting with directory: {}"SpoolDirectorySource source starting with directory: {}ioe"Error instantiating spooling event parser"Error instantiating spooling event parser"SpoolDirectorySource source started"SpoolDirectorySource source started10L"Interrupted while awaiting termination"Interrupted while awaiting termination"SpoolDir source {} stopped. Metrics: {}"SpoolDir source {} stopped. Metrics: {}"Spool Directory source "Spool Directory source ": { spoolDir: ": { spoolDir: bufferMaxLineLength"Configuration must specify a spooling directory"Configuration must specify a spooling directoryEnum<ConsumeOrder>Comparable<ConsumeOrder>compareTo(org.apache.flume.source.SpoolDirectorySourceConfigurationConstants.ConsumeOrder)EnumDesc<ConsumeOrder>DynamicConstantDesc<ConsumeOrder>Optional<EnumDesc<ConsumeOrder>>Class<ConsumeOrder>Enum<ConsumeOrder>(java.lang.String,int)waitAndGetNewBackoffIntervalwaitAndGetNewBackoffInterval(int)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/SpoolDirectorySource$SpoolDirectoryRunnable.classbackoffInterval250readingEvents"The channel is full, and cannot write data now. The " +
                "source will try again after "The channel is full, and cannot write data now. The source will try again after " milliseconds" milliseconds"The channel threw an exception, and cannot write data now. The " +
                "source will try again after "The channel threw an exception, and cannot write data now. The source will try again after "FATAL: "FATAL: ": " +
            "Uncaught exception in SpoolDirectorySource thread. " +
            "Restart or reconfigure Flume to continue processing.": Uncaught exception in SpoolDirectorySource thread. Restart or reconfigure Flume to continue processing.Config options "Hack" to support backwards compatibility with previous generation of spooling directory source, which did not support deserializersThe class always backs off, this exists only so that we can test withouttaking a really long time.- whether the source should backoff if the channel is full/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/SpoolDirectorySourceConfigurationConstants.java"spoolDir"spoolDir"fileSuffix"fileSuffix".COMPLETED".COMPLETED"fileHeaderKey""fileHeader""basenameHeaderKey""basename""basenameHeader""bufferMaxLines"bufferMaxLines"bufferMaxLineLength""includePattern""^.*$"^.*$"ignorePattern""^$"^$"trackerDir"trackerDir".flumespool".flumespool"deserializer""LINE"LINE"deletePolicy""never"never"trackingPolicy""rename"rename"inputCharset""decodeErrorPolicy"Enum<DecodeErrorPolicy>Comparable<DecodeErrorPolicy>compareTo(org.apache.flume.serialization.DecodeErrorPolicy)EnumDesc<DecodeErrorPolicy>DynamicConstantDesc<DecodeErrorPolicy>Optional<EnumDesc<DecodeErrorPolicy>>Class<DecodeErrorPolicy>Enum<DecodeErrorPolicy>(java.lang.String,int)"maxBackoff"4000"consumeOrder""recursiveDirectorySearch""pollDelay"ConsumeOrderConsumeOrder()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/SpoolDirectorySourceConfigurationConstants$ConsumeOrder.classDirectory where files are deposited.Suffix appended to files when they are finished being sent.Header in which to put absolute path filename.Whether to include absolute path filename in a header.Header in which to put the basename of file.Whether to include the basename of a file in a header.What size to batch with before sending to ChannelProcessor.Maximum number of lines to buffer between commits.Maximum length of line (in characters) in buffer between commits.Pattern of files to include any filePattern of files to ignore no effectDirectory to store metadata about files being processedDeserializer to use to parse the file data into Flume EventsCharacter set used when reading the input.What to do when there is a character set decoding error.Consume order.Flag to indicate if we should recursively checking for new files. Thedefault is false, so a configuration file entry would be needed to enablethis settingDelay(in milliseconds) used when polling for new files. The default is 500ms/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/SslContextAwareAbstractSource.javaparseListparseList(java.lang.String,java.util.Set)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/SslContextAwareAbstractSource.classgetFilteredCipherSuitesgetFilteredCipherSuites(java.lang.String[])getFilteredProtocolsgetFilteredProtocols(java.lang.String[])getSslContextgetSslContext()includeCipherSuitesexcludeCipherSuitesincludeProtocolsexcludeProtocolssslEnabledkeystoreTypekeystorePasswordkeystoreINCLUDE_CIPHER_SUITESEXCLUDE_CIPHER_SUITESINCLUDE_PROTOCOLSEXCLUDE_PROTOCOLSKEYSTORE_TYPE_DEFAULT_VALUEKEYSTORE_TYPE_KEYKEYSTORE_PASSWORD_KEYKEYSTORE_KEYSSL_ENABLED_DEFAULT_VALUESSL_ENABLED_KEYLinkedHashSet<String>/modules/java.base/java/util/LinkedHashSet.classLinkedHashSet<String>(java.util.Collection)LinkedHashMap<E,Object>map()LinkedHashMap<String,Object>HashMap<String,Object>AbstractMap<String,Object>Map<String,Object>SequencedMap<String,Object>LinkedHashSet<>LinkedHashSet<T>SequencedSet<T>newLinkedHashSetnewLinkedHashSet(int)LinkedHashSetLinkedHashSet(java.util.Collection)LinkedHashSet()LinkedHashSet<String>()LinkedHashSet(int)LinkedHashSet<String>(int)LinkedHashSet(int,float)LinkedHashSet<String>(int,float)"SSLv3"SSLv3"keystore""keystore-password"keystore-password"keystore-type"keystore-type"JKS"JKS"exclude-protocols"exclude-protocols"include-protocols"include-protocols"exclude-cipher-suites"exclude-cipher-suites"include-cipher-suites"include-cipher-suites" must be specified when SSL is enabled" must be specified when SSL is enabled"Source "Source " configured with invalid keystore: " configured with invalid keystore: algorithmkmfserverContext"TLS"TLSKeyManager[]TrustManager[]"Failed to initialize the server-side SSLContext"Failed to initialize the server-side SSLContextuseClientModeFunction<SSLContext,SSLEngine>Function<T,T>identityidentity()Function<T,V>andThen(java.util.function.Function)? super RFunction<? super R,? extends V>Function<SSLContext,V>Function<? super SSLEngine,? extends V>Function<V,R>composecompose(java.util.function.Function)Function<? super V,? extends T>Function<V,SSLEngine>Function<? super V,? extends SSLContext>applyapply(java.lang.Object)apply(javax.net.ssl.SSLContext)Supplier<Optional<SSLEngine>>enabledProtocolsIntFunction<A[]>onCloseonClose(java.lang.Runnable)unorderedunordered()parallelparallel()sequentialsequential()isParallelisParallel()concatconcat(java.util.stream.Stream,java.util.stream.Stream)Stream<? extends T>BaseStream<? extends T,Stream<? extends T>>generate(java.util.function.Supplier)Predicate<>UnaryOperator<>iterateiterate(java.lang.Object,java.util.function.Predicate,java.util.function.UnaryOperator)UnaryOperator<T>iterate(java.lang.Object,java.util.function.UnaryOperator)/modules/java.base/java/util/stream/Stream$Builder.classBuilder<T>Consumer<T>findAnyfindAny()Optional<String>findFirstfindFirst()noneMatchnoneMatch(java.util.function.Predicate)allMatchallMatch(java.util.function.Predicate)anyMatchanyMatch(java.util.function.Predicate)count()max(java.util.Comparator)min(java.util.Comparator)toListtoList()collectcollect(java.util.stream.Collector)Collector<? super T,A,R>Collector<? super String,A,R>BiConsumer<>collect(java.util.function.Supplier,java.util.function.BiConsumer,java.util.function.BiConsumer)Supplier<R>BiConsumer<R,? super T>BiConsumer<R,R>BiConsumer<R,? super String>reduce(java.lang.Object,java.util.function.BiFunction,java.util.function.BinaryOperator)BiFunction<U,? super T,U>BinaryOperator<U>BiFunction<U,U,U>BiFunction<U,? super String,U>BinaryOperator<T>BiFunction<T,T,T>reduce(java.util.function.BinaryOperator)BinaryOperator<String>BiFunction<String,String,String>reduce(java.lang.Object,java.util.function.BinaryOperator)reduce(java.lang.String,java.util.function.BinaryOperator)forEachOrderedforEachOrdered(java.util.function.Consumer)dropWhiledropWhile(java.util.function.Predicate)takeWhiletakeWhile(java.util.function.Predicate)skip(long)limit(long)peek(java.util.function.Consumer)sortedsorted(java.util.Comparator)sorted()distinctdistinct()? super DoubleConsumerBiConsumer<? super T,? super DoubleConsumer>mapMultiToDoublemapMultiToDouble(java.util.function.BiConsumer)BiConsumer<? super String,? super DoubleConsumer>? super LongConsumerBiConsumer<? super T,? super LongConsumer>mapMultiToLongmapMultiToLong(java.util.function.BiConsumer)BiConsumer<? super String,? super LongConsumer>? super IntConsumerBiConsumer<? super T,? super IntConsumer>mapMultiToIntmapMultiToInt(java.util.function.BiConsumer)BiConsumer<? super String,? super IntConsumer>Stream<R>BaseStream<R,Stream<R>>mapMultimapMulti(java.util.function.BiConsumer)Consumer<R>? super Consumer<R>BiConsumer<? super T,? super Consumer<R>>BiConsumer<? super String,? super Consumer<R>>? extends DoubleStreamFunction<? super T,? extends DoubleStream>flatMapToDoubleflatMapToDouble(java.util.function.Function)Function<? super String,? extends DoubleStream>? extends LongStreamFunction<? super T,? extends LongStream>flatMapToLongflatMapToLong(java.util.function.Function)Function<? super String,? extends LongStream>? extends IntStreamFunction<? super T,? extends IntStream>flatMapToIntflatMapToInt(java.util.function.Function)Function<? super String,? extends IntStream>? extends RStream<? extends R>BaseStream<? extends R,Stream<? extends R>>? extends Stream<? extends R>Function<? super T,? extends Stream<? extends R>>Function<? super String,? extends Stream<? extends R>>mapToDoublemapToDouble(java.util.function.ToDoubleFunction)ToDoubleFunction<? super String>mapToLongmapToLong(java.util.function.ToLongFunction)ToLongFunction<? super String>mapToIntmapToInt(java.util.function.ToIntFunction)ToIntFunction<? super String>Function<? super T,? extends R>Function<? super String,? extends R>IntFunction<String[]>Predicate<String>Predicate<T>notnot(java.util.function.Predicate)isEqualisEqual(java.lang.Object)or(java.util.function.Predicate)negatenegate()andand(java.util.function.Predicate)test(java.lang.Object)test(java.lang.String)apply(int)enabledCipherSuites can be set with "ssl.KeyManagerFactory.algorithm" Set up key manager factory to use our key store/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/StressSource.javaprepEventDataprepEventData(int)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/StressSource.classlimitereventBatchListToProcesseventBatchListlastSentmaxSuccessfulEventsmaxTotalEventsClass<StressSource>"size"rateLimit"maxEventsPerSecond"maxEventsPerSecond"maxTotalEvents""maxSuccessfulEvents"bufferSizetotalEventSent"events.total"events.total"events.successful"events.successfuleventsLeft"Stress source doStart finished"Stress source doStart finished"Stress source do stop. Metrics:{}"Stress source do stop. Metrics:{}StressSource is an internal load-generating source implementationwhich is very useful for stress tests. It allows User to configurethe size of Event payload, with empty headers. User can configuretotal number of events to be sent as well maximum number of SuccessfulEvents to be delivered. Useful for testsExample configuration for Agent a1<PRE>a1.sources = stresssource-1a1.channels = memoryChannel-1a1.sources.stresssource-1.type = org.apache.flume.source.StressSourcea1.sources.stresssource-1.size = 10240a1.sources.stresssource-1.maxTotalEvents = 1000000a1.sources.stresssource-1.channels = memoryChannel-1</PRE>See {@link StressSource#configure(Context)} for configuration options.<li>-maxTotalEvents = type long that defines the total number of Events to be sent<li>-maxSuccessfulEvents = type long that defines the number of successful Events<li>-size = type int that defines the number of bytes in each Event<li>-batchSize = type int that defines the number of Events being sent in one batchLimit on the total number of events.Limit on the total number of successful events.Set max events in a batch submissionSize of events to be generated.Create event objects in case of batch testCreate single event in case of non-batch testCast is safe because eventsLeft must be <= batchSize which is an int/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/SyslogParser.javacom.google.common.cacheLoadingCache<String,Long>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/cache/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/cache/LoadingCache.classCache<String,Long>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/cache/Cache.class"Use CacheBuilder.newBuilder().build()"Use CacheBuilder.newBuilder().build()Function<String,Long>timestampCache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/SyslogParser.classtimeParserRFC5424_PREFIX_LENRFC3164_LENtimePatrfc3164FormatTWO_SPACESTS_CACHE_MAXClass<SyslogParser>"MMM d HH:mm:ss"MMM d HH:mm:ss"yyyy-MM-dd'T'HH:mm:ss"yyyy-MM-dd'T'HH:mm:ss19CacheBuilder<Object,Object>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/cache/CacheBuilder.classCacheLoader<>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/cache/CacheLoader.classLoadingCache<>Cache<>LoadingCache<K1,V1>Cache<K1,V1>Function<K1,V1>build(com.google.common.cache.CacheLoader)? super K1CacheLoader<? super K1,V1>? extends StatsCounterSupplier<? extends StatsCounter>getStatsCounterSuppliergetStatsCounterSupplier()isRecordingStatsisRecordingStats()CacheBuilder<>recordStatsrecordStats()RemovalListener<>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/cache/RemovalListener.classRemovalListener<K1,V1>getRemovalListenergetRemovalListener()CacheBuilder<K1,V1>removalListenerremovalListener(com.google.common.cache.RemovalListener)? super V1RemovalListener<? super K1,? super V1>getTickergetTicker(boolean)tickerticker(com.google.common.base.Ticker)getRefreshNanosgetRefreshNanos()refreshAfterWriterefreshAfterWrite(long,java.util.concurrent.TimeUnit)refreshAfterWrite(java.time.Duration)getExpireAfterAccessNanosgetExpireAfterAccessNanos()expireAfterAccessexpireAfterAccess(long,java.util.concurrent.TimeUnit)expireAfterAccess(java.time.Duration)getExpireAfterWriteNanosgetExpireAfterWriteNanos()expireAfterWriteexpireAfterWrite(long,java.util.concurrent.TimeUnit)expireAfterWrite(java.time.Duration)getValueStrengthgetValueStrength()setValueStrengthsetValueStrength(com.google.common.cache.LocalCache.Strength)softValuessoftValues()weakValuesweakValues()getKeyStrengthgetKeyStrength()setKeyStrengthsetKeyStrength(com.google.common.cache.LocalCache.Strength)weakKeysweakKeys()Weigher<>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/cache/Weigher.classWeigher<K1,V1>getWeighergetWeigher()getMaximumWeightgetMaximumWeight()weigherweigher(com.google.common.cache.Weigher)Weigher<? super K1,? super V1>maximumWeightmaximumWeight(long)maximumSizemaximumSize(long)getConcurrencyLevelgetConcurrencyLevel()concurrencyLevelconcurrencyLevel(int)getInitialCapacitygetInitialCapacity()initialCapacityinitialCapacity(int)Equivalence<>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/base/Equivalence.classBiPredicate<>/modules/java.base/java/util/function/BiPredicate.classEquivalence<Object>BiPredicate<Object,Object>getValueEquivalencegetValueEquivalence()valueEquivalencevalueEquivalence(com.google.common.base.Equivalence)getKeyEquivalencegetKeyEquivalence()keyEquivalencekeyEquivalence(com.google.common.base.Equivalence)lenientParsinglenientParsing()from(java.lang.String)from(com.google.common.cache.CacheBuilderSpec)newBuildernewBuilder()statsCounterSupplierRemovalListener<? super K,? super V>refreshNanosexpireAfterAccessNanosexpireAfterWriteNanosvalueStrengthkeyStrengthWeigher<? super K,? super V>strictParsingUNSET_INTNULL_TICKERSupplier<StatsCounter>CACHE_STATS_COUNTEREMPTY_STATSNULL_STATS_COUNTER/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/SyslogParser$1.classCacheLoader<String,Long>CacheLoader<K,V>asyncReloadingasyncReloading(com.google.common.cache.CacheLoader,java.util.concurrent.Executor)CacheLoader<Object,V>from(com.google.common.base.Supplier)Supplier<V>from(com.google.common.base.Function)Function<K,V>loadAllloadAll(java.lang.Iterable)Map<String,Long>com.google.common.util.concurrentListenableFuture<>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/util/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/util/concurrent/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/util/concurrent/ListenableFuture.class"Use the methods in Futures (like immediateFuture) or SettableFuture"Use the methods in Futures (like immediateFuture) or SettableFutureListenableFuture<V>Future<V>reloadreload(java.lang.Object,java.lang.Object)ListenableFuture<Long>Future<Long>reload(java.lang.String,java.lang.Long)loadload(java.lang.Object)load(java.lang.String)CacheLoaderCacheLoader()CacheLoader<String,Long>()msgLencurPosendBracketPos'>'>prifacilitytsStringdateStartCharnextSpace' ''<'<"Bad format: invalid priority: cannot find open bracket '<' (%s)"Bad format: invalid priority: cannot find open bracket '<' (%s)"Bad format: invalid priority: cannot find end bracket '>' (%s)"Bad format: invalid priority: cannot find end bracket '>' (%s)"Bad format: no data except priority (%s)"Bad format: no data except priority (%s)"1 "1 "bad syslog format (missing hostname)"bad syslog format (missing hostname)'Z'Z"bad timestamp format"bad timestamp format"Unable to parse message: "Unable to parse message: timestampPrefixtzFirst"Bad format: Not a valid RFC5424 timestamp: %s"Bad format: Not a valid RFC5424 timestamp: %scleanUpcleanUp()ConcurrentMap<String,Long>statsstats()invalidateAllinvalidateAll()invalidateAll(java.lang.Iterable)invalidateinvalidate(java.lang.Object)Map<? extends String,? extends Long>put(java.lang.String,java.lang.Long)getAllPresentgetAllPresent(java.lang.Iterable)ImmutableMap<String,Long>Callable<? extends V>get(java.lang.Object,java.util.concurrent.Callable)Callable<? extends Long>get(java.lang.String,java.util.concurrent.Callable)getIfPresentgetIfPresent(java.lang.Object)Function<String,V>Function<? super Long,? extends V>Function<V,Long>Function<? super V,? extends String>apply(java.lang.String)refresh(java.lang.Object)refresh(java.lang.String)getAllgetAll(java.lang.Iterable)getUncheckedgetUnchecked(java.lang.Object)getUnchecked(java.lang.String)"Parsing error: timestamp is null"Parsing error: timestamp is nullfoundEndendMillisPosfractionalPositions"bad timestamp format (no TZ)"bad timestamp format (no TZ)curDigit'0''9'milliseconds"Bad format: Invalid timestamp (fractional portion): "Bad format: Invalid timestamp (fractional portion): '+'+polarity"Bad format: Invalid timezone (%s)"Bad format: Invalid timezone (%s)':'hourOffsetminOffset60000"Bad format: Invalid timezone: "Bad format: Invalid timezone: year"rfc3164 date parse failed on ("rfc3164 date parse failed on ("): invalid format"): invalid formatfixedTo change this template, choose Tools | Templatesand open the template in the editor. timestamp cache size limitParses a Flume Event out of a syslog message string.Syslog message, not including the newline characterParsed Flume Eventif unable to successfully parse message Remember priority put fac / sev into header update parsing position remember version string now parse timestamp (handle different varieties) no timestamp specified; use relay current time assume we skip past a space to get to the hostname rfc3164 timestamp rfc 5424 timestamp parse out hostname copy the host string to avoid holding the message string in memory if using a memory-based queue EventBuilder will do a copy of its own, so no defensive copy of the bodyParse date in RFC 5424 format. Uses an LRU cache to speed up parsing formultiple messages that occur in the same second.Typical (for Java) milliseconds since UNIX epoch look for the optional fractional seconds figure out how many numeric digits FIXME: TODO: ensure we handle all bad formatting cases if they had a valid fractional second, append it rounded to millis look for timezone UTCParse the RFC3164 date format. This is trickier than it sounds because thisformat does not specify a year so we get weird edge cases at yearboundaries. This implementation tries to "do what I mean".RFC3164-compatible timestamp to be parsedTypical (for Java) milliseconds since the UNIX epoch rfc3164 dates are really dumb.Some code to try and add some smarts to the year insertion as without a year in the messagewe need to make some educated guessing.First set the "fixed" to be the timestamp with the current year.If the "fixed" time is more than one month in the future then roll it back a year.If the "fixed" time is more than eleven months in the past then roll it forward a year.This gives us a 12 month rolling window (11 months in the past, 1 month in the future) oftimestamps. flume clock is ahead or there is some latency, and the year rolled flume clock is behind and the year rolled/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/SyslogSourceConfigurationConstants.javaSyslogSourceConfigurationConstantsSyslogSourceConfigurationConstants()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/SyslogSourceConfigurationConstants.class"ports""format."format."search""replace""dateFormat"dateFormat"numProcessors""eventSize"eventSize"charset.default"charset.default"charset.port."charset.port."portHeader""readBufferBytes"readBufferBytes"keepFields""priority""version""clientIPHeader""clientHostnameHeader"List of ports to listen to.Number of processors used to calculate number of threads to spawn.Maximum allowable size of events. Disable explicit creation of objects./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/SyslogTcpSource.javagetSourceCountergetSourceCounter()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/SyslogTcpSource.classgetBoundAddressgetBoundAddress()serverChannelworkerGroupbossGroupformaterPropClass<SyslogTcpSource>SyslogTcpHandlerSyslogTcpHandler(int,java.util.Set,java.util.Map,java.lang.String,java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/SyslogTcpSource$SyslogTcpHandler.classsyslogUtilsSimpleChannelInboundHandler<ByteBuf>formatterPropbuff"Parsed partial event, event will be generated when " +
              "rest of the event is received."Parsed partial event, event will be generated when rest of the event is received."Error writting to channel, event dropped"Error writting to channel, event dropped"Error parsing event from syslog stream, event dropped"Error parsing event from syslog stream, event dropped/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/SyslogTcpSource$1.classChannelInitializer<SocketChannel>/Users/burakyetistiren/.m2/repository/io/netty/netty-transport/4.1.86.Final/netty-transport-4.1.86.Final.jar/io/netty/channel/ChannelInitializer.classhandlerRemovedhandlerRemoved(io.netty.channel.ChannelHandlerContext)handlerAddedhandlerAdded(io.netty.channel.ChannelHandlerContext)exceptionCaughtexceptionCaught(io.netty.channel.ChannelHandlerContext,java.lang.Throwable)channelRegisteredchannelRegistered(io.netty.channel.ChannelHandlerContext)initChannelinitChannel(io.netty.channel.Channel)initChannel(io.netty.channel.socket.SocketChannel)ChannelInitializerChannelInitializer()ChannelInitializer<SocketChannel>()AbstractBootstrap<ServerBootstrap,ServerChannel>? extends ServerChannelChannelFactory<? extends ServerChannel>AbstractBootstrapConfig<ServerBootstrap,ServerChannel>Class<? extends ServerChannel>AbstractBootstrap<ServerBootstrap,ServerChannel>(io.netty.bootstrap.AbstractBootstrap)AbstractBootstrap<ServerBootstrap,ServerChannel>()ChannelOption<Integer>AbstractConstant<ChannelOption<Integer>>Constant<ChannelOption<Integer>>Comparable<ChannelOption<Integer>>Class<NioServerSocketChannel>ChannelHandler[]"Syslog TCP Source starting..."Syslog TCP Source starting..."Unable to start Syslog TCP Source"Unable to start Syslog TCP Source"Syslog TCP Source stopping..."Syslog TCP Source stopping..."Metrics: {}"Metrics: {}"Not bound to an internet address"Not bound to an internet addressuse {@link MultiportSyslogTCPSource} instead. Start the server. TODO Auto-generated catch block/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/SyslogUDPSource.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/SyslogUDPSource.classmaxsizeClass<SyslogUDPSource>"Syslog UDP Source stopping..."Syslog UDP Source stopping...SyslogUdpHandlerSyslogUdpHandler(java.util.Map,java.util.Set,java.lang.String,java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/SyslogUDPSource$SyslogUdpHandler.classformatProps"Error writting to channel"Error writting to channel Default Min size 64k is max allowable in RFC 5426/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/SyslogUtils.javaresetreset()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/SyslogUtils.classformatHeadersformatHeaders()buildEventbuildEvent()initHeaderFormatsinitHeaderFormats()DEFAULT_FIELDS_TO_KEEPmsgBodytimeStampArrayList<SyslogFormatter>AbstractList<SyslogFormatter>AbstractCollection<SyslogFormatter>Collection<SyslogFormatter>Iterable<SyslogFormatter>List<SyslogFormatter>SequencedCollection<SyslogFormatter>isIncompleteEventisBadEventisUdpprioArrayList<SyslogFormatter>()Spliterator<SyslogFormatter>? super SyslogFormatterConsumer<? super SyslogFormatter>Iterator<SyslogFormatter>Stream<SyslogFormatter>BaseStream<SyslogFormatter,Stream<SyslogFormatter>>Predicate<? super SyslogFormatter>? extends SyslogFormatterCollection<? extends SyslogFormatter>Iterable<? extends SyslogFormatter>add(org.apache.flume.source.SyslogUtils.SyslogFormatter)AbstractCollection<SyslogFormatter>()addLast(org.apache.flume.source.SyslogUtils.SyslogFormatter)addFirst(org.apache.flume.source.SyslogUtils.SyslogFormatter)ListIterator<SyslogFormatter>add(int,org.apache.flume.source.SyslogUtils.SyslogFormatter)set(int,org.apache.flume.source.SyslogUtils.SyslogFormatter)Comparator<? super SyslogFormatter>UnaryOperator<SyslogFormatter>Function<SyslogFormatter,SyslogFormatter>AbstractList<SyslogFormatter>()ArrayList<SyslogFormatter>(java.util.Collection)ArrayList<SyslogFormatter>(int)"yyyy-MM-dd'T'HH:mm:ss.SZ"yyyy-MM-dd'T'HH:mm:ss.SZ"yyyy-MM-dd'T'HH:mm:ss.S"yyyy-MM-dd'T'HH:mm:ss.S"yyyy-MM-dd'T'HH:mm:ssZ"yyyy-MM-dd'T'HH:mm:ssZ"yyyyMMM d HH:mm:ss"yyyyMMM d HH:mm:ss"(?:\\<(\\d{1,3})\\>)" + // priority
          "(?:(\\d?)\\s?)" + // version
          /* yyyy-MM-dd'T'HH:mm:ss.SZ or yyyy-MM-dd'T'HH:mm:ss.S+hh:mm or - (null stamp) */
          "(?:" +
          "(\\d{4}[-]\\d{2}[-]\\d{2}[T]\\d{2}[:]\\d{2}[:]\\d{2}" +
          "(?:\\.\\d{1,6})?(?:[+-]\\d{2}[:]\\d{2}|Z)?)|-)" + // stamp
          "\\s" + // separator
          "(?:([\\w][\\w\\d\\.@\\-]*)|-)" + // host name or - (null)
          "\\s" + // separator
          "(.*)$"(?:\<(\d{1,3})\>)(?:(\d?)\s?)(?:(\d{4}[-]\d{2}[-]\d{2}[T]\d{2}[:]\d{2}[:]\d{2}(?:\.\d{1,6})?(?:[+-]\d{2}[:]\d{2}|Z)?)|-)\s(?:([\w][\w\d\.@\-]*)|-)\s(.*)$"(?:\\<(\\d{1,3})\\>)" +
          "(?:(\\d)?\\s?)" + // version
          // stamp MMM d HH:mm:ss, single digit date has two spaces
          "([A-Z][a-z][a-z]\\s{1,2}\\d{1,2}\\s\\d{2}[:]\\d{2}[:]\\d{2})" +
          "\\s" + // separator
          "([\\w][\\w\\d\\.@-]*)" + // host
          "\\s(.*)$"(?:\<(\d{1,3})\>)(?:(\d)?\s?)([A-Z][a-z][a-z]\s{1,2}\d{1,2}\s\d{2}[:]\d{2}[:]\d{2})\s([\w][\w\d\.@-]*)\s(.*)$Class<SyslogUtils>"Facility"Facility"Severity"Severity"Priority"Priority"flume.syslog.status"flume.syslog.status2500"--all--"--all--SyslogFormatterSyslogFormatter()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/SyslogUtils$SyslogFormatter.classArrayList<SimpleDateFormat>AbstractList<SimpleDateFormat>AbstractCollection<SimpleDateFormat>List<SimpleDateFormat>SequencedCollection<SimpleDateFormat>ArrayList<SimpleDateFormat>()Spliterator<SimpleDateFormat>Consumer<? super SimpleDateFormat>Iterator<SimpleDateFormat>Stream<SimpleDateFormat>BaseStream<SimpleDateFormat,Stream<SimpleDateFormat>>Predicate<? super SimpleDateFormat>Collection<? extends SimpleDateFormat>Iterable<? extends SimpleDateFormat>add(java.text.SimpleDateFormat)AbstractCollection<SimpleDateFormat>()addLast(java.text.SimpleDateFormat)addFirst(java.text.SimpleDateFormat)ListIterator<SimpleDateFormat>add(int,java.text.SimpleDateFormat)set(int,java.text.SimpleDateFormat)Comparator<? super SimpleDateFormat>UnaryOperator<SimpleDateFormat>Function<SimpleDateFormat,SimpleDateFormat>AbstractList<SimpleDateFormat>()ArrayList<SimpleDateFormat>(java.util.Collection)ArrayList<SimpleDateFormat>(int)fieldsToKeep"true""all"allfield"<"">"socketAddress"The returned IP is null"The returned IP is null"Unable to retrieve client IP address"Unable to retrieve client IP address"The returned hostname is null"The returned hostname is null"Unable to retrieve client hostname"Unable to retrieve client hostnamedefaultSizeformatPropfmt1fmt2"Z""+0000"+0000"([+-])(\\d{2})[:](\\d{2})"([+-])(\d{2})[:](\d{2})"$1$2$3"$1$2$3"(T\\d{2}:\\d{2}:\\d{2}\\.\\d{3})(\\d*)"(T\d{2}:\d{2}:\d{2}\.\d{3})(\d*)"$1"$1ModeMode()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/SyslogUtils$Mode.classSyslogStatusSyslogStatus(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/SyslogUtils$SyslogStatus.classsyslogStatus"Unknown"Unknown"Invalid"Invalid"Incomplete"Incompletesev"Event created from Invalid Syslog data."Event created from Invalid Syslog data."Event size larger than specified event size: {}. You should " +
            "consider increasing your event size."Event size larger than specified event size: {}. You should consider increasing your event size.eventStrtimeStampStringfmtgrpspdtparsedDatecalParsedcalMinusOneMonthcalPlusElevenMonthscurrentTimeMillisc1"Delimiter found while in START mode, ignoring.."Delimiter found while in START mode, ignoring.. priority versionyyyy-MM-dd'T'HH:mm:ss.SZ or yyyy-MM-dd'T'HH:mm:ss.S+hh:mm or - (null stamp) stamp separator host name or - (null) body stamp MMM d HH:mm:ss, single digit date has two spaces host Prepend fields to be kept in message body. return a safe value instead of null extend the default header formatter setup built-in formats setup RFC5424 formater 'Z' in timestamp indicates UTC zone, so replace it it with '+0000' for date formatting timezone in RFC5424 is [+-]tt:tt, so remove the ':' for java date formatting FLUME-2497: SimpleDateFormat does not handle microseconds, Truncate after 3 digits. setup RFC3164 formater the single digit date has two spaces, so trim it create the event from syslog data Parse failed. format the message Apply each known pattern to message apply available format replacements to timestamp Add year to timestamp if needed try the available time formats to timestampSome code to try and add some smarts to the year insertion.Original code just added the current year which was okay-ish, but aroundJanuary 1st becomes pretty nave.The current year is added above. This code, if the year has been added doesthe following:1. Compute what the computed time, but one month in the past would be.2. Compute what the computed time, but eleven months in the future would be.If the computed time is more than one month in the future then roll it back ayear. If the computed time is more than eleven months in the past then roll itforward a year. This gives us a 12 month rolling window (11 months in the past,1 month in the future) of timestamps.Need to roll back a yearNeed to roll forward a year done. formatted the time Error formatting the timeStamp, try next format we successfully parsed the message using this pattern extract relevant syslog data needed for building Flume eventfor protocol debuggingByteBuffer bb = in.toByteBuffer();int remaining = bb.remaining();byte[] buf = new byte[remaining];bb.get(buf);HexDump.dump(buf, 0, System.out, 0);If the character is \n, it was because the last event was exactlyas long  as the maximum size allowed andthe only remaining character was the delimiter - '\n', ormultiple delimiters were sent in a row.Just ignore it, and move forward, don't change the mode.This is a no-op, just ignore it.Bad event, just dump everything as if it is data. Priority is max 3 digits per both RFC 3164 and 5424 With this check there is basically no danger of boas.size() exceeding this.maxSize before getting to the DATA state where this is actually checkedIf we hit a bad priority, just write as if everything is data. TCP syslog entries are separated by '\n' UDP doesn't send a newline, so just use what we received/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/ThriftSource.javagetSASLTransportFactorygetSASLTransportFactory()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/ThriftSource.classorg.apache.thrift.serverAbstractServerArgs<>/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/Users/burakyetistiren/.m2/repository/org/apache/thrift/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/apache/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/apache/thrift/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/apache/thrift/server/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/apache/thrift/server/TServer$AbstractServerArgs.classpopulateServerParamspopulateServerParams(org.apache.thrift.server.TServer.AbstractServerArgs)getTThreadPoolServergetTThreadPoolServer()getTThreadedSelectorServergetTThreadedSelectorServer()getProtocolFactorygetProtocolFactory()getTServerTransportgetTServerTransport()getSSLServerTransportgetSSLServerTransport()flumeAuthservingExecutorAGENT_KEYTABAGENT_PRINCIPALKERBEROS_KEYClass<ThriftSource>"protocol""binary"binary"compact"compact"agent-principal"agent-principal"agent-keytab"agent-keytab"Configuring thrift source."Configuring thrift source."Port must be specified for Thrift " +
        "Source."Port must be specified for Thrift Source."Bind address must be specified " +
        "for Thrift Source."Bind address must be specified for Thrift Source."Thrift source\'s \"threads\" property must specify an " +
                  "integer value: "Thrift source's "threads" property must specify an integer value: "binary or compact are the only valid Thrift protocol types to " +
                "choose from."binary or compact are the only valid Thrift protocol types to choose from."Authentication failed in Kerberos mode for " +
                "principal "timeAfterStart"Starting thrift source"Starting thrift source"Flume Thrift Source I/O Boss"Flume Thrift Source I/O Boss/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/ThriftSource$1.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/ThriftSource$1$1.classPrivilegedAction<Object>"Thrift server failed to start!"Thrift server failed to start!"Interrupted while waiting for Thrift server" +
            " to start."Interrupted while waiting for Thrift server to start."Started Thrift source."Started Thrift source.serverSock120000sslServerSock"Cannot start Thrift source."Cannot start Thrift source."Using TBinaryProtocol"Using TBinaryProtocol"Using TCompactProtocol"Using TCompactProtocolserverClassargsClassserverTransportsourceServicethreadFactory"Flume Thrift IPC Thread %d"Flume Thrift IPC Thread %dClass<ExecutorService>"executorService"executorService"org.apache.thrift" +
              ".server.TThreadedSelectorServer"org.apache.thrift.server.TThreadedSelectorServer"org.apache.thrift" +
              ".server.TThreadedSelectorServer$Args"org.apache.thrift.server.TThreadedSelectorServer$ArgsAbstractNonblockingServerArgs<>/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/apache/thrift/server/AbstractNonblockingServer$AbstractNonblockingServerArgs.classTypeVariable<Constructor<?>>TypeVariable<Constructor<?>>[]Constructor<?>(java.lang.Class,java.lang.Class[],java.lang.Class[],int,int,java.lang.String,byte[],byte[])Class<TNonblockingServerTransport>"Cannot start Thrift Source."Cannot start Thrift Source.serverArgsprotocolFactoryprotocolFactory(org.apache.thrift.protocol.TProtocolFactory)outputProtocolFactoryoutputProtocolFactory(org.apache.thrift.protocol.TProtocolFactory)inputProtocolFactoryinputProtocolFactory(org.apache.thrift.protocol.TProtocolFactory)outputTransportFactoryoutputTransportFactory(org.apache.thrift.transport.TTransportFactory)inputTransportFactoryinputTransportFactory(org.apache.thrift.transport.TTransportFactory)transportFactorytransportFactory(org.apache.thrift.transport.TTransportFactory)processor(org.apache.thrift.TProcessor)processorFactoryprocessorFactory(org.apache.thrift.TProcessorFactory)AbstractServerArgsAbstractServerArgs(org.apache.thrift.transport.TServerTransport)AbstractServerArgs<>(org.apache.thrift.transport.TServerTransport)org.apache.flume.thriftProcessor<ThriftSourceHandler>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$Processor.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thriftTBaseProcessor<ThriftSourceHandler>/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/apache/thrift/TBaseProcessor.classProcessor<ThriftSourceHandler>(org.apache.flume.source.ThriftSource.ThriftSourceHandler)process(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)TBase<>/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/apache/thrift/TBase.class? extends TBase<>ProcessFunction<I,? extends TBase<>>/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/apache/thrift/ProcessFunction.classMap<String,ProcessFunction<I,? extends TBase<>>>getProcessMapViewgetProcessMapView()ProcessFunction<ThriftSourceHandler,? extends TBase<>>Map<String,ProcessFunction<ThriftSourceHandler,? extends TBase<>>>TBaseProcessorTBaseProcessor(java.lang.Object,java.util.Map)TBaseProcessor<ThriftSourceHandler>(org.apache.flume.source.ThriftSource.ThriftSourceHandler,java.util.Map)ProcessorProcessor(org.apache.flume.thrift.ThriftSourceProtocol.Iface,java.util.Map)Processor<ThriftSourceHandler>(org.apache.flume.source.ThriftSource.ThriftSourceHandler,java.util.Map)Processor(org.apache.flume.thrift.ThriftSourceProtocol.Iface)saslTransportFactory"Interrupted while waiting for server to be " +
          "shutdown."Interrupted while waiting for server to be shutdown.ThriftSourceHandlerThriftSourceHandler()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/ThriftSource$ThriftSourceHandler.class"Thrift source "Thrift source " could not append events " +
                    "to the channel." could not append events to the channel.List<ThriftFlumeEvent>SequencedCollection<ThriftFlumeEvent>Collection<ThriftFlumeEvent>Iterable<ThriftFlumeEvent>Spliterator<ThriftFlumeEvent>? super ThriftFlumeEventConsumer<? super ThriftFlumeEvent>Iterator<ThriftFlumeEvent>Stream<ThriftFlumeEvent>BaseStream<ThriftFlumeEvent,Stream<ThriftFlumeEvent>>Predicate<? super ThriftFlumeEvent>? extends ThriftFlumeEventCollection<? extends ThriftFlumeEvent>Iterable<? extends ThriftFlumeEvent>add(org.apache.flume.thrift.ThriftFlumeEvent)addLast(org.apache.flume.thrift.ThriftFlumeEvent)addFirst(org.apache.flume.thrift.ThriftFlumeEvent)ListIterator<ThriftFlumeEvent>add(int,org.apache.flume.thrift.ThriftFlumeEvent)set(int,org.apache.flume.thrift.ThriftFlumeEvent)Comparator<? super ThriftFlumeEvent>UnaryOperator<ThriftFlumeEvent>Function<ThriftFlumeEvent,ThriftFlumeEvent>"Thrift source %s could not append events to the channel."Thrift source %s could not append events to the channel.Config param for the maximum number of threads this source should use tohandle incoming data.Config param for the hostname to listen on.Config param for the port to listen on.Config param for the thrift protocol to use. default is to use the compact protocol. create the server if in ssl mode or if SelectorServer is unavailableStart serving.Both THsHaServer and TThreadedSelectorServer allows us to pass inthe executor service to use - unfortunately the "executorService"method does not exist in the parent abstract Args class,so use reflection to pass the executor in.populate the ProtocolFactorypopulate the transportFactory populate the  Processor/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/http/BLOBHandler.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/httporg.apache.flume.source.httpmandatoryHeaders/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/http/BLOBHandler.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/httpcommaSeparatedHeadersClass<BLOBHandler>"mandatoryParameters"mandatoryParametersMap<String,String[]>parameter? super String[]? extends String[]BiFunction<? super String[],? super String[],? extends String[]>merge(java.lang.String,java.lang.String[],java.util.function.BiFunction)BiFunction<? super String,? super String[],? extends String[]>Function<? super String,? extends String[]>replace(java.lang.String,java.lang.String[])replace(java.lang.String,java.lang.String[],java.lang.String[])putIfAbsent(java.lang.String,java.lang.String[])BiConsumer<? super String,? super String[]>getOrDefault(java.lang.Object,java.lang.String[])Entry<String,String[]>Set<Entry<String,String[]>>Collection<Entry<String,String[]>>Iterable<Entry<String,String[]>>Collection<String[]>Iterable<String[]>Map<? extends String,? extends String[]>put(java.lang.String,java.lang.String[])"Setting Header [Key, Value] as [{},{}] "Setting Header [Key, Value] as [{},{}] "Please specify "Please specify " parameter in the request." parameter in the request."Building an Event with stream of size -- {}"Building an Event with stream of size -- {}BLOBHandler for HTTPSource that accepts any binary stream of data as event.{@inheritDoc}/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/http/HTTPBadRequestException.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/http/HTTPBadRequestException.class3540764742069390951L3540764742069390951Exception thrown by an HTTP Handler if the request was not parsed correctlyinto an event because the request was not in the expected format./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/http/HTTPSource.javahandleDeprecatedParameterhandleDeprecatedParameter(org.apache.flume.Context,java.lang.String,java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/http/HTTPSource.classsourceContextClass<HTTPSource>handlerClassName? extends HTTPSourceHandlerClass<? extends HTTPSourceHandler>subProps"HTTPSource hostname specified is empty"HTTPSource hostname specified is empty"HTTPSource requires a port number to be"
          + " specified"HTTPSource requires a port number to be specifiedConstructor<? extends HTTPSourceHandler>TypeVariable<Constructor<? extends HTTPSourceHandler>>TypeVariable<Constructor<? extends HTTPSourceHandler>>[]Constructor<? extends HTTPSourceHandler>(java.lang.Class,java.lang.Class[],java.lang.Class[],int,int,java.lang.String,byte[],byte[])Map<String,? extends HTTPSourceHandler>HTTPSourceHandler[]TypeVariable<Class<? extends HTTPSourceHandler>>TypeVariable<Class<? extends HTTPSourceHandler>>[]"Error while configuring HTTPSource. Exception follows."Error while configuring HTTPSource. Exception follows."Deserializer is not an instance of HTTPSourceHandler."
              + "Deserializer must implement HTTPSourceHandler."Deserializer is not an instance of HTTPSourceHandler.Deserializer must implement HTTPSourceHandler."Error configuring HTTPSource!"Error configuring HTTPSource!threadPoolmbContainerOptional<ServerConnector>orElse(org.eclipse.jetty.server.ServerConnector)? extends ServerConnectorSupplier<? extends ServerConnector>Stream<ServerConnector>BaseStream<ServerConnector,Stream<ServerConnector>>Optional<? extends ServerConnector>? extends Optional<? extends ServerConnector>Supplier<? extends Optional<? extends ServerConnector>>? super ServerConnectorFunction<? super ServerConnector,? extends Optional<? extends U>>Function<? super ServerConnector,? extends U>Predicate<? super ServerConnector>Consumer<? super ServerConnector>Function<SSLContext,ServerConnector>Function<? super ServerConnector,? extends V>Function<V,ServerConnector>sslCtxFactory"https"https"Running HTTP Server found in source: "Running HTTP Server found in source: " before I started one."
            + "Will not attempt to start." before I started one.Will not attempt to start."QueuedThreadPool."QueuedThreadPool."Error while starting HTTPSource. Exception follows."Error while starting HTTPSource. Exception follows."Error while stopping HTTPSource. Exception follows."Error while stopping HTTPSource. Exception follows."Http source {} stopped. Metrics: {}"Http source {} stopped. Metrics: {}/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/http/HTTPSource$FlumeHTTPServlet.classFlumeHTTPServletFlumeHTTPServlet()4891924863218790344L4891924863218790344"Received bad request from client. "Received bad request from client. "Bad request from client. "Bad request from client. "Deserializer threw unexpected exception. "Deserializer threw unexpected exception. "Error appending event to channel. "
                + "Channel might be full. Consider increasing the channel "
                + "capacity or make sure the sinks perform faster."Error appending event to channel. Channel might be full. Consider increasing the channel capacity or make sure the sinks perform faster."Error appending event to channel. Channel might be full."Error appending event to channel. Channel might be full."Unexpected error appending event to channel. "Unexpected error appending event to channel. "Unexpected error while appending event to channel. "Unexpected error while appending event to channel. "enableSSL"enableSSL"excludeProtocols""keystorePassword"newParamoldParamA source which accepts Flume Events by HTTP POST and GET. GET should be usedfor experimentation only. HTTP requests are converted into flume events by apluggable "handler" which must implement the{@linkplain HTTPSourceHandler} interface. This handler takes a{@linkplain HttpServletRequest} and returns a list of flume events.The source accepts the following parameters: <p> <tt>port</tt>: port to whichthe server should bind. Mandatory <p> <tt>handler</tt>: the class thatdeserializes a HttpServletRequest into a list of flume events. This classmust implement HTTPSourceHandler. Default:{@linkplain JSONHandler}. <p> <tt>handler.*</tt> Any configurationto be passed to the handler. <p>All events deserialized from one Http request are committed to the channel inone transaction, thus allowing for increased efficiency on channels like thefile channel. If the handler throws an exception this source will returna HTTP status of 400. If the channel is full, or the source is unable toappend events to the channel, the source will return a HTTP 503 - Temporarilyunavailable status.A JSON handler which converts JSON objects to Flume events is provided.There are 2 ways of doing this:a. Have a static server instance and use connectors in each sourcewhich binds to the port defined for that source.b. Each source starts its own server instance, which binds to the source'sport.b is more efficient than a because Jetty does not allow binding aservlet to a connector. So each request will need to go through eacheach of the handlers/servlet till the correct one is found.Register with JMX for advanced monitoringcreate empty list/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/http/HTTPSourceConfigurationConstants.java"handler""0.0.0.0"0.0.0.0"org.apache.flume.source.http.JSONHandler"org.apache.flume.source.http.JSONHandler/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/http/HTTPSourceHandler.javaTakes an {@linkplain HttpServletRequest} and returns a list of FlumeEvents. If this request cannot be parsed into Flume events based on theformat this method will throw an exception. This method may also throw anexception if there is some sort of other error. <p>The request to be parsed into Flume events.List of Flume events generated from the request.HTTPBadRequestExceptionIf the was not parsed correctly into anevent because the request was not in the expected format.If there was an unexpected error./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/http/JSONHandler.javagetSimpleEventsgetSimpleEvents(java.util.List)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/http/JSONHandler.classlistType/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/http/JSONHandler$1.classList<JSONEvent>SequencedCollection<JSONEvent>Collection<JSONEvent>Iterable<JSONEvent>TypeToken<List<JSONEvent>>? super List<JSONEvent>Class<? super List<JSONEvent>>TypeToken<List<JSONEvent>>()Class<JSONHandler>"Charset is null, default charset of UTF-8 will be used."Charset is null, default charset of UTF-8 will be used."utf-16"utf-16"utf-32"utf-32"Unsupported character set in request {}. "
              + "JSON handler supports UTF-8, "
              + "UTF-16 and UTF-32 only."Unsupported character set in request {}. JSON handler supports UTF-8, UTF-16 and UTF-32 only."JSON handler supports UTF-8, "
              + "UTF-16 and UTF-32 only."JSON handler supports UTF-8, UTF-16 and UTF-32 only."Request has invalid JSON Syntax."Request has invalid JSON Syntax.newEventsJSONHandler for HTTPSource that accepts an array of events.This handler throws exception if the deserialization fails because of badformat or any other reason.Each event must be encoded as a map with two key-value pairs. <p> 1. headers- the key for this key-value pair is "headers". The value for this key isanother map, which represent the event headers. These headers are insertedinto the Flume event as is. <p> 2. body - The body is a string whichrepresents the body of the event. The key for this key-value pair is "body".All key-value pairs are considered to be headers. An example: <p> [{"headers": {"a":"b", "c":"d"},"body": "random_body"}, {"headers" : {"e": "f"},"body":"random_body2"}] <p> would be interpreted as the following two flume events:<p> * Event with body: "random_body" (in UTF-8/UTF-16/UTF-32 encoded bytes)and headers : (a:b, c:d) <p> *Event with body: "random_body2" (in UTF-8/UTF-16/UTF-32 encoded bytes) andheaders : (e:f) <p>The charset of the body is read from the request and used. If no charset isset in the request, then the charset is assumed to be JSON's default - UTF-8.The JSON handler supports UTF-8, UTF-16 and UTF-32.To set the charset, the request must have content type specified as"application/json; charset=UTF-8" (replace UTF-8 with UTF-16 or UTF-32 asrequired).One way to create an event in the format expected by this handler, is touse {@linkplain JSONEvent} and use {@linkplain Gson} to create the JSONstring using the{@linkplain Gson#toJson(java.lang.Object, java.lang.reflect.Type) }method. The type token to pass as the 2nd argument of this methodfor list of events can be created by: <p>Type type = new TypeToken<List<JSONEvent>>() {}.getType();UTF-8 is default for JSON. If no charset is specified, UTF-8 is tobe assumed.Gson throws Exception if the data is not parseable to JSON.Need not catch it since the source will catch it and return error./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/shaded/guava/RateLimiter.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/shaded/guava/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/shadedorg.apache.flume.source.shaded.guavacheckArgumentcheckNotNullMICROSECONDSSECONDScheckPermitscheckPermits(int)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/shaded/guava/RateLimiter.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/shaded/guava/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/shadedreserveEarliestAvailablereserveEarliestAvailable(int,long)queryEarliestAvailablequeryEarliestAvailable(long)reserveAndGetWaitLengthreserveAndGetWaitLength(int,long)canAcquirecanAcquire(long,long)reservereserve(int)doGetRatedoGetRate()doSetRatedoSetRate(double,long)RateLimiterRateLimiter(org.apache.flume.source.shaded.guava.RateLimiter.SleepingStopwatch)mutexmutex()mutexDoNotUseDirectlystopwatchcreate(org.apache.flume.source.shaded.guava.RateLimiter.SleepingStopwatch,double,long,java.util.concurrent.TimeUnit)create(org.apache.flume.source.shaded.guava.RateLimiter.SleepingStopwatch,double)permitsPerSecondrateLimiter1.0warmupPeriod"warmupPeriod must not be negative: %s"warmupPeriod must not be negative: %s0.0"rate must be positive"rate must be positivenowMicrosmicrosToWaittimeoutMicrosmomentAvailable"RateLimiter[stableRate=%3.1fqps]"RateLimiter[stableRate=%3.1fqps]createFromSystemTimercreateFromSystemTimer()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/shaded/guava/RateLimiter$SleepingStopwatch.classsleepMicrosUninterruptiblysleepMicrosUninterruptibly(long)readMicrosreadMicros()SleepingStopwatchSleepingStopwatch()micros/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/shaded/guava/RateLimiter$SleepingStopwatch$1.class"Requested permits (%s) must be positive"Requested permits (%s) must be positiveCopyright (C) 2012 The Guava AuthorsLicensed under the Apache License, Version 2.0 (the "License");you may not use this file except in compliance with the License.A rate limiter. Conceptually, a rate limiter distributes permits at aconfigurable rate. Each {@link #acquire()} blocks if necessary until a permit isavailable, and then takes it. Once acquired, permits need not be released.<p>Rate limiters are often used to restrict the rate at which somephysical or logical resource is accessed. This is in contrast to {@linkjava.util.concurrent.Semaphore} which restricts the number of concurrentaccesses instead of the rate (note though that concurrency and rate are closely related,e.g. see <a href="http://en.wikipedia.org/wiki/Little's_law">Little's Law</a>).<p>A {@code RateLimiter} is defined primarily by the rate at which permitsare issued. Absent additional configuration, permits will be distributed at afixed rate, defined in terms of permits per second. Permits will be distributedsmoothly, with the delay between individual permits being adjusted to ensurethat the configured rate is maintained.<p>It is possible to configure a {@code RateLimiter} to have a warmupperiod during which time the permits issued each second steadily increases untilit hits the stable rate.<p>As an example, imagine that we have a list of tasks to execute, but we don't want tosubmit more than 2 per second:<pre>  {@codefinal RateLimiter rateLimiter = RateLimiter.create(2.0); // rate is "2 permits per second"void submitTasks(List<Runnable> tasks, Executor executor) {for (Runnable task : tasks) {rateLimiter.acquire(); // may waitexecutor.execute(task);}</pre><p>As another example, imagine that we produce a stream of data, and we want to cap itat 5kb per second. This could be accomplished by requiring a permit per byte, and specifyinga rate of 5000 permits per second:final RateLimiter rateLimiter = RateLimiter.create(5000.0); // rate = 5000 permits per secondvoid submitPacket(byte[] packet) {rateLimiter.acquire(packet.length);networkService.send(packet);<p>It is important to note that the number of permits requested <i>never</i>affect the throttling of the request itself (an invocation to {@code acquire(1)}and an invocation to {@code acquire(1000)} will result in exactly the same throttling, if any),but it affects the throttling of the <i>next</i> request. I.e., if an expensive taskarrives at an idle RateLimiter, it will be granted immediately, but it is the <i>next</i>request that will experience extra throttling, thus paying for the cost of the expensivetask.<p>Note: {@code RateLimiter} does not provide fairness guarantees.@authorDimitris Andreou@since13.0 TODO(user): switch to nano precision. A natural unit of cost is "bytes", and a micro precision     would mean a maximum rate of "1MB/s", which might be small in some cases.Creates a {@code RateLimiter} with the specified stable throughput, given as"permits per second" (commonly referred to as <i>QPS</i>, queries per second).<p>The returned {@code RateLimiter} ensures that on average no more than {@codepermitsPerSecond} are issued during any given second, with sustained requestsbeing smoothly spread over each second. When the incoming request rate exceeds{@code permitsPerSecond} the rate limiter will release one permit every {@code(1.0 / permitsPerSecond)} seconds. When the rate limiter is unused,bursts of up to {@code permitsPerSecond} permits will be allowed, with subsequentrequests being smoothly limited at the stable rate of {@code permitsPerSecond}.the rate of the returned {@code RateLimiter}, measured inhow many permits become available per secondif {@code permitsPerSecond} is negative or zero TODO(user): "This is equivalent to                 {@code createWithCapacity(permitsPerSecond, 1, TimeUnit.SECONDS)}".The default RateLimiter configuration can save the unused permits of up to one second.This is to avoid unnecessary stalls in situations like this: A RateLimiter of 1qps,and 4 threads, all calling acquire() at these moments:T0 at 0 secondsT1 at 1.05 secondsT2 at 2 secondsT3 at 3 secondsDue to the slight delay of T1, T2 would have to sleep till 2.05 seconds,and T3 would also have to sleep till 3.05 seconds.TODO(cpovirk): make SleepingStopwatch the last parameter throughout the class so that theoverloads follow the usual convention: Foo(int), Foo(int, SleepingStopwatch)maxBurstSeconds"permits per second" (commonly referred to as <i>QPS</i>, queries per second), and a<i>warmup period</i>, during which the {@code RateLimiter} smoothly ramps up its rate,until it reaches its maximum rate at the end of the period (as long as there are enoughrequests to saturate it). Similarly, if the {@code RateLimiter} is left <i>unused</i> fora duration of {@code warmupPeriod}, it will gradually return to its "cold" state,i.e. it will go through the same warming up process as when it was first created.<p>The returned {@code RateLimiter} is intended for cases where the resource that actuallyfulfills the requests (e.g., a remote server) needs "warmup" time, rather thanbeing immediately accessed at the stable (maximum) rate.<p>The returned {@code RateLimiter} starts in a "cold" state (i.e. the warmup periodwill follow), and if it is left unused for long enough, it will return to that state.the duration of the period where the {@code RateLimiter} ramps up itsrate, before reaching its stable (maximum) ratethe time unit of the warmupPeriod argumentif {@code permitsPerSecond} is negative or zero or{@code warmupPeriod} is negativeThe underlying timer; used both to measure elapsed time and sleep as necessary. A separateobject to facilitate testing. Can't be initialized in the constructor because mocks don't call the constructor.Updates the stable rate of this {@code RateLimiter}, that is, the{@code permitsPerSecond} argument provided in the factory method thatconstructed the {@code RateLimiter}. Currently throttled threads will <b>not</b>be awakened as a result of this invocation, thus they do not observe the new rate;only subsequent requests will.<p>Note though that, since each request repays (by waiting, if necessary) the costof the <i>previous</i> request, this means that the very next requestafter an invocation to {@code setRate} will not be affected by the new rate;it will pay the cost of the previous request, which is in terms of the previous rate.<p>The behavior of the {@code RateLimiter} is not modified in any other way,e.g. if the {@code RateLimiter} was configured with a warmup period of 20 seconds,it still has a warmup period of 20 seconds after this method invocation.the new stable rate of this {@code RateLimiter}Returns the stable rate (as {@code permits per seconds}) with which this{@code RateLimiter} is configured with. The initial value of this is the same asthe {@code permitsPerSecond} argument passed in the factory method that producedthis {@code RateLimiter}, and it is only updated after invocationsto {@linkplain #setRate}.Acquires a single permit from this {@code RateLimiter}, blocking until therequest can be granted. Tells the amount of time slept, if any.<p>This method is equivalent to {@code acquire(1)}.time spent sleeping to enforce rate, in seconds; 0.0 if not rate-limited16.0 (present in 13.0 with {@code void} return type})Acquires the given number of permits from this {@code RateLimiter}, blocking until thethe number of permits to acquireif the requested number of permits is negative or zeroReserves the given number of permits from this {@code RateLimiter} for future use, returningthe number of microseconds until the reservation can be consumed.time in microseconds to wait until the resource can be acquired, never negativeAcquires a permit from this {@code RateLimiter} if it can be obtainedwithout exceeding the specified {@code timeout}, or returns {@code false}immediately (without waiting) if the permit would not have been grantedbefore the timeout expired.<p>This method is equivalent to {@code tryAcquire(1, timeout, unit)}.the maximum time to wait for the permit. Negative values are treated as zero.the time unit of the timeout argument{@code true} if the permit was acquired, {@code false} otherwiseAcquires permits from this {@link RateLimiter} if it can be acquired immediately without delay.This method is equivalent to {@code tryAcquire(permits, 0, anyUnit)}.{@code true} if the permits were acquired, {@code false} otherwise14.0Acquires a permit from this {@link RateLimiter} if it can be acquired immediately withoutdelay.This method is equivalent to {@code tryAcquire(1)}.Acquires the given number of permits from this {@code RateLimiter} if it can be obtainedimmediately (without waiting) if the permits would not have been grantedthe maximum time to wait for the permits. Negative values are treated as zero.Reserves next ticket and returns the wait time that the caller must wait for.the required wait time, never negativeReturns the earliest time that permits are available (with one caveat).the time that permits are available, or, if permits are available immediately, anarbitrary past or present timeReserves the requested number of permits and returns the time that those permits can be used(with one caveat).the time that the permits may be used, or, if the permits may be used immediately, anWe always hold the mutex when calling this. TODO(cpovirk): Is that important? Perhaps we needto guarantee that each call to reserveEarliestAvailable, etc. sees a value >= the previous?Also, is it OK that we don't hold the mutex when sleeping?/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/shaded/guava/SmoothRateLimiter.javaresyncresync(long)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/shaded/guava/SmoothRateLimiter.classstoredPermitsToWaitTimestoredPermitsToWaitTime(double,double)doSetRate(double,double)SmoothRateLimiterSmoothRateLimiter(org.apache.flume.source.shaded.guava.RateLimiter.SleepingStopwatch)nextFreeTicketMicrosstableIntervalMicrosmaxPermitsstoredPermitspermitsToTimepermitsToTime(double)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/shaded/guava/SmoothRateLimiter$SmoothWarmingUp.classSmoothWarmingUpSmoothWarmingUp(org.apache.flume.source.shaded.guava.RateLimiter.SleepingStopwatch,long,java.util.concurrent.TimeUnit)halfPermitsslopewarmupPeriodMicrostimeUnitoldMaxPermitscoldIntervalMicros3.02.0permitsToTakeavailablePermitsAboveHalfpermitsAboveHalfToTake/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/shaded/guava/SmoothRateLimiter$SmoothBursty.classSmoothBurstySmoothBursty(org.apache.flume.source.shaded.guava.RateLimiter.SleepingStopwatch,double)requiredPermitsreturnValuestoredPermitsToSpendfreshPermitswaitMicrosHow is the RateLimiter designed, and why?The primary feature of a RateLimiter is its "stable rate", the maximum rate thatis should allow at normal conditions. This is enforced by "throttling" incomingrequests as needed, i.e. compute, for an incoming request, the appropriate throttle time,and make the calling thread wait as much.The simplest way to maintain a rate of QPS is to keep the timestamp of the lastgranted request, and ensure that (1/QPS) seconds have elapsed since then. For example,for a rate of QPS=5 (5 tokens per second), if we ensure that a request isn't grantedearlier than 200ms after the last one, then we achieve the intended rate.If a request comes and the last request was granted only 100ms ago, then we wait foranother 100ms. At this rate, serving 15 fresh permits (i.e. for an acquire(15) request)naturally takes 3 seconds.It is important to realize that such a RateLimiter has a very superficial memoryof the past: it only remembers the last request. What if the RateLimiter was unused fora long period of time, then a request arrived and was immediately granted?This RateLimiter would immediately forget about that past underutilization. This mayresult in either underutilization or overflow, depending on the real world consequencesof not using the expected rate.Past underutilization could mean that excess resources are available. Then, the RateLimitershould speed up for a while, to take advantage of these resources. This is importantwhen the rate is applied to networking (limiting bandwidth), where past underutilizationtypically translates to "almost empty buffers", which can be filled immediately.On the other hand, past underutilization could mean that "the server responsible forhandling the request has become less ready for future requests", i.e. its caches becomestale, and requests become more likely to trigger expensive operations (a more extremecase of this example is when a server has just booted, and it is mostly busy with gettingitself up to speed).To deal with such scenarios, we add an extra dimension, that of "past underutilization",modeled by "storedPermits" variable. This variable is zero when there is nounderutilization, and it can grow up to maxStoredPermits, for sufficiently largeunderutilization. So, the requested permits, by an invocation acquire(permits),are served from:- stored permits (if available)- fresh permits (for any remaining permits)How this works is best explained with an example:For a RateLimiter that produces 1 token per second, every secondthat goes by with the RateLimiter being unused, we increase storedPermits by 1.Say we leave the RateLimiter unused for 10 seconds (i.e., we expected a request at timeX, but we are at time X + 10 seconds before a request actually arrives; this isalso related to the point made in the last paragraph), thus storedPermitsbecomes 10.0 (assuming maxStoredPermits >= 10.0). At that point, a request of acquire(3)arrives. We serve this request out of storedPermits, and reduce that to 7.0 (how this istranslated to throttling time is discussed later). Immediately after, assume that anacquire(10) request arriving. We serve the request partly from storedPermits,using all the remaining 7.0 permits, and the remaining 3.0, we serve them by fresh permitsproduced by the rate limiter.We already know how much time it takes to serve 3 fresh permits: if the rate is"1 token per second", then this will take 3 seconds. But what does it mean to serve 7stored permits? As explained above, there is no unique answer. If we are primarilyinterested to deal with underutilization, then we want stored permits to be given out/faster/ than fresh ones, because underutilization = free resources for the taking.If we are primarily interested to deal with overflow, then stored permits couldbe given out /slower/ than fresh ones. Thus, we require a (different in each case)function that translates storedPermits to throtting time.This role is played by storedPermitsToWaitTime(double storedPermits, double permitsToTake).The underlying model is a continuous function mapping storedPermits(from 0.0 to maxStoredPermits) onto the 1/rate (i.e. intervals) that is effective at the givenstoredPermits. "storedPermits" essentially measure unused time; we spend unused timebuying/storing permits. Rate is "permits / time", thus "1 / rate = time / permits".Thus, "1/rate" (time / permits) times "permits" gives time, i.e., integrals on thisfunction (which is what storedPermitsToWaitTime() computes) correspond to minimum intervalsbetween subsequent requests, for the specified number of requested permits.Here is an example of storedPermitsToWaitTime:If storedPermits == 10.0, and we want 3 permits, we take them from storedPermits,reducing them to 7.0, and compute the throttling for these as a call tostoredPermitsToWaitTime(storedPermits = 10.0, permitsToTake = 3.0), which willevaluate the integral of the function from 7.0 to 10.0.Using integrals guarantees that the effect of a single acquire(3) is equivalentto { acquire(1); acquire(1); acquire(1); }, or { acquire(2); acquire(1); }, etc,since the integral of the function in [7.0, 10.0] is equivalent to the sum of theintegrals of [7.0, 8.0], [8.0, 9.0], [9.0, 10.0] (and so on), no matterwhat the function is. This guarantees that we handle correctly requests of varying weight(permits), /no matter/ what the actual function is - so we can tweak the latter freely.(The only requirement, obviously, is that we can compute its integrals).Note well that if, for this function, we chose a horizontal line, at height of exactly(1/QPS), then the effect of the function is non-existent: we serve storedPermits atexactly the same cost as fresh ones (1/QPS is the cost for each). We use this trick later.If we pick a function that goes /below/ that horizontal line, it means that we reducethe area of the function, thus time. Thus, the RateLimiter becomes /faster/ after aperiod of underutilization. If, on the other hand, we pick a function thatgoes /above/ that horizontal line, then it means that the area (time) is increased,thus storedPermits are more costly than fresh permits, thus the RateLimiter becomes/slower/ after a period of underutilization.Last, but not least: consider a RateLimiter with rate of 1 permit per second, currentlycompletely unused, and an expensive acquire(100) request comes. It would be nonsensicalto just wait for 100 seconds, and /then/ start the actual task. Why wait without doinganything? A much better approach is to /allow/ the request right away (as if it was anacquire(1) request instead), and postpone /subsequent/ requests as needed. In this version,we allow starting the task immediately, and postpone by 100 seconds future requests,thus we allow for work to get done in the meantime instead of waiting idly.This has important consequences: it means that the RateLimiter doesn't remember the timeof the _last_ request, but it remembers the (expected) time of the _next_ request. Thisalso enables us to tell immediately (see tryAcquire(timeout)) whether a particulartimeout is enough to get us to the point of the next scheduling time, since we alwaysmaintain that. And what we mean by "an unused RateLimiter" is also defined by thatnotion: when we observe that the "expected arrival time of the next request" is actuallyin the past, then the difference (now - past) is the amount of time that the RateLimiterwas formally unused, and it is that amount of time which we translate to storedPermits.(We increase storedPermits with the amount of permits that would have been producedin that idle time). So, if rate == 1 permit per second, and arrivals come exactlyone second after the previous, then storedPermits is _never_ increased -- we would onlyincrease it for arrivals _later_ than the expected one second.This implements the following function:^ throttling3*stable +                  /interval |                 /.(cold)  |                / .|               /  .   <-- "warmup period" is the area of the trapezoid between2*stable +              /   .       halfPermits and maxPermitsinterval |             /    .|            /     .|           /      .stable +----------/  WARM . }interval |          .   UP  . } <-- this rectangle (from 0 to maxPermits, and|          . PERIOD. }     height == stableInterval) defines the cooldown period,|          .       . }     and we want cooldownPeriod == warmupPeriod|---------------------------------> storedPermits(halfPermits) (maxPermits)Before going into the details of this particular function, let's keep in mind the basics:1) The state of the RateLimiter (storedPermits) is a vertical line in this figure.2) When the RateLimiter is not used, this goes right (up to maxPermits)3) When the RateLimiter is used, this goes left (down to zero), since if we have storedPermits,we serve from those first4) When _unused_, we go right at the same speed (rate)! I.e., if our rate is2 permits per second, and 3 unused seconds pass, we will always save 6 permits(no matter what our initial position was), up to maxPermits.If we invert the rate, we get the "stableInterval" (interval between two requestsin a perfectly spaced out sequence of requests of the given rate). Thus, if youwant to see "how much time it will take to go from X storedPermits to X+K storedPermits?",the answer is always stableInterval * K. In the same example, for 2 permits per second,stableInterval is 500ms. Thus to go from X storedPermits to X+6 storedPermits, werequire 6 * 500ms = 3 seconds.In short, the time it takes to move to the right (save K permits) is equal to therectangle of width == K and height == stableInterval.4) When _used_, the time it takes, as explained in the introductory class note, isequal to the integral of our function, between X permits and X-K permits, assumingwe want to spend K saved permits.In summary, the time it takes to move to the left (spend K permits), is equal to thearea of the function of width == K.Let's dive into this function now:When we have storedPermits <= halfPermits (the left portion of the function), thenwe spend them at the exact same rate thatfresh permits would be generated anyway (that rate is 1/stableInterval). We sizethis area to be equal to _half_ the specified warmup period. Why we need this?And why half? We'll explain shortly below (after explaining the second part).Stored permits that are beyond halfPermits, are mapped to an ascending line, that goesfrom stableInterval to 3 * stableInterval. The average height for that part is2 * stableInterval, and is sized appropriately to have an area _equal_ to thespecified warmup period. Thus, by point (4) above, it takes "warmupPeriod" amount of timeto go from maxPermits to halfPermits.BUT, by point (3) above, it only takes "warmupPeriod / 2" amount of time to return backto maxPermits, from halfPermits! (Because the trapezoid has double the area of the rectangleof height stableInterval and equivalent width). We decided that the "cooldown period"time should be equivalent to "warmup period", thus a fully saturated RateLimiter(with zero stored permits, serving only fresh ones) can go to a fully unsaturated(with storedPermits == maxPermits) in the same amount of time it takes for a fullyunsaturated RateLimiter to return to the stableInterval -- which happens in halfPermits,since beyond that point, we use a horizontal line of "stableInterval" height, simulatingthe regular rate.Thus, we have figured all dimensions of this shape, to give all the desiredproperties:- the width is warmupPeriod / stableInterval, to make cooldownPeriod == warmupPeriod- the slope starts at the middle, and goes from stableInterval to 3*stableInterval soto have halfPermits being spend in double the usual time (half the rate), while theirrespective rate is steadily ramping upThe slope of the line from the stable interval (when permits == 0), to the cold interval(when permits == maxPermits) Stable interval is x, cold is 3x, so on average it's 2x. Double the time -> halve the rate if we don't special-case this, we would get storedPermits == NaN, below initial state is cold measuring the integral on the right part of the function (the climbing line) measuring the integral on the left part of the function (the horizontal line)This implements a "bursty" RateLimiter, where storedPermits are translated tozero throttling. The maximum number of permits that can be saved (when the RateLimiter isunused) is defined in terms of time, in this sense: if a RateLimiter is 2qps, and thistime is specified as 10 seconds, we can save up to 2 * 10 = 20 permits.The work (permits) of how many seconds can be saved up if this RateLimiter is unused? initial stateThe currently stored permits.The maximum number of stored permits.The interval between two unit requests, at our stable rate. E.g., a stable rate of 5 permitsper second has a stable interval of 200ms.The time when the next request (no matter its size) will be granted. After granting arequest, this is pushed further in the future. Large requests push this further than smallrequests. could be either in the past or futureTranslates a specified portion of our currently stored permits which we want tospend/acquire, into a throttling time. Conceptually, this evaluates the integralof the underlying function we use, for the range of[(storedPermits - permitsToTake), storedPermits].<p>This always holds: {@code 0 <= permitsToTake <= storedPermits} if nextFreeTicket is in the past, resync to now/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/shaded/guava/Stopwatch.javacheckStateDAYSHOURSMILLISECONDSMINUTESNANOSECONDSabbreviateabbreviate(java.util.concurrent.TimeUnit)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/shaded/guava/Stopwatch.classchooseUnitchooseUnit(long)elapsedNanoselapsedNanos()StopwatchStopwatch(com.google.common.base.Ticker)Stopwatch()startTickisRunning"ticker""This stopwatch is already running."This stopwatch is already running.tick"This stopwatch is already stopped."This stopwatch is already stopped.desiredUnit"String.format()"String.format()nanos"%.4g %s"%.4g %s"ns"ns"\u03bcs"s"s""min"Copyright (C) 2008 The Guava AuthorsAn object that measures elapsed time in nanoseconds. It is useful to measureelapsed time using this class instead of direct calls to {@linkSystem#nanoTime} for a few reasons:<li>An alternate time source can be substituted, for testing or performancereasons.<li>As documented by {@code nanoTime}, the value returned has no absolutemeaning, and can only be interpreted as relative to another timestampreturned by {@code nanoTime} at a different time. {@code Stopwatch} is amore effective abstraction because it exposes only these relative values,not the absolute ones.<p>Basic usage:Stopwatch stopwatch = Stopwatch.{@link #createStarted createStarted}();doSomething();stopwatch.{@link #stop stop}(); // optionallong millis = stopwatch.elapsed(MILLISECONDS);log.info("time: " + stopwatch); // formatted string like "12.3 ms"</pre><p>Stopwatch methods are not idempotent; it is an error to start or stop astopwatch that is already in the desired state.<p>When testing code that uses this class, use{@link #createUnstarted(Ticker)} or {@link #createStarted(Ticker)} tosupply a fake or mock ticker.<!-- TODO(kevinb): restore the "such as" --> This allows you tosimulate any valid behavior of the stopwatch.<p><b>Note:</b> This class is not thread-safe.Kevin Bourrillion10.0Creates (but does not start) a new stopwatch using {@link System#nanoTime}as its time source.15.0Creates (but does not start) a new stopwatch, using the specified timesource.Creates (and starts) a new stopwatch using {@link System#nanoTime}Creates (and starts) a new stopwatch, using the specified timeUse {@link Stopwatch#createUnstarted()} instead.Use {@link Stopwatch#createUnstarted(Ticker)} instead.Returns {@code true} if {@link #start()} has been called on this stopwatch,and {@link #stop()} has not been called since the last call to {@codestart()}.Starts the stopwatch.this {@code Stopwatch} instanceIllegalStateExceptionif the stopwatch is already running.Stops the stopwatch. Future reads will return the fixed duration that hadelapsed up to this point.if the stopwatch is already stopped.Sets the elapsed time for this stopwatch to zero,and places it in a stopped state.Returns the current elapsed time shown on this stopwatch, expressedin the desired time unit, with any fraction rounded down.<p>Note that the overhead of measurement can be more than a microsecond, soit is generally not useful to specify {@link TimeUnit#NANOSECONDS}precision here.14.0 (since 10.0 as {@code elapsedTime()})Returns a string representation of the current elapsed time. Too bad this functionality is not exposed as a regular method call s/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/source/shaded/guava/Uninterruptibles.javaUninterruptiblesUninterruptibles()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/source/shaded/guava/Uninterruptibles.classlatchremainingNanostoJoinBlockingQueue<>BlockingQueue<E>Queue<E>sleepForsemaphoreCopyright (C) 2011 The Guava AuthorsUtilities for treating interruptible operations as uninterruptible.In all cases, if a thread is interrupted during such a call, the callcontinues to block until the result is available or the timeout elapses,and only then re-interrupts the thread.Anthony Zana Implementation Note: As of 3-7-11, the logic for each blocking/timeout methods is identical, save for method being invoked.Invokes {@code latch.}{@link CountDownLatch#await() await()}uninterruptibly.Invokes{@code latch.}{@link CountDownLatch#await(long, TimeUnit)await(timeout, unit)} uninterruptibly. CountDownLatch treats negative timeouts just like zero.Invokes {@code toJoin.}{@link Thread#join() join()} uninterruptibly.{@code unit.}{@link TimeUnit#timedJoin(Thread, long)timedJoin(toJoin, timeout)} uninterruptibly. TimeUnit.timedJoin() treats negative timeouts just like zero.Invokes {@code future.}{@link Future#get() get()} uninterruptibly.To get uninterruptibility and remove checked exceptions, see{@link Futures#getUnchecked}.<p>If instead, you wish to treat {@link InterruptedException} uniformlywith other exceptions, see {@link Futures#get(Future, Class) Futures.get}or {@link Futures#makeChecked}.ExecutionExceptionif the computation threw an exceptionCancellationExceptionif the computation was cancelled{@code future.}{@link Future#get(long, TimeUnit) get(timeout, unit)}TimeoutExceptionif the wait timed out Future treats negative timeouts just like zero.Invokes {@code queue.}{@link BlockingQueue#take() take()} uninterruptibly.Invokes {@code queue.}{@link BlockingQueue#put(Object) put(element)}ClassCastExceptionif the class of the specified element preventsit from being added to the given queueif some property of the specified elementprevents it from being added to the given queue TODO(user): Support Sleeper somehow (wrapper or interface method)?Invokes {@code unit.}{@link TimeUnit#sleep(long) sleep(sleepFor)} TimeUnit.sleep() treats negative timeouts just like zero.Invokes {@code semaphore.}{@link Semaphore#tryAcquire(int, long, TimeUnit)tryAcquire(1, timeout, unit)} uninterruptibly.18.0tryAcquire(permits, timeout, unit)} uninterruptibly. Semaphore treats negative timeouts just like zero. TODO(user): Add support for waitUninterruptibly./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/tools/DirectMemoryUtils.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/toolsorg.apache.flume.toolsgetDefaultDirectMemorySizegetDefaultDirectMemorySize()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/tools/DirectMemoryUtils.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/toolsallocatedDEFAULT_SIZEMAX_DIRECT_MEMORY_PARAMClass<DirectMemoryUtils>"-XX:MaxDirectMemorySize="-XX:MaxDirectMemorySize=maxDirectMemoryallocatedCurrently"Size must be greater than zero"Size must be greater than zero"Direct Memory Allocation: " +
        " Allocation = "Direct Memory Allocation:  Allocation = ", Allocated = ", Allocated = ", MaxDirectMemorySize = ", MaxDirectMemorySize = ", Remaining = ", Remaining = "Error allocating "Error allocating ", you likely want" +
          " to increase ", you likely want to increase cleanerMethod? extends ByteBufferClass<? extends ByteBuffer>Map<String,? extends ByteBuffer>ByteBuffer[]Constructor<? extends ByteBuffer>TypeVariable<Class<? extends ByteBuffer>>TypeVariable<Class<? extends ByteBuffer>>[]"cleaner"cleanercleanMethodMap<String,? extends Object>Constructor<? extends Object>TypeVariable<Class<? extends Object>>TypeVariable<Class<? extends Object>>[]"clean"clean"buffer isn't direct!"buffer isn't direct!"Direct Memory Deallocation: " +
        ", Allocated = "Direct Memory Deallocation: , Allocated = RuntimemxBeanargumentsmultipliermemSizeretValue"k""m"1048576"g"g1073741824"[^\\d]"[^\d]VM"sun.misc.VM"sun.misc.VM"maxDirectMemory""Unable to get maxDirectMemory from VM: "Unable to get maxDirectMemory from VM: for the byte case. default according to VM.maxDirectMemory()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/tools/FlumeBeanConfigurator.javaconfigurablefieldNamefieldTypeClass<Boolean>Class<Short>Class<Float>Class<Integer>Class<Double>Class<Character>Class<Byte>Class<String[]>"Unable to configure component due to an unsupported type on field: "Unable to configure component due to an unsupported type on field: "Unable to configure component: "Unable to configure component: subPropertiesPrefixUtility class to enable runtime configuration of Java objects using providedFlume context objects (or equivalent). The methods use reflection to identifyFields on the configurable object and then looks for matching properties inthe provided properties bundle.Utility method that will set properties on a Java bean (<code>Object configurable</code>)based on the provided <code>properties</code> bundle.If there is a type issue, or an access problemthen a <code>ConfigurationException</code> will be thrown.Any properties must be modifiable via setter methods.Map&lt;String, String&gt;ConfigurationExceptionbased on the provided <code>Context</code>.N.B. This method will take the Flume Context and look for sub-properties named after theclass name of the <code>configurable</code> object.<code>subPropertiesPrefix</code> String.Object: Any properties must be modifiable via setter methods.org.apache.flume.Context;String/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/tools/GetJavaProperty.javapropA generic way for querying Java properties./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/tools/HTTPServerConstraintUtil.javaHTTPServerConstraintUtilHTTPServerConstraintUtil()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/tools/HTTPServerConstraintUtil.classcmtcmo"/*"/*ConstraintMapping[] Most of the code in this class is copied from HBASE-10473Utility class to define constraints on Jetty HTTP serversGenerate constraints for the Flume HTTP SourceConstraintSecurityHandler for use with Jetty servlet/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/tools/PlatformDetect.java"os.name"isWin"win"winUtilities for platform & operating system detectionDetects whether we are running under Microsoft Windows.true if and only if we are running on a Windows system./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/tools/TimestampRoundDownUtil.javaroundDownFieldroundDownField(long,int,int,java.util.TimeZone)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/tools/TimestampRoundDownUtil.classroundDownSeccal"RoundDownSec must be > 0 and <=60"RoundDownSec must be > 0 and <=60roundDownMins"RoundDown must be > 0 and <=60"RoundDown must be > 0 and <=60roundDownHours"RoundDown must be > 0 and <=24"RoundDown must be > 0 and <=24fieldValremainder"Timestamp must be positive"Timestamp must be positive- The time stamp to be rounded down.For parsing the <tt>timestamp</tt> the system default timezone will be used.- The <tt>timestamp</tt> is rounded down to the largestmultiple of <tt>roundDownSec</tt> secondsless than or equal to <tt>timestamp.</tt> Should be between 0 and 60.- Rounded down timestampTimestampRoundDownUtil#roundDownTimeStampSeconds(long, int, TimeZone)- The timezone to use for parsing the <tt>timestamp</tt>.- The <tt>timestamp</tt> is rounded down to thelargest multiple of <tt>roundDownMins</tt> minutes less thanor equal to <tt>timestamp.</tt> Should be between 0 and 60.TimestampRoundDownUtil#roundDownTimeStampMinutes(long, int, TimeZone)If <tt>null</tt> the system default will be used.largest multiple of <tt>roundDownHours</tt> hours less thanor equal to <tt>timestamp.</tt> Should be between 0 and 24.TimestampRoundDownUtil#roundDownTimeStampHours(long, int, TimeZone)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/main/java/org/apache/flume/tools/VersionInfo.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/tools/VersionInfo.classmyPackageClass<VersionAnnotation>Map<String,VersionAnnotation>VersionAnnotation[]Constructor<VersionAnnotation>? super VersionAnnotationClass<? super VersionAnnotation>TypeVariable<Class<VersionAnnotation>>TypeVariable<Class<VersionAnnotation>>[]" by " by " on " on " source checksum " source checksum "Flume "Flume "Source code repository: "
            + "https://git.apache.org/repos/asf/flume.git"Source code repository: https://git.apache.org/repos/asf/flume.git"Revision: "Revision: "Compiled by "Compiled by "From source with checksum "From source with checksum This class provides version info of Flume NGGet the meta-data for the Flume package.Get the Flume version.the Flume version string, eg. "1.1"Get the subversion revision number for the root directorythe revision number, eg. "100755"Get the branch on which this originated.The branch name, e.g. "trunk" or "branches/branch-1.1"The date that Flume was compiled.the compilation date in unix date formatThe user that compiled Flume.the username of the userGet the subversion URL for the root Flume directory.Get the checksum of the source files from which Flume wasbuilt.Returns the build version info which includes version,revision, user, date and source checksum/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/test/resources/log4j2.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/test/resources/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/src/testDEBUG/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/antrun/build-main.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/antrunmaven-antrun-main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/generated-sources/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/scripts/saveVersion.sh 1.11.0 /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/generated-sources/avro/org/apache/flume/serialization/TransferStateFileMeta.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/generated-sources/avro/org/apache/flume/serialization/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/generated-sources/avro/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/generated-sources/avro/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/generated-sources/avro/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/generated-sources/avro/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/generated-sourcesREADER$/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/serialization/TransferStateFileMeta.classWRITER$org.apache.avro.messageBinaryMessageDecoder<TransferStateFileMeta>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/message/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/message/BinaryMessageDecoder.classBaseDecoder<TransferStateFileMeta>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/message/MessageDecoder$BaseDecoder.classMessageDecoder<TransferStateFileMeta>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/message/MessageDecoder.classDECODERBinaryMessageEncoder<TransferStateFileMeta>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/message/BinaryMessageEncoder.classMessageEncoder<TransferStateFileMeta>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/message/MessageEncoder.classENCODERMODEL$8236404304129631718L8236404304129631718"{\"type\":\"record\",\"name\":\"TransferStateFileMeta\",\"namespace\":\"org.apache.flume.serialization\",\"fields\":[{\"name\":\"offset\",\"type\":\"long\"}]}"{"type":"record","name":"TransferStateFileMeta","namespace":"org.apache.flume.serialization","fields":[{"name":"offset","type":"long"}]}BinaryMessageEncoder<TransferStateFileMeta>(org.apache.avro.generic.GenericData,org.apache.avro.Schema)encodeencode(java.lang.Object,java.io.OutputStream)encode(org.apache.flume.serialization.TransferStateFileMeta,java.io.OutputStream)encode(java.lang.Object)encode(org.apache.flume.serialization.TransferStateFileMeta)BinaryMessageEncoderBinaryMessageEncoder(org.apache.avro.generic.GenericData,org.apache.avro.Schema,boolean)BinaryMessageEncoder<TransferStateFileMeta>(org.apache.avro.generic.GenericData,org.apache.avro.Schema,boolean)BinaryMessageEncoder(org.apache.avro.generic.GenericData,org.apache.avro.Schema)V1_HEADERBinaryMessageDecoder<TransferStateFileMeta>(org.apache.avro.generic.GenericData,org.apache.avro.Schema)decodedecode(byte[],java.lang.Object)decode(byte[],org.apache.flume.serialization.TransferStateFileMeta)decode(byte[])decode(java.nio.ByteBuffer,java.lang.Object)decode(java.nio.ByteBuffer,org.apache.flume.serialization.TransferStateFileMeta)decode(java.nio.ByteBuffer)decode(java.io.InputStream,java.lang.Object)decode(java.io.InputStream,org.apache.flume.serialization.TransferStateFileMeta)decode(java.io.InputStream)BaseDecoderBaseDecoder()BaseDecoder<TransferStateFileMeta>()addSchemaaddSchema(org.apache.avro.Schema)BinaryMessageDecoderBinaryMessageDecoder(org.apache.avro.generic.GenericData,org.apache.avro.Schema,org.apache.avro.message.SchemaStore)BinaryMessageDecoder<TransferStateFileMeta>(org.apache.avro.generic.GenericData,org.apache.avro.Schema,org.apache.avro.message.SchemaStore)BinaryMessageDecoder(org.apache.avro.generic.GenericData,org.apache.avro.Schema)BinaryMessageEncoder<>MessageEncoder<>BinaryMessageDecoder<>BaseDecoder<>MessageDecoder<>resolverfield$"Invalid index: "Invalid index: value$Builder(org.apache.flume.serialization.TransferStateFileMeta)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/classes/org/apache/flume/serialization/TransferStateFileMeta$Builder.classBuilder(org.apache.flume.serialization.TransferStateFileMeta.Builder)SpecificRecordBuilderBase<TransferStateFileMeta>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/specific/SpecificRecordBuilderBase.classorg.apache.avro.dataRecordBuilderBase<TransferStateFileMeta>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/data/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/data/RecordBuilderBase.classRecordBuilder<TransferStateFileMeta>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/data/RecordBuilder.classfieldsfields()defaultValue(org.apache.avro.Schema.Field)isValidValueisValidValue(org.apache.avro.Schema.Field,java.lang.Object)validate(org.apache.avro.Schema.Field,java.lang.Object)RecordBuilderBaseRecordBuilderBase(org.apache.avro.data.RecordBuilderBase,org.apache.avro.generic.GenericData)RecordBuilderBase<TransferStateFileMeta>(org.apache.avro.data.RecordBuilderBase,org.apache.avro.generic.GenericData)RecordBuilderBase(org.apache.avro.Schema,org.apache.avro.generic.GenericData)RecordBuilderBase<TransferStateFileMeta>(org.apache.avro.Schema,org.apache.avro.generic.GenericData)data()boolean[]fieldSetFlagsfieldSetFlags()schema()fieldOrder"Corrupt ResolvingDecoder."Corrupt ResolvingDecoder.Autogenerated by AvroDO NOT EDIT DIRECTLYReturn the BinaryMessageEncoder instance used by this class.the message encoder used by this classReturn the BinaryMessageDecoder instance used by this class.the message decoder used by this classCreate a new BinaryMessageDecoder instance for this class that uses the specified {@link SchemaStore}.a {@link SchemaStore} used to find schemas by fingerprinta BinaryMessageDecoder instance for this class backed by the given SchemaStoreSerializes this TransferStateFileMeta to a ByteBuffer.a buffer holding the serialized data for this instanceif this instance could not be serializedDeserializes a TransferStateFileMeta from a ByteBuffer.a byte buffer holding serialized data for an instance of this classa TransferStateFileMeta instance decoded from the given bufferif the given bytes could not be deserialized into an instance of this classDefault constructor.  Note that this does not initialize fieldsto their default values from the schema.  If that is desired thenone should use <code>newBuilder()</code>.All-args constructor.The new value for offset Used by DatumWriter.  Applications should not call. Used by DatumReader.  Applications should not call.Gets the value of the 'offset' field.The value of the 'offset' field.Sets the value of the 'offset' field.the value to set.Creates a new TransferStateFileMeta RecordBuilder.A new TransferStateFileMeta RecordBuilderCreates a new TransferStateFileMeta RecordBuilder by copying an existing Builder.The existing builder to copy.Creates a new TransferStateFileMeta RecordBuilder by copying an existing TransferStateFileMeta instance.The existing instance to copy.RecordBuilder for TransferStateFileMeta instances.Creates a new BuilderCreates a Builder by copying an existing Builder.The existing Builder to copy.Creates a Builder by copying an existing TransferStateFileMeta instanceThe value.The value of 'offset'.This builder.Checks whether the 'offset' field has been set.True if the 'offset' field has been set, false otherwise.Clears the value of the 'offset' field./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/generated-sources/java/org/apache/flume/package-info.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/generated-sources/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/generated-sources/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-core/target/generated-sources/java/orgGenerated by scripts/saveVersion.sh/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-dist/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-dist1.9.0flume-ng-distFlume NG distributionrepositoriesrepositoryapache-repourlhttps://repository.apache.org/content/repositories/releases/releasesenabledsnapshots3.3.0assemblefinalNameapache-flume-${project.version}src/main/assembly/bin.xmlsrc/main/assembly/src.xmltarLongFileModeposixnet.nicoulaj.maven.pluginschecksum-maven-plugin1.7verifyalgorithmsSHA-512attachChecksumscsvSummary${project.build.directory}includesinclude*.tar.gzfailIfNoFilesexclusionsexclusion2.13.3/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-dist/src/main/assembly/bin.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-dist/src/main/assembly/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-dist/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-dist/srchttp://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.2http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.2 http://maven.apache.org/xsd/assembly-1.1.2.xsdbintar.gzapache-flume-${project.version}-binlibuseProjectArtifactorg.apache.flume.flume-ng-clients:flume-ng-log4jappender:jar:jar-with-dependencies The below test classes get pulled in by flume-shared-kafka-test,
             but we don't want them in the final release classpath. Maybe there
             is a better way to exclude all of the dependencies of that module;
             for now, specifically exclude these test deps. junit:junitorg.hamcrest:hamcrest-coretools../flume-ng-configuration/**flume-ng-sdk/**flume-ng-core/**flume-ng-node/**flume-ng-dist/**flume-ng-channels/**flume-ng-sinks/**flume-ng-sources/**flume-ng-legacy-sources/**flume-ng-clients/**flume-ng-embedded-agent/**flume-tools/**flume-ng-auth/**flume-shared/****/target/****/.classpath**/.project**/.settings/**lib/****/*.imlDEVNOTESREADME.mdCHANGELOGRELEASE-NOTESbin/**conf/**doap_Flume.rdf../target/site/**docstarget/maven-shared-archive-resources/META-INF/LICENSEtarget/maven-shared-archive-resources/META-INF/NOTICE/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-dist/src/main/assembly/src.xmlapache-flume-${project.version}-src**/.idea/****/.DS_Store./mvn/wrapper/maven-wrapper.jar/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agentFlume NG Embedded AgentFlume Embedded Agent: Stable public API for Embedding a Flume 1.x Agent TODO fix pmd violations org.apache.flume.agent.embedded only used for tests /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/src/main/java/org/apache/flume/agent/embedded/EmbeddedAgent.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/src/main/java/org/apache/flume/agent/embedded/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/src/main/java/org/apache/flume/agent/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/src/main/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/src/main/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/src/main/java/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/src/main/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/srcstopLogErrorstopLogError(org.apache.flume.lifecycle.LifecycleAware)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/target/classes/org/apache/flume/agent/embedded/EmbeddedAgent.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/target/classes/org/apache/flume/agent/embedded/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/target/classes/org/apache/flume/agent/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/target/classes/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/target/classes/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/target/classes/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/target/classes/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/targetdoStartdoStart()doConfiguredoConfigure(java.util.Map)EmbeddedAgentEmbeddedAgent(org.apache.flume.agent.embedded.MaterializedConfigurationProvider,java.lang.String)embeddedSourcesinkRunnersourceRunnersupervisorconfigurationProviderClass<EmbeddedAgent>"Cannot be configured while started"Cannot be configured while started"Source runner returned null source"Source runner returned null source"Cannot be started while started"Cannot be started while started"Cannot be started before being " +
          "configured"Cannot be started before being configured"Unknown source type: "Unknown source type: "Cannot be stopped unless started"Cannot be stopped unless startedMap<String,SourceRunner>Map<String,SinkRunner>"Agent configuration values"Agent configuration valuesTreeSet<String>TreeSet<String>(java.util.Collection)tailSet(java.lang.String)headSet(java.lang.String)subSet(java.lang.String,java.lang.String)tailSet(java.lang.String,boolean)headSet(java.lang.String,boolean)subSet(java.lang.String,boolean,java.lang.String,boolean)higher(java.lang.String)ceiling(java.lang.String)floor(java.lang.String)lower(java.lang.String)TreeSet<String>(java.util.SortedSet)TreeSet<String>(java.util.Comparator)TreeSet<String>()NavigableMap<String,Object>SortedMap<String,Object>TreeSet<String>(java.util.NavigableMap)? super SourceRunner? extends SourceRunnerBiFunction<? super SourceRunner,? super SourceRunner,? extends SourceRunner>merge(java.lang.String,org.apache.flume.SourceRunner,java.util.function.BiFunction)BiFunction<? super String,? super SourceRunner,? extends SourceRunner>Function<? super String,? extends SourceRunner>replace(java.lang.String,org.apache.flume.SourceRunner)replace(java.lang.String,org.apache.flume.SourceRunner,org.apache.flume.SourceRunner)putIfAbsent(java.lang.String,org.apache.flume.SourceRunner)BiConsumer<? super String,? super SourceRunner>getOrDefault(java.lang.Object,org.apache.flume.SourceRunner)Entry<String,SourceRunner>Set<Entry<String,SourceRunner>>Collection<Entry<String,SourceRunner>>Iterable<Entry<String,SourceRunner>>Collection<SourceRunner>Iterable<SourceRunner>Map<? extends String,? extends SourceRunner>put(java.lang.String,org.apache.flume.SourceRunner)"Expected one source and got "Expected one source and got "Expected one channel and got "Expected one channel and got ? super SinkRunner? extends SinkRunnerBiFunction<? super SinkRunner,? super SinkRunner,? extends SinkRunner>merge(java.lang.String,org.apache.flume.SinkRunner,java.util.function.BiFunction)BiFunction<? super String,? super SinkRunner,? extends SinkRunner>Function<? super String,? extends SinkRunner>replace(java.lang.String,org.apache.flume.SinkRunner)replace(java.lang.String,org.apache.flume.SinkRunner,org.apache.flume.SinkRunner)putIfAbsent(java.lang.String,org.apache.flume.SinkRunner)BiConsumer<? super String,? super SinkRunner>getOrDefault(java.lang.Object,org.apache.flume.SinkRunner)Entry<String,SinkRunner>Set<Entry<String,SinkRunner>>Collection<Entry<String,SinkRunner>>Iterable<Entry<String,SinkRunner>>Collection<SinkRunner>Iterable<SinkRunner>Map<? extends String,? extends SinkRunner>put(java.lang.String,org.apache.flume.SinkRunner)"Expected one sink group and got "Expected one sink group and got Iterator<SourceRunner>Consumer<? super SourceRunner>Spliterator<SourceRunner>Stream<SourceRunner>BaseStream<SourceRunner,Stream<SourceRunner>>Predicate<? super SourceRunner>Collection<? extends SourceRunner>Iterable<? extends SourceRunner>add(org.apache.flume.SourceRunner)Iterator<SinkRunner>Consumer<? super SinkRunner>Spliterator<SinkRunner>Stream<SinkRunner>BaseStream<SinkRunner,Stream<SinkRunner>>Predicate<? super SinkRunner>Collection<? extends SinkRunner>Iterable<? extends SinkRunner>add(org.apache.flume.SinkRunner)"Cannot put events unless started"Cannot put events unless started"Embedded agent "Embedded agent ": Unable to process event: ": Unable to process event: lifeCycleAware"Exception while stopping "Exception while stopping /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/target/classes/org/apache/flume/agent/embedded/EmbeddedAgent$State.classEmbeddedAgent gives Flume users the ability to embed simple agents inapplications. This Agent is mean to be much simpler than a traditionalagent and as such it's more restrictive than what can be configuredfor a traditional agent. For specifics see the Flume Developer Guide.Configures the embedded agent. Can only be called after the objectis created or after the stop() method is called.source, channel, and sink group configurationif a component is unable to be found or configuredif called while the agent is startedStarted the agent. Can only be called after a successful call toconfigure().if a component cannot be startedif the agent has not been configured or isalready started This check needs to be done before doStart(), as doStart() accesses sourceRunner.getSource()Stops the agent. Can only be called after a successful call to start().After a call to stop(), the agent can be re-configured with theconfigure() method or re-started with the start() method.if a component cannot be stoppedif the agent is not startedAdds event to the channel owned by the agent. Note however, that theevent is not copied and as such, the byte array and headers cannotbe re-used by the caller.if unable to add event to channelAdds events to the channel owned by the agent. Note however, that the/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/src/main/java/org/apache/flume/agent/embedded/EmbeddedAgentConfiguration.javaEmbeddedAgentConfigurationEmbeddedAgentConfiguration()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/target/classes/org/apache/flume/agent/embedded/EmbeddedAgentConfiguration.classjoinjoin(java.lang.String[])checkRequiredcheckRequired(java.util.Map,java.lang.String)checkAllowedcheckAllowed(java.lang.String[],java.lang.String)configure(java.lang.String,java.util.Map)validate(java.lang.String,java.util.Map)ImmutableList<String>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/ImmutableList.classImmutableCollection<String>DISALLOWED_SINK_NAMESALLOWED_SINK_PROCESSORSALLOWED_SINKSALLOWED_CHANNELSALLOWED_SOURCESSOURCE_TYPE_EMBEDDED_ALIASJOINER"source""processor"Class<EmbeddedSource>Map<String,EmbeddedSource>EmbeddedSource[]Constructor<EmbeddedSource>? super EmbeddedSourceClass<? super EmbeddedSource>TypeVariable<Class<EmbeddedSource>>TypeVariable<Class<EmbeddedSource>>[]"EMBEDDED"EMBEDDEDImmutableList<>sinkNamescopyIntoArraycopyIntoArray(java.lang.Object[],int)ImmutableList<E>asListasList()internalArrayEndinternalArrayEnd()internalArrayStartinternalArrayStart()internalArrayinternalArray()UnmodifiableIterator<E>UnmodifiableIterator<String>ImmutableCollectionImmutableCollection()ImmutableCollection<String>()SPLITERATOR_CHARACTERISTICS/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/ImmutableList$Builder.class/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/ImmutableCollection$Builder.classBuilder<E>reversereverse()subListUncheckedsubListUnchecked(int,int)UnmodifiableListIterator<>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/UnmodifiableListIterator.classUnmodifiableListIterator<E>UnmodifiableListIterator<String>ImmutableListImmutableList()ImmutableList<String>()ImmutableCollection<E>AbstractCollection<E>asImmutableListasImmutableList(java.lang.Object[],int)asImmutableList(java.lang.Object[])sortedCopyOfsortedCopyOf(java.util.Comparator,java.lang.Iterable)Comparable<? super E>sortedCopyOf(java.lang.Iterable)copyOf(java.lang.Object[])copyOf(java.util.Iterator)Iterator<? extends E>of(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object[])of(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)Collector<E,?,ImmutableList<E>>toImmutableListtoImmutableList()"Sink name "Sink name " is one of the" +
            " disallowed sink names: " is one of the disallowed sink names: strippedName"source-"source-"channel-"channel-sinkGroupName"sink-group-"sink-group-userProvidedKeys"Unknown configuration "Unknown configuration allowedTypesisAllowedallowedType"Component type of "Component type of " is not in " +
          "allowed types of " is not in allowed types of "Required parameter not found "Required parameter not found partsStores publicly accessible configuration constants and privateconfiguration constants and methods.Prefix for source propertiesPrefix for channel propertiesPrefix for sink processor propertiesSpace delimited list of sink names: e.g. sink1 sink2 sink3Source type, choices are `embedded'Prefix for passing configuration parameters to the sourceChannel type, choices are `memory' or `file'Prefix for passing configuration parameters to the channelSink processor type, choices are `default', `failover' or `load_balance'Prefix for passing configuration parameters to the sink processorEmbedded source which provides simple in-memory transfer to channel.Use this source via the put,putAll methods on the EmbeddedAgent. Thisis the only supported source to use for Embedded Agents.Memory channel which stores events in heap. See Flume User Guide forconfiguration information. This is the recommended channel to use forEmbedded Agents.Spillable Memory channel which stores events in heap. See Flume User Guide forFile based channel which stores events in on local disk. See Flume UserGuide for configuration information.Avro sink which can send events to a downstream avro source. This is theonly supported sink for Embedded Agents.Default sink processors which may be used when there is only a single sink.Failover sink processor. See Flume User Guide for configurationinformation.Load balancing sink processor. See Flume User Guide for configurationFolds embedded configuration structure into an agent configuration.Should only be called after validate returns without error.- agent name- embedded agent configurationconfiguration applicable to a flume agent we are going to modify the properties as we parse the configNow we are going to process the user supplied configurationand generate an agent configuration. This is only to supplya simpler client api than passing in an entire agent configuration. user supplied config -> agent configurationFirst we are going to setup all the root level pointers. I.Epoint the agent at the components, sink group at sinks, andsource at the channel. point agent at source point agent at channel point agent at sinks points the agent at the sinkgroup points the sinkgroup at the sinks points the source at the channel Properties will be modified during iteration so we need a copy of the keys.Second process the sink configuration and point the sinksat the channel. point the sink at the channelThird, process all remaining configuration items, prefixing themcorrectly and then passing them on to the agent. users use `source' but agent needs the actual source name users use `channel' but agent needs the actual channel name agent.sinkgroups.sinkgroup.processor.* XXX should we simply ignore this?/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/src/main/java/org/apache/flume/agent/embedded/EmbeddedSource.javaSimple source used to allow direct access to the channel for the EmbeddedAgent./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/src/main/java/org/apache/flume/agent/embedded/MaterializedConfigurationProvider.javaget(java.lang.String,java.util.Map)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/target/classes/org/apache/flume/agent/embedded/MaterializedConfigurationProvider.classMaterializedConfigurationProviderMaterializedConfigurationProvider()confProviderProvides {@see MaterializedConfiguration} for a given agent and set ofproperties. This class exists simply to make more easily testable. That isit allows us to mock the actual Source, Sink, and Channel componentsas opposed to instantiation of real components./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/src/main/java/org/apache/flume/agent/embedded/MemoryConfigurationProvider.javaMemoryConfigurationProviderMemoryConfigurationProvider(java.lang.String,java.util.Map)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/target/classes/org/apache/flume/agent/embedded/MemoryConfigurationProvider.classMemoryConfigurationProvider is the simplest possibleAbstractConfigurationProvider simply turning a give properties file andagent name into a FlumeConfiguration object./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/src/main/java/org/apache/flume/agent/embedded/package-info.javaThis package provides Flume users the ability to embed simple agentsin applications. For specific and up to date information, please seethe Flume Developer Guide./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/src/test/resources/log4j2.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/src/test/resources/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-embedded-agent/src/test/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node1.10.1Flume NG Nodeorg.apache.flume.node
    <dependency>
      <groupId>org.apache.flume</groupId>
      <artifactId>flume-hdfs-sink</artifactId>
    </dependency><dependency>
      <groupId>org.apache.flume.flume-ng-channels</groupId>
      <artifactId>flume-jdbc-channel</artifactId>
    </dependency>log4j-apiorg.apache.commonscommons-textorg.apache.curatorcurator-frameworkcurator-recipescurator-testnet.jcipjcip-annotationscom.github.spotbugsspotbugs-annotations/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/AbstractConfigurationProvider.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/srcloadSinkGroupsloadSinkGroups(org.apache.flume.conf.FlumeConfiguration.AgentConfiguration,java.util.Map,java.util.Map)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/AbstractConfigurationProvider.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/targetMap<String,ChannelComponent>loadSinksloadSinks(org.apache.flume.conf.FlumeConfiguration.AgentConfiguration,java.util.Map,java.util.Map)checkSinkChannelCompatibilitycheckSinkChannelCompatibility(org.apache.flume.Sink,org.apache.flume.Channel)checkSourceChannelCompatibilitycheckSourceChannelCompatibility(org.apache.flume.Source,org.apache.flume.Channel)getSourceChannelsgetSourceChannels(java.util.Map,org.apache.flume.Source,java.util.Collection)loadSourcesloadSources(org.apache.flume.conf.FlumeConfiguration.AgentConfiguration,java.util.Map,java.util.Map)ListMultimap<Class<? extends Channel>,String>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar/Users/burakyetistiren/.m2/repository/com/google/guava/guava/11.0.2/Users/burakyetistiren/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar/com/Users/burakyetistiren/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar/com/google/Users/burakyetistiren/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar/com/google/common/Users/burakyetistiren/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar/com/google/common/collect/Users/burakyetistiren/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar/com/google/common/collect/ListMultimap.classMultimap<Class<? extends Channel>,String>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar/com/google/common/collect/Multimap.classgetOrCreateChannelgetOrCreateChannel(com.google.common.collect.ListMultimap,java.lang.String,java.lang.String)loadChannelsloadChannels(org.apache.flume.conf.FlumeConfiguration.AgentConfiguration,java.util.Map)Map<Class<? extends Channel>,Map<String,Channel>>channelCachesinkFactorysourceFactory"REC_CATCH_EXCEPTION"REC_CATCH_EXCEPTIONClass<AbstractConfigurationProvider>HashMap<Class<? extends Channel>,Map<String,Channel>>AbstractMap<Class<? extends Channel>,Map<String,Channel>>HashMap<Class<? extends Channel>,Map<String,Channel>>()? super Map<String,Channel>? extends Map<String,Channel>BiFunction<? super Map<String,Channel>,? super Map<String,Channel>,? extends Map<String,Channel>>merge(java.lang.Class,java.util.Map,java.util.function.BiFunction)? super Class<? extends Channel>BiFunction<? super Class<? extends Channel>,? super Map<String,Channel>,? extends Map<String,Channel>>compute(java.lang.Class,java.util.function.BiFunction)computeIfPresent(java.lang.Class,java.util.function.BiFunction)Function<? super Class<? extends Channel>,? extends Map<String,Channel>>computeIfAbsent(java.lang.Class,java.util.function.Function)replace(java.lang.Class,java.util.Map)replace(java.lang.Class,java.util.Map,java.util.Map)putIfAbsent(java.lang.Class,java.util.Map)BiConsumer<? super Class<? extends Channel>,? super Map<String,Channel>>Entry<Class<? extends Channel>,Map<String,Channel>>Set<Entry<Class<? extends Channel>,Map<String,Channel>>>Collection<Entry<Class<? extends Channel>,Map<String,Channel>>>Iterable<Entry<Class<? extends Channel>,Map<String,Channel>>>Collection<Map<String,Channel>>Iterable<Map<String,Channel>>Set<Class<? extends Channel>>Collection<Class<? extends Channel>>Iterable<Class<? extends Channel>>? extends Class<? extends Channel>Map<? extends Class<? extends Channel>,? extends Map<String,Channel>>put(java.lang.Class,java.util.Map)AbstractMap<Class<? extends Channel>,Map<String,Channel>>()Node<Class<? extends Channel>,Map<String,Channel>>TreeNode<Class<? extends Channel>,Map<String,Channel>>newTreeNode(int,java.lang.Class,java.util.Map,java.util.HashMap.Node)newNode(int,java.lang.Class,java.util.Map,java.util.HashMap.Node)Node<Class<? extends Channel>,Map<String,Channel>>[]putVal(int,java.lang.Class,java.util.Map,boolean,boolean)HashMap<Class<? extends Channel>,Map<String,Channel>>(java.util.Map)HashMap<Class<? extends Channel>,Map<String,Channel>>(int)HashMap<Class<? extends Channel>,Map<String,Channel>>(int,float)fconfigagentConfchannelComponentMapHashMap<String,ChannelComponent>AbstractMap<String,ChannelComponent>sourceRunnerMapHashMap<String,SourceRunner>AbstractMap<String,SourceRunner>sinkRunnerMapHashMap<String,SinkRunner>AbstractMap<String,SinkRunner>? super ChannelComponent? extends ChannelComponentBiFunction<? super ChannelComponent,? super ChannelComponent,? extends ChannelComponent>merge(java.lang.String,org.apache.flume.node.AbstractConfigurationProvider.ChannelComponent,java.util.function.BiFunction)BiFunction<? super String,? super ChannelComponent,? extends ChannelComponent>Function<? super String,? extends ChannelComponent>replace(java.lang.String,org.apache.flume.node.AbstractConfigurationProvider.ChannelComponent)replace(java.lang.String,org.apache.flume.node.AbstractConfigurationProvider.ChannelComponent,org.apache.flume.node.AbstractConfigurationProvider.ChannelComponent)putIfAbsent(java.lang.String,org.apache.flume.node.AbstractConfigurationProvider.ChannelComponent)BiConsumer<? super String,? super ChannelComponent>getOrDefault(java.lang.Object,org.apache.flume.node.AbstractConfigurationProvider.ChannelComponent)Entry<String,ChannelComponent>Set<Entry<String,ChannelComponent>>Collection<Entry<String,ChannelComponent>>Iterable<Entry<String,ChannelComponent>>Collection<ChannelComponent>Iterable<ChannelComponent>Map<? extends String,? extends ChannelComponent>put(java.lang.String,org.apache.flume.node.AbstractConfigurationProvider.ChannelComponent)channelComponentnameChannelMap"Channel %s has no components connected" +
                " and has been removed."Channel %s has no components connected and has been removed."Channel %s connected to %s"Channel %s connected to %ssetValue(org.apache.flume.SourceRunner)setValue(org.apache.flume.SinkRunner)"Failed to instantiate component"Failed to instantiate component"No configuration found for this host:{}"No configuration found for this host:{}channelsNotReusedArrayListMultimap<Class<? extends Channel>,String>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar/com/google/common/collect/ArrayListMultimap.classAbstractListMultimap<Class<? extends Channel>,String>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar/com/google/common/collect/AbstractListMultimap.classAbstractMultimap<Class<? extends Channel>,String>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar/com/google/common/collect/AbstractMultimap.classArrayListMultimap<>AbstractListMultimap<>ListMultimap<>compMap"Creating channels"Creating channelschannelKlasssetValue(java.util.Map)Map<Class<? extends Channel>,Collection<String>>Entry<Class<? extends Channel>,String>Collection<Entry<Class<? extends Channel>,String>>Iterable<Entry<Class<? extends Channel>,String>>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar/com/google/common/collect/Multiset.classMultiset<Class<? extends Channel>>replaceValues(java.lang.Class,java.lang.Iterable)Multimap<? extends Class<? extends Channel>,? extends String>putAll(java.lang.Class,java.lang.Iterable)put(java.lang.Class,java.lang.String)List<V>chNamecomp"Created channel "Created channel "Channel %s has been removed due to an " +
              "error during configuration"Channel %s has been removed due to an error during configurationchannelMap"Removed {} of type {}"Removed {} of type {}Class<Disposable>sourceNamessourceContextssourceChannelsselectorConfig"Source %s is not connected to a " +
                "channel"Source %s is not connected to a channel"Channel %s"Channel %s"Source %s has been removed due to an " +
              "error during configuration"Source %s has been removed due to an error during configurationtransCap"Incompatible source and channel settings defined. " +
                "source's batch size is greater than the channels transaction capacity. " +
                "Source: %s, batch size = %d, channel %s, transaction capacity = %d"Incompatible source and channel settings defined. source's batch size is greater than the channels transaction capacity. Source: %s, batch size = %d, channel %s, transaction capacity = %d"Incompatible sink and channel settings defined. " +
                "sink's batch size is greater than the channels transaction capacity. " +
                "Sink: %s, batch size = %d, channel %s, transaction capacity = %d"Incompatible sink and channel settings defined. sink's batch size is greater than the channels transaction capacity. Sink: %s, batch size = %d, channel %s, transaction capacity = %dsinkContexts"Sink %s is not connected to a " +
                "channel"Sink %s is not connected to a channel"Sink %s has been removed due to an " +
              "error during configuration"Sink %s has been removed due to an error during configurationsinkGroupNamesgroupNamesinkUser"Sink %s of group %s already " +
                      "in use by group %s"Sink %s of group %s already in use by group %s"Sink %s of group %s does "
                      + "not exist or is not properly configured"Sink %s of group %s does not exist or is not properly configured"SinkGroup %s has been removed due to " +
              "an error during configuration"SinkGroup %s has been removed due to an error during configurationprsinkMapChannelComponentChannelComponent(org.apache.flume.Channel)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/AbstractConfigurationProvider$ChannelComponent.classcomponentsEnumeration<?>propertyNameshasMoreElementshasMoreElements()asIteratorasIterator()nextElementnextElement()Some channels will be reused across re-configurations. To handle this,we store all the names of current channels, perform the reconfiguration,and then if a channel was not used, we delete our reference to it.This supports the scenario where you enable channel "ch0" then remove itand add it back. Without this, channels like memory channel would causethe first instances data to show up in the seconds. assume all channels will not be re-usedComponents which have a ComponentConfiguration objectComponents which DO NOT have a ComponentConfiguration objectand use only ContextAny channel which was not re-used, will have it's reference removedChannel has requested a new instance on each re-configuration add any unassigned sinks to solo collectors/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/AbstractZooKeeperConfigurationProvider.javaDEFAULT_ZK_BASE_PATH/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/AbstractZooKeeperConfigurationProvider.class"/flume"/flumezkConnString"Invalid Zookeeper Connection String %s"Invalid Zookeeper Connection String %sconfigDataconfigMapfileContentZooKeeper based configuration implementation provider.The Agent configuration can be uploaded in ZooKeeper under a base name, whichdefaults to /flumeCurrently the agent configuration is stored under the agent name node inZooKeeper/a1 [agent config file]/a2 [agent config file]/a3 [agent config file]Configuration format is same as PropertiesFileConfigurationProviderConfiguration propertiesagentName - Name of Agent for which configuration needs to be pulledzkConnString - Connection string to ZooKeeper Ensemble(host:port,host1:port1)basePath - Base Path where agent configuration needs to be stored. Defaultsto /flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/Application.javaloadConfigOptsloadConfigOpts()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/Application.class"PMD"PMDloadMonitoringloadMonitoring()startAllComponentsstartAllComponents(org.apache.flume.node.MaterializedConfiguration)initializeAllComponentsinitializeAllComponents(org.apache.flume.node.MaterializedConfiguration)stopAllComponentsstopAllComponents()lifecycleLockmonitorServermaterializedConfigurationDEFAULT_FILE_INTERVALDEFAULT_INTERVALClass<Application>"flume.monitoring.type"flume.monitoring.type"flume.monitoring."flume.monitoring.300ArrayList<LifecycleAware>AbstractList<LifecycleAware>AbstractCollection<LifecycleAware>ArrayList<LifecycleAware>(int)Spliterator<LifecycleAware>Consumer<? super LifecycleAware>Iterator<LifecycleAware>Stream<LifecycleAware>BaseStream<LifecycleAware,Stream<LifecycleAware>>Predicate<? super LifecycleAware>Collection<? extends LifecycleAware>Iterable<? extends LifecycleAware>add(org.apache.flume.lifecycle.LifecycleAware)AbstractCollection<LifecycleAware>()addLast(org.apache.flume.lifecycle.LifecycleAware)addFirst(org.apache.flume.lifecycle.LifecycleAware)ListIterator<LifecycleAware>add(int,org.apache.flume.lifecycle.LifecycleAware)set(int,org.apache.flume.lifecycle.LifecycleAware)Comparator<? super LifecycleAware>UnaryOperator<LifecycleAware>Function<LifecycleAware,LifecycleAware>AbstractList<LifecycleAware>()ArrayList<LifecycleAware>(java.util.Collection)ArrayList<LifecycleAware>()"Interrupted while trying to handle configuration event"Interrupted while trying to handle configuration event"Shutting down configuration: {}"Shutting down configuration: {}"Stopping Source "Stopping Source "Error while stopping {}"Error while stopping {}"Stopping Sink "Stopping Sink "Stopping Channel "Stopping Channel setValue(org.apache.flume.Channel)"Initializing components"Initializing components"Starting new configuration:{}"Starting new configuration:{}"Starting Channel "Starting Channel "Error while starting {}"Error while starting {}"Waiting for channel: "Waiting for channel: " to start. Sleeping for 500 ms" to start. Sleeping for 500 ms"Interrupted while waiting for channel to start."Interrupted while waiting for channel to start."Starting Sink "Starting Sink "Starting Source "Starting Source systemPropsmonitorTypeConstructor<? extends MonitorService>TypeVariable<Constructor<? extends MonitorService>>TypeVariable<Constructor<? extends MonitorService>>[]Constructor<? extends MonitorService>(java.lang.Class,java.lang.Class[],java.lang.Class[],int,int,java.lang.String,byte[],byte[])Map<String,? extends MonitorService>MonitorService[]TypeVariable<Class<? extends MonitorService>>TypeVariable<Class<? extends MonitorService>>[]"Error starting monitoring. "
          + "Monitoring might not be available."Error starting monitoring. Monitoring might not be available.initProps"the name of this agent"the name of this agent"no-reload-conf"no-reload-confisZkConfiguredList<URI>SequencedCollection<URI>Collection<URI>Iterable<URI>confUridefaultIntervalapplicationappReference"f"f"conf-file"conf-file"specify a config file (required if -c, -u, and -z are missing)"specify a config file (required if -c, -u, and -z are missing)"u"u"conf-uri"conf-uri"specify a config uri (required if -c, -f and -z are missing)"specify a config uri (required if -c, -f and -z are missing)"auth-provider"auth-provider"specify an authorization provider class"specify an authorization provider class"prov"prov"conf-provider"conf-provider"specify a configuration provider class (required if -f, -u, and -z are missing)"specify a configuration provider class (required if -f, -u, and -z are missing)"user"user"conf-user"conf-user"user name to access configuration uri"user name to access configuration uri"pwd"pwd"conf-password"conf-password"password to access configuration uri"password to access configuration uri"poll-interval"poll-interval"number of seconds between checks for a configuration change"number of seconds between checks for a configuration change"b""backup-directory"backup-directory"directory in which to store the backup configuration file"directory in which to store the backup configuration file"do not reload config file if changed"do not reload config file if changed"z""zkConnString""specify the ZooKeeper connection to use (required if -c, -f, and -u are missing)"specify the ZooKeeper connection to use (required if -c, -f, and -u are missing)"zkBasePath"zkBasePath"specify the base path in ZooKeeper for agent configs"specify the base path in ZooKeeper for agent configs"flume-ng agent"flume-ng agent'u'ArrayList<URI>AbstractList<URI>AbstractCollection<URI>ArrayList<URI>()Spliterator<URI>? super URIConsumer<? super URI>Iterator<URI>Stream<URI>BaseStream<URI,Stream<URI>>Predicate<? super URI>? extends URICollection<? extends URI>Iterable<? extends URI>add(java.net.URI)AbstractCollection<URI>()addLast(java.net.URI)addFirst(java.net.URI)ListIterator<URI>add(int,java.net.URI)set(int,java.net.URI)Comparator<? super URI>UnaryOperator<URI>Function<URI,URI>AbstractList<URI>()ArrayList<URI>(java.util.Collection)ArrayList<URI>(int)uri"http"httpfilePathMap<String,Application>Application[]Constructor<Application>? super ApplicationClass<? super Application>TypeVariable<Class<Application>>TypeVariable<Class<Application>>[]"Error creating ConfigurationProvider {}"Error creating ConfigurationProvider {}zkConnectionStrbaseZkPatheventBus"-event-bus"-event-buszookeeperConfigurationProviderconfUserconfPasswordpollInterval"verify-host"verify-hostverifyHostauthorizationProviderauthProviderClassList<ConfigurationSource>SequencedCollection<ConfigurationSource>Collection<ConfigurationSource>Iterable<ConfigurationSource>configurationSourcesArrayList<ConfigurationSource>AbstractList<ConfigurationSource>AbstractCollection<ConfigurationSource>ArrayList<ConfigurationSource>()Spliterator<ConfigurationSource>? super ConfigurationSourceConsumer<? super ConfigurationSource>Iterator<ConfigurationSource>Stream<ConfigurationSource>BaseStream<ConfigurationSource,Stream<ConfigurationSource>>Predicate<? super ConfigurationSource>? extends ConfigurationSourceCollection<? extends ConfigurationSource>Iterable<? extends ConfigurationSource>add(org.apache.flume.node.ConfigurationSource)AbstractCollection<ConfigurationSource>()addLast(org.apache.flume.node.ConfigurationSource)addFirst(org.apache.flume.node.ConfigurationSource)ListIterator<ConfigurationSource>add(int,org.apache.flume.node.ConfigurationSource)set(int,org.apache.flume.node.ConfigurationSource)Comparator<? super ConfigurationSource>UnaryOperator<ConfigurationSource>Function<ConfigurationSource,ConfigurationSource>AbstractList<ConfigurationSource>()ArrayList<ConfigurationSource>(java.util.Collection)ArrayList<ConfigurationSource>(int)"The supplied authorization provider does not implement AuthorizationProvider"The supplied authorization provider does not implement AuthorizationProvider"Unable to create authorization provider: {}"Unable to create authorization provider: {}configurationSource"No configuiration was provided"No configuiration was provided/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/Application$1.class(java.lang.String)"agent-shutdown-hook"agent-shutdown-hook"A fatal error occurred while running. Exception follows."A fatal error occurred while running. Exception follows.is"/etc/flume/flume.opts"/etc/flume/flume.opts"flume.opts"flume.opts"Unable to load options file due to: {}"Unable to load options file due to: {} If interrupted while trying to lock, we don't own the lock, so must not attempt to unlockWait for all channels to start.Is it a known type?Not a known type, use FQCN Options for Zookeeper get options Ignore the exception. Ignore this error./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/ClasspathConfigurationSource.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/ClasspathConfigurationSource.class"Invalid uri: "Invalid uri: ? extends ClasspathConfigurationSourceClass<? extends ClasspathConfigurationSource>Map<String,? extends ClasspathConfigurationSource>ClasspathConfigurationSource[]Constructor<? extends ClasspathConfigurationSource>TypeVariable<Class<? extends ClasspathConfigurationSource>>TypeVariable<Class<? extends ClasspathConfigurationSource>>[]"{ classpath: "{ classpath:  classpath:///filename && classpath:/filename classpath://filename classpath:filename/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/ClasspathConfigurationSourceFactory.javaSCHEMES/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/ClasspathConfigurationSourceFactory.class"classpath"classpathCreates a ConfigurationSource from a file on the classpath../Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/ConfigurationProvider.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/ConfigurationSource.java"properties""json""yaml"yaml"xml"xmlInterface for retrieving configuration data.Returns the InputStream if it hasn't already been processed.The InputStream or null.Returns the URI string.The string URI.Determine if the configuration data source has been modified since it was last checked.true if the data was modified.Return the "file" extension for the specified uri.The file extension./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/ConfigurationSourceFactory.javaServiceLoader<ConfigurationSourceFactory>/modules/java.base/java/util/ServiceLoader.classIterable<ConfigurationSourceFactory>serviceLoaderServiceLoader<>Class<ConfigurationSourceFactory>Map<String,ConfigurationSourceFactory>ConfigurationSourceFactory[]Constructor<ConfigurationSourceFactory>? super ConfigurationSourceFactoryClass<? super ConfigurationSourceFactory>TypeVariable<Class<ConfigurationSourceFactory>>TypeVariable<Class<ConfigurationSourceFactory>>[]configurationSourceFactoryCreates ConfigurationSources./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/EnvVarResolverProperties.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/EnvVarResolverProperties.class9134232469049352862L9134232469049352862"\\$\\{(\\w+)\\}"\$\{(\w+)\}envVarNameenvVarValueA class that extends the Java built-in Properties overriding{@link java.util.Properties#getProperty(String)} to allow ${ENV_VAR_NAME}-style environmentvariable inclusionsUse ${env:key} instead.The input string with ${ENV_VAR_NAME}-style environment variable namesThe output string with ${ENV_VAR_NAME} replaced with their environment variable values match ${ENV_VAR_NAME}the property keythe value of the property key with ${ENV_VAR_NAME}-style environment variables replaced/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/FileConfigurationSource.javalastChange/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/FileConfigurationSource.classClass<FileConfigurationSource>"Unable to read {}: {}"Unable to read {}: {}"Unable to read file "Unable to read file "Checking file:{} for changes"Checking file:{} for changes"file.checks"file.checks"Reloading configuration file:{}"Reloading configuration file:{}"file.loads"file.loads"Failed to load configuration data. Exception follows."Failed to load configuration data. Exception follows."Failed to start agent because dependencies were not found in classpath."
            + "Error follows."Failed to start agent because dependencies were not found in classpath.Error follows."Unhandled error"Unhandled error"{ file:"{ file: caught because the caller does not handle or log Throwables/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/FileConfigurationSourceFactory.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/FileConfigurationSourceFactory.class"EI_EXPOSE_REP"EI_EXPOSE_REPCreates a FileConfigurationSource./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/HttpConfigurationSource.javareadStreamreadStream(java.io.InputStream)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/HttpConfigurationSource.classreadInputStreamreadInputStream()BUF_SIZEOKNOT_FOUNDNOT_AUTHORIZEDNOT_MODIFIEDClass<HttpConfigurationSource>304401404200"Checking {} for changes"Checking {} for changes"uri.checks"uri.checks"Reloading configuration from:{}"Reloading configuration from:{}"uri.loads"uri.loads"Unable to access configuration due to {}: "Unable to access configuration due to {}: connectioncode"Configuration Not Modified"Configuration Not Modified"Content was modified for {}. lastModified: {}"Content was modified for {}. lastModified: {}es"Error accessing configuration at {}: {}"Error accessing configuration at {}: {}"Unable to access "Unable to access "Unable to locate "Unable to locate "Authorization failed"Authorization failed"Invalid response code returned"Invalid response code returned"Unexpected response code returned {}"Unexpected response code returned {}"Error accessing {}: {}"Error accessing {}: {}"{ uri:"{ uri:/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/HttpConfigurationSourceFactory.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/HttpConfigurationSourceFactory.classCreates an HttpConfigurationSource./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/Initializable.javaAn interface implmmented by components that need access after all components have been created but beforeany have been started.Called to initialize the component.the materialized configuration./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/MapResolver.javaloadPropertiesloadProperties()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/MapResolver.classMap<String,StringLookup>createLookupMapcreateLookupMap()LookupEntry[]LOOKUP_ENTRIESLOOKUPENV_VAR_PROPERTYPROPS_IMPL_KEYCUSTOM_LOOKUPS_KEYDEFAULT_LOOKUPSMapResolverMapResolver()Class<MapResolver>"lookups.properties"lookups.properties"lookups"lookups"propertiesImplementation"propertiesImplementation"org.apache.flume.node.EnvVarResolverProperties"org.apache.flume.node.EnvVarResolverProperties"org.apache.commons.text.lookup.DefaultStringLookup."org.apache.commons.text.lookup.DefaultStringLookup."sys"sys"env"env"java"java"date"useEnvVarsdefaultLookupsubstitutorConsumer<String>accept(java.lang.String)HashMap<String,StringLookup>AbstractMap<String,StringLookup>HashMap<String,StringLookup>()? super StringLookup? extends StringLookupBiFunction<? super StringLookup,? super StringLookup,? extends StringLookup>merge(java.lang.String,org.apache.commons.text.lookup.StringLookup,java.util.function.BiFunction)BiFunction<? super String,? super StringLookup,? extends StringLookup>Function<? super String,? extends StringLookup>replace(java.lang.String,org.apache.commons.text.lookup.StringLookup)replace(java.lang.String,org.apache.commons.text.lookup.StringLookup,org.apache.commons.text.lookup.StringLookup)putIfAbsent(java.lang.String,org.apache.commons.text.lookup.StringLookup)BiConsumer<? super String,? super StringLookup>getOrDefault(java.lang.Object,org.apache.commons.text.lookup.StringLookup)Entry<String,StringLookup>Set<Entry<String,StringLookup>>Collection<Entry<String,StringLookup>>Iterable<Entry<String,StringLookup>>Collection<StringLookup>Iterable<StringLookup>Map<? extends String,? extends StringLookup>put(java.lang.String,org.apache.commons.text.lookup.StringLookup)AbstractMap<String,StringLookup>()Node<String,StringLookup>TreeNode<String,StringLookup>newTreeNode(int,java.lang.String,org.apache.commons.text.lookup.StringLookup,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.commons.text.lookup.StringLookup,java.util.HashMap.Node)Node<String,StringLookup>[]putVal(int,java.lang.String,org.apache.commons.text.lookup.StringLookup,boolean,boolean)HashMap<String,StringLookup>(java.util.Map)HashMap<String,StringLookup>(int)HashMap<String,StringLookup>(int,float)Stream<LookupEntry>BaseStream<LookupEntry,Stream<LookupEntry>>? super LookupEntryConsumer<? super LookupEntry>Spliterator<LookupEntry>Iterator<LookupEntry>Optional<LookupEntry>Predicate<? super LookupEntry>Comparator<? super LookupEntry>List<LookupEntry>SequencedCollection<LookupEntry>Collection<LookupEntry>Iterable<LookupEntry>Collector<? super LookupEntry,A,R>BiConsumer<R,? super LookupEntry>BiFunction<U,? super LookupEntry,U>BinaryOperator<LookupEntry>BiFunction<LookupEntry,LookupEntry,LookupEntry>reduce(org.apache.flume.node.MapResolver.LookupEntry,java.util.function.BinaryOperator)BiConsumer<? super LookupEntry,? super DoubleConsumer>BiConsumer<? super LookupEntry,? super LongConsumer>BiConsumer<? super LookupEntry,? super IntConsumer>BiConsumer<? super LookupEntry,? super Consumer<R>>Function<? super LookupEntry,? extends DoubleStream>Function<? super LookupEntry,? extends LongStream>Function<? super LookupEntry,? extends IntStream>Function<? super LookupEntry,? extends Stream<? extends R>>ToDoubleFunction<? super LookupEntry>ToLongFunction<? super LookupEntry>ToIntFunction<? super LookupEntry>Function<? super LookupEntry,? extends R>Consumer<LookupEntry>accept(org.apache.flume.node.MapResolver.LookupEntry)BiConsumer<? super Object,? super Object>BiConsumer<Object,Object>BiConsumer<? super T,? super U>andThen(java.util.function.BiConsumer)accept(java.lang.Object,java.lang.Object)vlookupEnumstringLookup"{} is not a DefaultStringLookup enum value, ignoring"{} is not a DefaultStringLookup enum value, ignoringClass<StringLookup>StringLookup[]Constructor<StringLookup>Class<? super StringLookup>TypeVariable<Class<StringLookup>>TypeVariable<Class<StringLookup>>[]"{} is not a StringLookup, ignoring"{} is not a StringLookup, ignoring"Unable to load {} due to {}, ignoring"Unable to load {} due to {}, ignoringDefaultLookupDefaultLookup(java.util.Map)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/MapResolver$DefaultLookup.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/MapResolver$LookupEntry.class"Unable to load {} due to {}"Unable to load {} due to {}Resolves replaceable tokens to create a Map.Needs org.apache.commons:commons-lang3 on classpathProvide compatibility with EnvVarResolverProperties.The key.The value associated with the key or null./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/MaterializedConfiguration.javaMaterializedConfiguration represents the materialization of a Flumeproperties file. That is it's the actual Source, Sink, and Channelsrepresented in the configuration file./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/PollingPropertiesFileConfigurationProvider.javaUse UriConfigurationProvider instead./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/PollingZooKeeperConfigurationProvider.javarefreshConfigurationrefreshConfiguration()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/PollingZooKeeperConfigurationProvider.classflumeConfigurationagentNodeCacheClass<PollingZooKeeperConfigurationProvider>"Starting..."Starting...org.apache.curator.framework.listenListenable<NodeCacheListener>/Users/burakyetistiren/.m2/repository/org/apache/curator/curator-framework/5.1.0/curator-framework-5.1.0.jar/Users/burakyetistiren/.m2/repository/org/apache/curator/curator-framework/5.1.0/Users/burakyetistiren/.m2/repository/org/apache/curator/curator-framework/Users/burakyetistiren/.m2/repository/org/apache/curator/Users/burakyetistiren/.m2/repository/org/apache/curator/curator-framework/5.1.0/curator-framework-5.1.0.jar/org/Users/burakyetistiren/.m2/repository/org/apache/curator/curator-framework/5.1.0/curator-framework-5.1.0.jar/org/apache/Users/burakyetistiren/.m2/repository/org/apache/curator/curator-framework/5.1.0/curator-framework-5.1.0.jar/org/apache/curator/Users/burakyetistiren/.m2/repository/org/apache/curator/curator-framework/5.1.0/curator-framework-5.1.0.jar/org/apache/curator/framework/Users/burakyetistiren/.m2/repository/org/apache/curator/curator-framework/5.1.0/curator-framework-5.1.0.jar/org/apache/curator/framework/listen/Users/burakyetistiren/.m2/repository/org/apache/curator/curator-framework/5.1.0/curator-framework-5.1.0.jar/org/apache/curator/framework/listen/Listenable.classaddListeneraddListener(org.apache.curator.framework.recipes.cache.NodeCacheListener)removeListenerremoveListener(java.lang.Object)removeListener(org.apache.curator.framework.recipes.cache.NodeCacheListener)addListener(java.lang.Object,java.util.concurrent.Executor)addListener(org.apache.curator.framework.recipes.cache.NodeCacheListener,java.util.concurrent.Executor)addListener(java.lang.Object)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/PollingZooKeeperConfigurationProvider$1.classnodeChangednodeChanged()Listenable<>childData"Refreshing configuration from ZooKeeper"Refreshing configuration from ZooKeeper"Stopping..."Stopping..."Encountered exception while stopping"Encountered exception while stopping"Error stopping Curator client"Error stopping Curator client/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/PropertiesFileConfigurationProvider.javaA configuration provider that uses properties file for specifyingconfiguration. The configuration files follow the Java properties file syntaxrules specified at {@link java.util.Properties#load(java.io.Reader)}. Everyconfiguration value specified in the properties file is prefixed by an<em>Agent Name</em> which helps isolate an individual agent&apos;s namespace.Valid configuration files must observe the following rules for every agentnamespace.<li>For every &lt;agent name&gt; there must be three lists specified thatinclude <tt>&lt;agent name&gt;.sources</tt>,<tt>&lt;agent name&gt;.sinks</tt>, and <tt>&lt;agent name&gt;.channels</tt>.Each of these lists must contain a space separated list of namescorresponding to that particular entity.</li><li>For each source named in <tt>&lt;agent name&gt;.sources</tt>, there mustbe a non-empty <tt>type</tt> attribute specified from the valid set of sourcetypes. For example:<tt>&lt;agent name&gt;.sources.&lt;source name&gt;.type = event</tt></li>be a space-separated list of channel names that the source will associatewith during runtime. Each of these names must be contained in the channelslist specified by <tt>&lt;agent name&gt;.channels</tt>. For example:<tt>&lt;agent name&gt;.sources.&lt;source name&gt;.channels =&lt;channel-1 name&gt; &lt;channel-2 name&gt;</tt></li><li>For each source named in the <tt>&lt;agent name&gt;.sources</tt>, theremust be a <tt>runner</tt> namespace of configuration that configures theassociated source runner. For example:<tt>&lt;agent name&gt;.sources.&lt;source name&gt;.runner.type = avro</tt>.This namespace can also be used to configure other configuration of thesource runner as needed. For example:<tt>&lt;agent name&gt;.sources.&lt;source name&gt;.runner.port = 10101</tt></li><li>For each source named in <tt>&lt;sources&gt;.sources</tt> there canbe an optional <tt>selector.type</tt> specified that identifies the typeof channel selector associated with the source. If not specified, thedefault replicating channel selector is used.</li><li>For each channel named in the <tt>&lt;agent name&gt;.channels</tt>,there must be a non-empty <tt>type</tt> attribute specified from the validset of channel types. For example:<tt>&lt;agent name&gt;.channels.&lt;channel name&gt;.type = mem</tt></li><li>For each sink named in the <tt>&lt;agent name&gt;.sinks</tt>, there mustbe a non-empty <tt>type</tt> attribute specified from the valid set of sink<tt>&lt;agent name&gt;.sinks.&lt;sink name&gt;.type = hdfs</tt></li>be a non-empty single-valued channel name specified as the value of the<tt>channel</tt> attribute. This value must be contained in the channels listspecified by <tt>&lt;agent name&gt;.channels</tt>. For example:<tt>&lt;agent name&gt;.sinks.&lt;sink name&gt;.channel =&lt;channel name&gt;</tt></li>be a <tt>runner</tt> namespace of configuration that configures theassociated sink runner. For example:<tt>&lt;agent name&gt;.sinks.&lt;sink name&gt;.runner.type = polling</tt>.This namespace can also be used to configure other configuration of the sinkrunner as needed. For example:<tt>&lt;agent name&gt;.sinks.&lt;sink name&gt;.runner.polling.interval =60</tt></li><li>A fourth optional list <tt>&lt;agent name&gt;.sinkgroups</tt>may be added to each agent, consisting of unique space separated namesfor groups</li><li>Each sinkgroup must specify sinks, containing a list of all sinksbelonging to it. These cannot be shared by multiple groups.Further, one can set a processor and behavioral parameters to determinehow sink selection is made via <tt>&lt;agent name&gt;.sinkgroups.&lt;group name&lt.processor</tt>. For further detail refer to individual processordocumentation</li><li>Sinks not assigned to a group will be assigned to default single sinkgroups.</li>Apart from the above required configuration values, each source, sink orchannel can have its own set of arbitrary configuration as required by theimplementation. Each of these configuration values are expressed by fullynamespace qualified configuration keys. For example, the configurationproperty called <tt>capacity</tt> for a channel called <tt>ch1</tt> for theagent named <tt>host1</tt> with value <tt>1000</tt> will be expressed as:<tt>host1.channels.ch1.capacity = 1000</tt>.Any information contained in the configuration file other than what pertainsto the configured agents, sources, sinks and channels via the explicitlyenumerated list of sources, sinks and channels per agent name are ignored bythis provider. Moreover, if any of the required configuration values are notpresent in the configuration file for the configured entities, that entityand anything that depends upon it is considered invalid and consequently notconfigured. For example, if a channel is missing its <tt>type</tt> attribute,it is considered misconfigured. Also, any sources or sinks that depend uponthis channel are also considered misconfigured and not initialized.Example configuration file:## Flume Configuration# This file contains configuration for one Agent identified as host1.host1.sources = avroSource thriftSourcehost1.channels = jdbcChannelhost1.sinks = hdfsSink# avroSource configurationhost1.sources.avroSource.type = org.apache.flume.source.AvroSourcehost1.sources.avroSource.runner.type = avrohost1.sources.avroSource.runner.port = 11001host1.sources.avroSource.channels = jdbcChannelhost1.sources.avroSource.selector.type = replicating# thriftSource configurationhost1.sources.thriftSource.type = org.apache.flume.source.ThriftSourcehost1.sources.thriftSource.runner.type = thrifthost1.sources.thriftSource.runner.port = 12001host1.sources.thriftSource.channels = jdbcChannel# jdbcChannel configurationhost1.channels.jdbcChannel.type = jdbchost1.channels.jdbcChannel.jdbc.driver = com.mysql.jdbc.Driverhost1.channels.jdbcChannel.jdbc.connect.url = http://localhost/flumedbhost1.channels.jdbcChannel.jdbc.username = flumehost1.channels.jdbcChannel.jdbc.password = flume# hdfsSink configurationhost1.sinks.hdfsSink.type = hdfshost1.sinks.hdfsSink.hdfs.path = hdfs://localhost/host1.sinks.hdfsSink.batchsize = 1000host1.sinks.hdfsSink.runner.type = pollinghost1.sinks.hdfsSink.runner.polling.interval = 60java.util.Properties#load(java.io.Reader)Use UriConfigurationProvider./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/SimpleMaterializedConfiguration.javasinkRunners/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/SimpleMaterializedConfiguration.classsourceRunnersHashMap<String,SourceRunner>()AbstractMap<String,SourceRunner>()Node<String,SourceRunner>TreeNode<String,SourceRunner>newTreeNode(int,java.lang.String,org.apache.flume.SourceRunner,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.flume.SourceRunner,java.util.HashMap.Node)Node<String,SourceRunner>[]putVal(int,java.lang.String,org.apache.flume.SourceRunner,boolean,boolean)HashMap<String,SourceRunner>(java.util.Map)HashMap<String,SourceRunner>(int)HashMap<String,SourceRunner>(int,float)HashMap<String,SinkRunner>()AbstractMap<String,SinkRunner>()Node<String,SinkRunner>TreeNode<String,SinkRunner>newTreeNode(int,java.lang.String,org.apache.flume.SinkRunner,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.flume.SinkRunner,java.util.HashMap.Node)Node<String,SinkRunner>[]putVal(int,java.lang.String,org.apache.flume.SinkRunner,boolean,boolean)HashMap<String,SinkRunner>(java.util.Map)HashMap<String,SinkRunner>(int)HashMap<String,SinkRunner>(int,float)"{ sourceRunners:"{ sourceRunners:" sinkRunners:" sinkRunners:" channels:" channels:ImmutableMap<String,Channel>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/11.0.2/guava-11.0.2.jar/com/google/common/collect/ImmutableMap.classImmutableMap<String,SourceRunner>ImmutableMap<String,SinkRunner>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/StaticZooKeeperConfigurationProvider.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/StaticZooKeeperConfigurationProvider.classClass<StaticZooKeeperConfigurationProvider>cforg.apache.curator.framework.apiPathable<byte[]>/Users/burakyetistiren/.m2/repository/org/apache/curator/curator-framework/5.1.0/curator-framework-5.1.0.jar/org/apache/curator/framework/api/Users/burakyetistiren/.m2/repository/org/apache/curator/curator-framework/5.1.0/curator-framework-5.1.0.jar/org/apache/curator/framework/api/Pathable.classforPathforPath(java.lang.String)"Error getting configuration info from Zookeeper"Error getting configuration info from Zookeeper/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/UriConfigurationProvider.javagetBackupFilegetBackupFile(java.io.File,java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/UriConfigurationProvider.classClass<UriConfigurationProvider>sourceListwatcherRunnable"conf-file-poller-%d"conf-file-poller-%d"File watcher has not terminated. Forcing shutdown of executor."File watcher has not terminated. Forcing shutdown of executor."Waiting for file watcher to terminate"Waiting for file watcher to terminate"Interrupted while waiting for file watcher to terminate"Interrupted while waiting for file watcher to terminate"File extension type {} is unsupported"File extension type {} is unsupported"Unable to load properties from {}: {}"Unable to load properties from {}: {}"Created directories for {}"Created directories for {}"Backup created at "Backup created at "Unable to create backup properties file: {}"Unable to create backup properties file: {}backup"Unable to access primary configuration. Trying backup"Unable to access primary configuration. Trying backup"Error reading backup file: {}"Error reading backup file: {}"No configuration could be found"No configuration could be found".properties".properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/UriConfigurationProvider$WatcherRunnable.class"Checking for changes to sources"Checking for changes to sourcesisModified"Unable to update configuration: {}"Unable to update configuration: {}A configuration provider that uses properties for specifyingconfiguration. The configurations follow the Java properties file syntaxrules specified at {@link Properties#load(java.io.Reader)}. Everyconfiguration value specified in the properties is prefixed by anValid configurations must observe the following rules for every agentProperties#load(java.io.Reader) This is only being logged to keep Spotbugs happy. We can't ignore the result of mkdirs./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/net/AuthorizationProvider.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/netorg.apache.flume.node.neturlConnectionInterface to be implemented to add an Authorization header to an HTTP request./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/net/BasicAuthorizationProvider.javaauthString/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/net/BasicAuthorizationProvider.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/netuserNamepasswordtoEncode"Basic "Basic "Authorization"AuthorizationProvides the Basic Authorization header to a request./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/net/LaxHostnameVerifier.javaLaxHostnameVerifierLaxHostnameVerifier()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/net/LaxHostnameVerifier.classsslSessionAn HostnameVerifier which accepts everything.Singleton instance./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/main/java/org/apache/flume/node/net/UrlConnectionFactory.javagetContentTypegetContentType(java.net.URL)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/target/classes/org/apache/flume/node/net/UrlConnectionFactory.classPROPERTIESJSONYAMLXMLreadTimeoutMillisconnectTimeoutMillisDEFAULT_TIMEOUT"application/xml"application/xml"application/yaml"application/yaml"application/json"application/json"text/x-java-properties"text/x-java-properties"text/plain"text/plainlastModifiedMillis"GET"GET"Content-Type"Content-Typezdt"If-Modified-Since"If-Modified-SincefileParts"yml"ymlConstructs an HTTPURLConnection./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/test/resources/flume-conf-init.properties/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/test/resources/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/testhost1.sourcessource1 processedSourcehost1.channelschannel1 processedChannelhost1.sinkssink1host1.sources.source1.typeorg.apache.flume.source.LocalSourcehost1.sources.source1.channelschannel1host1.sources.source1.totalEventshost1.sources.source1.backoffSleepIncrementhost1.sources.processedSource.typehost1.sources.processedSource.channelsprocessedChannelhost1.channels.channel1.typememoryhost1.channels.channel1.capacityhost1.channels.processedChannel.typehost1.channels.processedChannel.capacityhost1.sinks.sink1.typeorg.apache.flume.sink.NullInitSinkhost1.sinks.sink1.batchSizehost1.sinks.sink1.targetSourceprocessedSourcehost1.sinks.sink1.channel/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/test/resources/flume-conf-override.propertiesPROD_BIND192.168.12.110DEV_BIND192.168.13.101/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/test/resources/flume-conf-with-envLookup.propertiesa1.sourcesa1.sources.r1.typenetcata1.sources.r1.binda1.sources.r1.port${env:NC_PORT}a1.sources.r1.channelsa1.channelsa1.channels.c1.typea1.channels.c1.capacitya1.channels.c1.transactionCapacitya1.channels.c1.byteCapacityBufferPercentagea1.channels.c1.byteCapacity800000a1.sinksk1a1.sinks.k1.typea1.sinks.k1.channel/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/test/resources/flume-conf-with-envvars.properties${NC_PORT}/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/test/resources/flume-conf-with-recursiveLookup.properties192.168.10.110192.168.11.101${${sys:env}_BIND}6667/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/test/resources/flume-conf.propertiessource1seqhost2.sourcessrc1host2.sinkshost2.sources.src1.typefoohost2.sources.src1.runnerxxxhost2.sources.src1.runner.typettthost2.sinks.sink1.typebarhost2.sinks.sink1.runneryyyhost2.sinks.sink1.runner.typehost3.sourcessrc1 src2host3.channelsch1 ch2host3.sources.src1.typehost3.sources.src1.runner.typehost3.sources.src1.channelsch1 ch3host3.channels.ch2.foohost3.channels.ch3.typehost3.channels.ch3.xxxhost4.sourcessrc2host4.channelshost4.sources.src1.typehost4.sources.src1.runner.typehost4.sources.src1.channelshost4.channels.ch2.foohost4.channels.ch2.typeabchost4.channels.ch3.typehost4.channels.ch3.xxxhost5.sourceshost5.channelsch1host5.sinkssink1 sink2 sink3host5.sinkgroupssg1host5.channels.ch1.typehost5.sources.src1.typedefhost5.sources.src1.channelshost5.sinks.sink1.typehost5.sinks.sink1.channelhost5.sinks.sink2.typehost5.sinks.sink2.channelhost5.sinkgroups.sg1.sinkshost5.sinkgroups.sg1.policy.typefailoverhost5.sinkgroups.sg1.policy.priority.sink1host5.sinkgroups.sg1.policy.priority.sink2/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/test/resources/log4j2.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/test/resources/map-resolver.properties${sys:name}const${const:org.apache.flume.node.TestMapResolver.TEST_CONST}${java:version}${test:Value}/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-node/src/test/resources/test-lookups.propertiesorg.apache.commons.text.lookup.DefaultStringLookup.SYSTEM_PROPERTIESorg.apache.commons.text.lookup.DefaultStringLookup.ENVIRONMENTorg.apache.commons.text.lookup.DefaultStringLookup.JAVAorg.apache.flume.node.lookup.TestLookup/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/pom.xmlFlume NG SDKFlume Software Development Kit: Stable public API for integration with Flume 1.x69170org.apache.flume.sdkcompileThriftactiveByDefaultechotarget/compile-thrift.sh
                      LICENSE=src/main/thrift/aslv2
                      THRIFT_DIR=src/main/thrift
                      JAVA_DIR=target/generated-sources/thrift
                      mkdir -p $JAVA_DIR 2> /dev/null
                      JSTATUS=$?
                      if [ $JSTATUS -ne 0 ] ; then
                        echo "Could not create $JAVA_DIR. Will not generate thrift files."
                        exit $JSTATUS
                      fi
                      for THRIFT_FILE in `ls $THRIFT_DIR/*.thrift 2> /dev/null`
                      do
                        thrift --gen java:hashcode -o $JAVA_DIR $THRIFT_FILE
                      done
                      SRC_DIR=$JAVA_DIR/gen-java/org/apache/flume/thrift/
                      DEST_DIR=src/main/java/org/apache/flume/thrift
                      if [ ! -d $DEST_DIR ] ; then
                        mkdir $DEST_DIR 2> /dev/null
                        STATUS=$?
                        if [ $STATUS -ne 0 ] ; then
                          echo "Could not create $DEST_DIR. Will not generate thrift files."
                          exit $STATUS
                        fi
                      fi
                      for JAVA_FILE in `ls $SRC_DIR/*.java 2> /dev/null`
                      do
                        echo $JAVA_FILE
                        cat $LICENSE > $JAVA_FILE.tmp
                        cat $JAVA_FILE >> $JAVA_FILE.tmp
                        mv $JAVA_FILE.tmp $JAVA_FILE
                        cp $JAVA_FILE $DEST_DIR
                      done
                      rm -rf $JAVA_DIR
                    ${basedir}failonerrorstringTypeCharSequencemaven-jar-plugintest-jarlog4j-jul/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/Event.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/srcBasic representation of a data object in Flume.Provides access to data as it flows through the system.Returns a map of name-value pairs describing the data stored in the body.Set the event headersMap of headers to replace the current headers.Returns the raw byte array of the data contained in this event.Sets the raw byte array of the data contained in this event.The data./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/EventDeliveryException.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/EventDeliveryException.class1102327497549834945L1102327497549834945An event delivery exception is raised whenever an {@link Event} fails toreach at least one of its intended (next-hop) destinations./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/FlumeException.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/FlumeException.classBase class of all flume exceptions./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/api/AbstractRpcClient.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/api/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/AbstractRpcClient.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/apiClass<AbstractRpcClient>strBatchSize"Batch size string = "Batch size string = parsedBatch"Invalid value for batchSize: {}; Using default value."Invalid value for batchSize: {}; Using default value."BatchSize is not valid for RpcClient: "BatchSize is not valid for RpcClient: ". Default value assigned.". Default value assigned.Configure the client using the given properties object.if the client can not be configured using thismethod, or if the client was already configured once.This is to parse the batch size config for rpc clientsbatch size/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/api/FailoverRpcClient.javasetDefaultPropertiessetDefaultProperties(org.apache.flume.api.HostInfo,java.util.Properties)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/FailoverRpcClient.classgetNextClientgetNextClient()getClientgetClient()configureHostsconfigureHosts(java.util.Properties)configurationPropertiesisActivelastCheckedhostmaxTriesClass<FailoverRpcClient>tries"This client was already configured, " +
          "cannot reconfigure."This client was already configured, cannot reconfigure.localClient"Attempting to append to an already closed client."Attempting to append to an already closed client."Client failed. Exception follows: "Client failed. Exception follows: "Failed to send event: "Failed to send event: "Failed to send event. Exception follows: "Failed to send event. Exception follows: "Tried many times, could not send event."Tried many times, could not send event."Failed to send the event!"Failed to send the event!"Attempting to append to an already closed client!"Attempting to append to an already closed client!"No clients active: "No clients active: "No clients currently active. " +
            "Exception follows: "No clients currently active. Exception follows: hostInfo"Could not connect to "Could not connect to "No active client found."No active client found."No active client."No active client.Avro/Netty implementation of {@link RpcClient} which supports failover. Thistakes a list of hostname port combinations and connects to the next available(looping back to the first) host, from a given list of agents in the orderprovided.The properties used to build a FailoverRpcClient must have:<p><tt>hosts</tt> = <i>alias_for_host1</i> <i>alias_for_host2</i></p> ...<p><tt>hosts.alias_for_host1</tt> = <i>hostname1:port1</i>. </p><p><tt>hosts.alias_for_host2</tt> = <i>hostname2:port2</i>. </p> etc<p>Optionally it can also have a <p><tt>batch-size</tt> = <i>batchSize</i><tt>max-attempts</tt> = <i>maxAttempts</i>Given a failure, this client will attempt to append to <i>maxAttempts</i>clients in the <i>hosts</i> list immediately following the failed host(looping back to the beginning of the <i>hosts</i> list.This function has to be synchronized to establish a happens-beforerelationship for different threads that access this objectsince shared data structures are created here.Get the maximum number of "failed" hosts the client will try to establishconnection to before throwing an exception. Failed = was able to set up aconnection, but failed / returned error when the client tried to send data,The maximum number of failed retriesTries to append an event to the currently connected client. If it cannotsend the event, it tries to send to next available hostThe event to be appended.Why a local variable rather than just calling getClient()?If we get an EventDeliveryException, we need to call close onthat specific client, getClient in this case, will get usthe next client - leaving a resource leak. Sit in an infinite loop and try to append! Could not send event through this client, try to pick another client.Tries to append a list of events to the currently connected client. If itcannot send the event, it tries to send to next available hostThe events to be appended. Returns false if and only if this client has been closed explicitly. Should we check if any clients are active, if none are then return false? This method has to be lightweight, so not checking if hosts are active.Close the connection. This function is safe to call over and over.Get the last socket address this client connected to. No guarantee thiswill be the next it will connect to. If this host is down, it will connectto another host. To be used only from the unit tests!The last socket address this client connected toTry to connect to all hosts again, till we find one available This return should never be reached!/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/api/HostInfo.javaportNumber/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/HostInfo.classreferenceNameClass<HostInfo>hostListhostAndPortStr"Invalid host address"Invalid host address"Invalid port number"Invalid port numberA convenience class that holds the property reference name along with thehostname and port number of a specified host address. It also providesa method to parse out the host list from a given properties objectthat contains the host details. Ignore that host if value is not there/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/api/LoadBalancingRpcClient.javagetClientConfigurationPropertiesgetClientConfigurationProperties(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/LoadBalancingRpcClient.classcreateClientcreateClient(java.lang.String)getClient(org.apache.flume.api.HostInfo)throwIfClosedthrowIfClosed()Map<String,RpcClient>clientMapClass<LoadBalancingRpcClient>eventSent"Failed to send event to host "Failed to send event to host "Unable to send event to any host"Unable to send event to any hostbatchSent"Failed to send batch to host "Failed to send batch to host "Unable to send batch to any host"Unable to send batch to any host"Rpc Client is closed"Rpc Client is closed? super RpcClient? extends RpcClientBiFunction<? super RpcClient,? super RpcClient,? extends RpcClient>merge(java.lang.String,org.apache.flume.api.RpcClient,java.util.function.BiFunction)BiFunction<? super String,? super RpcClient,? extends RpcClient>Function<? super String,? extends RpcClient>replace(java.lang.String,org.apache.flume.api.RpcClient)replace(java.lang.String,org.apache.flume.api.RpcClient,org.apache.flume.api.RpcClient)putIfAbsent(java.lang.String,org.apache.flume.api.RpcClient)BiConsumer<? super String,? super RpcClient>getOrDefault(java.lang.Object,org.apache.flume.api.RpcClient)Entry<String,RpcClient>Set<Entry<String,RpcClient>>Collection<Entry<String,RpcClient>>Iterable<Entry<String,RpcClient>>Collection<RpcClient>Iterable<RpcClient>Map<? extends String,? extends RpcClient>put(java.lang.String,org.apache.flume.api.RpcClient)"Failed to close client: "Failed to close client: lbTypeNamemaxBackoffStrHashMap<String,RpcClient>AbstractMap<String,RpcClient>HashMap<String,RpcClient>()AbstractMap<String,RpcClient>()Node<String,RpcClient>TreeNode<String,RpcClient>newTreeNode(int,java.lang.String,org.apache.flume.api.RpcClient,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.flume.api.RpcClient,java.util.HashMap.Node)Node<String,RpcClient>[]putVal(int,java.lang.String,org.apache.flume.api.RpcClient,boolean,boolean)HashMap<String,RpcClient>(java.util.Map)HashMap<String,RpcClient>(int)HashMap<String,RpcClient>(int,float)"At least two hosts are required to use the "
          + "load balancing RPC client."At least two hosts are required to use the load balancing RPC client.? extends HostSelectorClass<? extends HostSelector>Map<String,? extends HostSelector>HostSelector[]Constructor<? extends HostSelector>TypeVariable<Class<? extends HostSelector>>TypeVariable<Class<? extends HostSelector>>[]"Unable to instantiate host selector: "Unable to instantiate host selector: "Failed to close client for "Failed to close client for failedHostRoundRobinHostSelectorRoundRobinHostSelector(boolean,long)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/LoadBalancingRpcClient$RoundRobinHostSelector.classOrderSelector<HostInfo>RoundRobinOrderSelector<HostInfo>RoundRobinOrderSelector<HostInfo>(boolean)informFailure(org.apache.flume.api.HostInfo)OrderSelector<HostInfo>(boolean)RandomOrderHostSelectorRandomOrderHostSelector(boolean,java.lang.Long)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/LoadBalancingRpcClient$RandomOrderHostSelector.classRandomOrderSelector<HostInfo>RandomOrderSelector<HostInfo>(boolean)<p>An implementation of RpcClient interface that uses NettyAvroRpcClientinstances to load-balance the requests over many different hosts. Thisimplementation supports a round-robin scheme or random scheme of doingload balancing over the various hosts. To specify round-robin scheme setthe value of the configuration property <tt>load-balance-type</tt> to<tt>round_robin</tt>. Similarly, for random scheme this value should beset to <tt>random</tt>, and for a custom scheme the full class name ofthe class that implements the <tt>HostSelector</tt> interface.This implementation also performs basic failover in case the randomlyselected host is not available for receiving the event.A host selector that implements the round-robin host selection policy./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/api/NettyAvroRpcClient.javacreateSSLEnginecreateSSLEngine()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/NettyAvroRpcClient.classtoCharSeqMaptoCharSeqMap(java.util.Map)assertReadyassertReady()setStatesetState(org.apache.flume.api.NettyAvroRpcClient.ConnState)org.apache.avro.ipcCallFuture<Status>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro-ipc/1.11.1/avro-ipc-1.11.1.jar/Users/burakyetistiren/.m2/repository/org/apache/avro/avro-ipc/1.11.1/Users/burakyetistiren/.m2/repository/org/apache/avro/avro-ipc/Users/burakyetistiren/.m2/repository/org/apache/avro/avro-ipc/1.11.1/avro-ipc-1.11.1.jar/org/Users/burakyetistiren/.m2/repository/org/apache/avro/avro-ipc/1.11.1/avro-ipc-1.11.1.jar/org/apache/Users/burakyetistiren/.m2/repository/org/apache/avro/avro-ipc/1.11.1/avro-ipc-1.11.1.jar/org/apache/avro/Users/burakyetistiren/.m2/repository/org/apache/avro/avro-ipc/1.11.1/avro-ipc-1.11.1.jar/org/apache/avro/ipc/Users/burakyetistiren/.m2/repository/org/apache/avro/avro-ipc/1.11.1/avro-ipc-1.11.1.jar/org/apache/avro/ipc/CallFuture.classFuture<Status>Callback<Status>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro-ipc/1.11.1/avro-ipc-1.11.1.jar/org/apache/avro/ipc/Callback.classwaitForStatusOKwaitForStatusOK(org.apache.avro.ipc.CallFuture,long,java.util.concurrent.TimeUnit)appendBatchappendBatch(java.util.List,long,java.util.concurrent.TimeUnit)append(org.apache.flume.Event,long,java.util.concurrent.TimeUnit)connectconnect(long,java.util.concurrent.TimeUnit)connect()compressionLevelenableDeflateCompressionavroClienttransceiverconnStatestateLockcallTimeoutPoolClass<NettyAvroRpcClient>tu"Flume Avro RPC Client Call Invoker"Flume Avro RPC Client Call InvokerClass<Callback>": RPC connection error": RPC connection error": Unexpected exception": Unexpected exception": Unable to cleanly shut down call timeout pool": Unable to cleanly shut down call timeout pool": Interrupted during close": Interrupted during close": Error closing transceiver.": Error closing transceiver."NettyAvroRpcClient { host: "NettyAvroRpcClient { host: ": Failed to send event. " +
            "RPC request timed out after ": Failed to send event. RPC request timed out after ": Failed to send event": Failed to send eventcallFutureCallFuture<Status>()handleErrorhandleError(java.lang.Throwable)handleResulthandleResult(java.lang.Object)handleResult(org.apache.flume.source.avro.Status)awaitawait(long,java.util.concurrent.TimeUnit)await()getErrorgetError()getResultgetResult()Callback<T>CallFutureCallFuture(org.apache.avro.ipc.Callback)CallFuture<Status>(org.apache.avro.ipc.Callback)CallFuture()Future<Void>handshake/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/NettyAvroRpcClient$1.classCallable<Void>": Executor error": Executor error": Handshake timed out after ": Handshake timed out after " ms" ms": Interrupted in handshake": Interrupted in handshake": RPC request exception": RPC request exception": RPC request cancelled": RPC request cancelled": Failed to send batch": Failed to send batchavroEventsLinkedList<AvroFlumeEvent>AbstractSequentialList<AvroFlumeEvent>AbstractList<AvroFlumeEvent>AbstractCollection<AvroFlumeEvent>Deque<AvroFlumeEvent>Queue<AvroFlumeEvent>LinkedList<AvroFlumeEvent>()AbstractCollection<AvroFlumeEvent>()AbstractList<AvroFlumeEvent>()AbstractSequentialList<AvroFlumeEvent>()offer(org.apache.flume.source.avro.AvroFlumeEvent)push(org.apache.flume.source.avro.AvroFlumeEvent)offerLast(org.apache.flume.source.avro.AvroFlumeEvent)offerFirst(org.apache.flume.source.avro.AvroFlumeEvent)Node<AvroFlumeEvent>linkBefore(org.apache.flume.source.avro.AvroFlumeEvent,java.util.LinkedList.Node)linkLast(org.apache.flume.source.avro.AvroFlumeEvent)LinkedList<AvroFlumeEvent>(java.util.Collection)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/NettyAvroRpcClient$2.class": Avro RPC call returned Status: ": Avro RPC call returned Status: ": RPC future was cancelled": RPC future was cancelled": Exception thrown from remote handler": Exception thrown from remote handler": RPC request timed out": RPC request timed out": RPC request interrupted": RPC request interruptednewState"Cannot transition from CLOSED state."Cannot transition from CLOSED state.curState"RPC failed, client in an invalid state: "RPC failed, client in an invalid state: HashMap<CharSequence,CharSequence>AbstractMap<CharSequence,CharSequence>HashMap<CharSequence,CharSequence>()AbstractMap<CharSequence,CharSequence>()Node<CharSequence,CharSequence>TreeNode<CharSequence,CharSequence>newTreeNode(int,java.lang.CharSequence,java.lang.CharSequence,java.util.HashMap.Node)newNode(int,java.lang.CharSequence,java.lang.CharSequence,java.util.HashMap.Node)Node<CharSequence,CharSequence>[]putVal(int,java.lang.CharSequence,java.lang.CharSequence,boolean,boolean)HashMap<CharSequence,CharSequence>(java.util.Map)HashMap<CharSequence,CharSequence>(int)HashMap<CharSequence,CharSequence>(int,float)ConnStateConnState()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/NettyAvroRpcClient$ConnState.classstrConnTimeoutstrReqTimeoutenableCompressionStrmaxIoWorkersStr"This client was already configured, cannot reconfigure.""Hosts list is invalid: "Hosts list is invalid: "More than one hosts are specified for the default client. "
          + "Only the first host will be used and others ignored. Specified: "More than one hosts are specified for the default client. Only the first host will be used and others ignored. Specified: "; to be used: "; to be used: "Host not found: "Host not found: "Invalid hostname: "Invalid hostname: "Invalid Port: "Invalid Port: "Connection timeout specified less than 1s. Using default value instead."Connection timeout specified less than 1s. Using default value instead."Invalid connect timeout specified: "Invalid connect timeout specified: "Request timeout specified less than 1s. Using default value instead."Request timeout specified less than 1s. Using default value instead."Invalid request timeout specified: "Invalid request timeout specified: compressionLvlStr"Invalid compression level: "Invalid compression level: "Specifying the number of workers is no longer supported"Specifying the number of workers is no longer supported/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/NettyAvroRpcClient$TransceiverThreadFactory.classthreadIdthreadmanagers"No truststore configured, setting TrustManager to accept all server certificates"No truststore configured, setting TrustManager to accept all server certificatestmf"SunX509"SunX509truststoreStreamsuite"SSLEngine protocols enabled: "SSLEngine protocols enabled: "SSLEngine cipher suites enabled: "SSLEngine cipher suites enabled: "Cannot create SSL channel"Cannot create SSL channelPermissiveTrustManagerPermissiveTrustManager()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/NettyAvroRpcClient$PermissiveTrustManager.classX509Certificate[]certsAvro/Netty implementation of {@link RpcClient}.The connections are intended to be opened before clients are given access sothat the object cannot ever be in an inconsistent when exposed to users.Guarded by {@code stateLock}This constructor is intended to be called from {@link RpcClientFactory}.A call to this constructor should be followed by call to configure().This method should only be invoked by the build functionInternal only, for now re-cancel if current thread also interrupted preserve interrupt status we mark as no longer active without trying to clean up resources client is required to call close() to clean up resources due to AVRO-1122, avroClient.append() may block send multiple batches... bail if there is a problem at any time due to AVRO-1122, avroClient.appendBatch() may blockHelper method that waits for a Status future to come back and validatesthat it returns Status == OK.Future to wait onTime to wait before failingTime Unit of {@code timeout}If there is a timeout or if Status != OKThis method should always be used to change {@code connState} so we ensurethat invalid state transitions do not occur and that the {@code isIdle}{@link Condition} variable gets signaled reliably.Throws {@code IllegalStateException} when called to transition from CLOSEDto another state.If the connection state != READY, throws {@link EventDeliveryException}.Helper function to convert a map of String to a map of CharSequence.Configure the actual client using the properties.<tt>properties</tt> should have at least 2 params:<p><tt>hosts</tt> = <i>alias_for_host</i></p><p><tt>alias_for_host</tt> = <i>hostname:port</i>. </p>Only the first host is added, rest are discarded.</p>The properties to instantiate the client with. host and port connect timeout request timeoutA thread factor implementation modeled after the implementation ofNettyTransceiver.NettyTransceiverThreadFactory class which isa private static class. The only difference between that and thisimplementation is that this implementation marks all the threads daemonwhich allows the termination of the VM when the non-daemon threadsare done.Creates a TransceiverThreadFactory that creates threads with thespecified name.the name prefix to use for all threads created by thisThreadFactory.  A unique ID will be appended to this prefix to form thefinal thread name. null keystore is OK, with SunX509 it defaults to system CA Certs see http://docs.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html#X509TrustManagerPermissive trust manager accepting any certificate nothing/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/api/RpcClient.java<p>Public client interface for sending data to Flume.</p><p>This interface is intended not to change incompatibly for Flume 1.x.</p><p><strong>Note:</strong> It is recommended for applications to construct{@link RpcClient} instances using the {@link RpcClientFactory} class,instead of using builders associated with a particular implementation class.org.apache.flume.api.RpcClientFactoryReturns the maximum number of {@link Event events} that may be batchedat once by {@link #appendBatch(List) appendBatch()}.<p>Send a single {@link Event} to the associated Flume source.</p><p>This method blocks until the RPC returns or until the request times out.<p><strong>Note:</strong> If this method throws an{@link EventDeliveryException}, there is no way to recover and theapplication must invoke {@link #close()} on this object to clean up systemresources.</p>when an error prevents event delivery.<p>Send a list of {@linkplain Event events} to the associated Flume source.<p>It is strongly recommended that the number of events in the List be nomore than {@link #getBatchSize()}. If it is more, multiple RPC calls willbe required, and the likelihood of duplicate Events being stored willincrease.</p>List of events to send<p>Returns {@code true} if this object appears to be in a usable state, andit returns {@code false} if this object is permanently disabled.</p><p>If this method returns {@code false}, an application must call{@link #close()} on this object to clean up system resources.</p><p>Immediately closes the client so that it may no longer be used.</p><p><strong>Note:</strong> This method MUST be called by applicationswhen they are done using the RPC client in order to clean up resources.</p><p>Multi-threaded applications may want to gracefully stop makingRPC calls before calling close(). Otherwise, they risk getting{@link EventDeliveryException} thrown from their in-flight calls when theunderlying connection is disabled.</p>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/api/RpcClientConfigurationConstants.javaRpcClientConfigurationConstantsRpcClientConfigurationConstants()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/RpcClientConfigurationConstants.class"hosts."hosts."batch-size"batch-size"connect-timeout"connect-timeout"request-timeout"request-timeout"max-attempts"max-attempts"client.type"client.type"host-selector"host-selector"maxConnections"maxConnections"compression-level"compression-level"trust-all-certs"trust-all-certs"truststore"truststore"truststore-password"truststore-password"truststore-type"truststore-type"maxIoWorkers"maxIoWorkersConfiguration constants used by the RpcClient. These configuration keyscan be specified via a Properties object to the appropriate method ofRpcClientFactory in order to obtain a customized RPC client.Hosts configuration key to specify a space delimited list of namedhosts. For example:hosts = h1 h2Hosts prefix to specify address of a particular named host. For examplehosts.h1 = server1.example.com:12121hosts.h2 = server2.example.com:12121Configuration key used to specify the batch size. Default batch size is{@value DEFAULT_BATCH_SIZE}.Configuration key to specify connection timeout in milliseconds. Thedefault connection timeout is {@value DEFAULT_CONNECT_TIMEOUT_MILLIS}.Configuration key to specify request timeout in milliseconds. Thedefault request timeout is {@value DEFAULT_REQUEST_TIMEOUT_MILLIS}.Default batch size.Default connection, handshake, and initial request timeout in milliseconds.Default request timeout in milliseconds.Maximum attempts to be made by the FailoverRpcClient in case offailures.Configuration key to specify the RpcClient type to be used. The availablevalues are <tt>DEFAULT</tt> which results in the creation of a regular<tt>NettyAvroRpcClient</tt> and <tt>DEFAULT_FAILOVER</tt> which resultsin the creation of a failover client implementation on top of multiple<tt>NettyAvroRpcClient</tt>s. The default value of this configurationis {@value #DEFAULT_CLIENT_TYPE}.The default client type to be created if no explicit type is specified.The selector type used by the <tt>LoadBalancingRpcClient</tt>. Thisvalue of this setting could be either <tt>round_robin</tt>,<tt>random</tt>, or the fully qualified name class that implements the<tt>LoadBalancingRpcClient.HostSelector</tt> interface.Maximum number of connections each Thrift Rpc client can open to a givenhost.The following are const for the NettyAvro Client.  To enable compressionand set a compression levelConfiguration constants for SSL supportConfiguration constants for the NettyAvroRpcClientNioClientSocketChannelFactory/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/api/RpcClientFactory.java? extends AbstractRpcClientClass<? extends AbstractRpcClient>clientClassTypeclientType"No such client!"No such client!Map<String,? extends AbstractRpcClient>AbstractRpcClient[]Constructor<? extends AbstractRpcClient>TypeVariable<Class<? extends AbstractRpcClient>>TypeVariable<Class<? extends AbstractRpcClient>>[]"Cannot instantiate client. " +
          "Exception follows:"Cannot instantiate client. Exception follows:propertiesFile"hostname must not be null"hostname must not be null"port must not be null"port must not be null"batchSize must not be null"batchSize must not be nullClientTypeClientType(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/RpcClientFactory$ClientType.classclientClassNameMap<String,NettyAvroRpcClient>NettyAvroRpcClient[]Constructor<NettyAvroRpcClient>? super NettyAvroRpcClientClass<? super NettyAvroRpcClient>TypeVariable<Class<NettyAvroRpcClient>>TypeVariable<Class<NettyAvroRpcClient>>[]Map<String,FailoverRpcClient>FailoverRpcClient[]Constructor<FailoverRpcClient>? super FailoverRpcClientClass<? super FailoverRpcClient>TypeVariable<Class<FailoverRpcClient>>TypeVariable<Class<FailoverRpcClient>>[]Map<String,LoadBalancingRpcClient>LoadBalancingRpcClient[]Constructor<LoadBalancingRpcClient>? super LoadBalancingRpcClientClass<? super LoadBalancingRpcClient>TypeVariable<Class<LoadBalancingRpcClient>>TypeVariable<Class<LoadBalancingRpcClient>>[]Class<ThriftRpcClient>Map<String,ThriftRpcClient>ThriftRpcClient[]Constructor<ThriftRpcClient>? super ThriftRpcClientClass<? super ThriftRpcClient>TypeVariable<Class<ThriftRpcClient>>TypeVariable<Class<ThriftRpcClient>>[]Returns an instance of {@link RpcClient}, optionally with failover.To create a failover client, the properties object should have aproperty <tt>client.type</tt> which has the value "failover". The clientconnects to hosts specified by <tt>hosts</tt> property in given properties.org.apache.flume.api.FailoverRpcClientIf no <tt>client.type</tt> is specified, a default client that connects tosingle host at a given port is created.(<tt>type</tt> can also simply be<tt>DEFAULT</tt> for the default client).org.apache.flume.api.NettyAvroClientDelegates to {@link #getInstance(Properties props)}, given a File pathto a {@link Properties} file.Valid properties fileRpcClient configured according to the given Properties file.If the file cannot be foundIf there is an IO errorDeprecated. Use{@link #getDefaultInstance(String, Integer)} instead.Returns an instance of {@link RpcClient} connected to the specified{@code hostname} and {@code port}.{@link #getDefaultInstance(String, Integer, Integer)}instead.{@code hostname} and {@code port} with the specified {@code batchSize}.Return an {@linkplain RpcClient} that uses Thrift for communicating withthe next hop. The next hop must have a ThriftSource listening on thespecified port.- The hostname of the next hop.- The port on which the ThriftSource is listening- batch size of each transaction.an {@linkplain RpcClient} which uses thrift configured with thespecified port. This will use the default batch size. See {@linkplainRpcClientConfigurationConstants}- An {@linkplain RpcClient} which uses thrift configured with thethe next hop./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/api/SSLContextAwareAbstractRpcClient.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/SSLContextAwareAbstractRpcClient.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/api/ThriftRpcClient.javacreateSSLSocketcreateSSLSocket(javax.net.ssl.SSLSocketFactory,java.lang.String,int,int,java.util.Set,java.util.Set,java.util.Set,java.util.Set)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/ThriftRpcClient.classcreateSSLContextcreateSSLContext(java.lang.String,java.lang.String,java.lang.String)doAppendBatchdoAppendBatch(org.apache.flume.api.ThriftRpcClient.ClientWrapper,java.util.List)doAppenddoAppend(org.apache.flume.api.ThriftRpcClient.ClientWrapper,org.apache.flume.thrift.ThriftFlumeEvent)threadCounterconnectionManagerrequestTimeout/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/ThriftRpcClient$1.classnewThreadnewThread(java.lang.Runnable)"Flume Thrift RPC thread - "Flume Thrift RPC thread - destroyedClientthriftEvent"Client was closed due to error. " +
            "Please create a new client"Client was closed due to error. Please create a new client"Append call timeout"Append call timeout"Failed to send event. "Failed to send event. thriftFlumeEventsArrayList<ThriftFlumeEvent>AbstractList<ThriftFlumeEvent>AbstractCollection<ThriftFlumeEvent>ArrayList<ThriftFlumeEvent>()AbstractCollection<ThriftFlumeEvent>()AbstractList<ThriftFlumeEvent>()ArrayList<ThriftFlumeEvent>(java.util.Collection)ArrayList<ThriftFlumeEvent>(int)eventsIter"Client was closed " +
            "due to error or is not yet configured."Client was closed due to error or is not yet configured./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/ThriftRpcClient$2.class"Failed to deliver events. Server " +
              "returned status : "Failed to deliver events. Server returned status : compareTo(org.apache.flume.thrift.Status)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/ThriftRpcClient$3.class"Failed to close RPC client. "Failed to close RPC client. "Attempting to re-configured an already " +
          "configured client!"Attempting to re-configured an already configured client!connectionPoolSize"'binary' or 'compact' are the only valid Thrift protocol types to "
            + "choose from. Defaulting to 'compact'."'binary' or 'compact' are the only valid Thrift protocol types to choose from. Defaulting to 'compact'."Request timeout specified less than 1s. " +
            "Using default value instead.""Connection Pool Size specified is less than 1. " +
            "Using default value instead."Connection Pool Size specified is less than 1. Using default value instead."Error while configuring RpcClient. "Error while configuring RpcClient. /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/ThriftRpcClient$State.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/ThriftRpcClient$ClientWrapper.classsslSockFactoryavailableClientsCondition/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/api/ThriftRpcClient$ConnectionPoolManager.classpoolLockcurrentPoolSizemaxPoolSizeSet<ClientWrapper>Collection<ClientWrapper>Iterable<ClientWrapper>checkedOutClientsQueue<ClientWrapper>availableClientspoolSizeLinkedList<ClientWrapper>AbstractSequentialList<ClientWrapper>AbstractList<ClientWrapper>AbstractCollection<ClientWrapper>List<ClientWrapper>SequencedCollection<ClientWrapper>Deque<ClientWrapper>LinkedList<ClientWrapper>()Spliterator<ClientWrapper>? super ClientWrapperConsumer<? super ClientWrapper>Iterator<ClientWrapper>Stream<ClientWrapper>BaseStream<ClientWrapper,Stream<ClientWrapper>>Predicate<? super ClientWrapper>? extends ClientWrapperCollection<? extends ClientWrapper>Iterable<? extends ClientWrapper>add(org.apache.flume.api.ThriftRpcClient.ClientWrapper)AbstractCollection<ClientWrapper>()addLast(org.apache.flume.api.ThriftRpcClient.ClientWrapper)addFirst(org.apache.flume.api.ThriftRpcClient.ClientWrapper)ListIterator<ClientWrapper>add(int,org.apache.flume.api.ThriftRpcClient.ClientWrapper)set(int,org.apache.flume.api.ThriftRpcClient.ClientWrapper)Comparator<? super ClientWrapper>UnaryOperator<ClientWrapper>Function<ClientWrapper,ClientWrapper>AbstractList<ClientWrapper>()AbstractSequentialList<ClientWrapper>()offer(org.apache.flume.api.ThriftRpcClient.ClientWrapper)push(org.apache.flume.api.ThriftRpcClient.ClientWrapper)offerLast(org.apache.flume.api.ThriftRpcClient.ClientWrapper)offerFirst(org.apache.flume.api.ThriftRpcClient.ClientWrapper)Node<ClientWrapper>linkBefore(org.apache.flume.api.ThriftRpcClient.ClientWrapper,java.util.LinkedList.Node)linkLast(org.apache.flume.api.ThriftRpcClient.ClientWrapper)LinkedList<ClientWrapper>(java.util.Collection)HashSet<ClientWrapper>AbstractSet<ClientWrapper>HashSet<ClientWrapper>()AbstractSet<ClientWrapper>()HashSet<ClientWrapper>(int,float,boolean)HashSet<ClientWrapper>(int)HashSet<ClientWrapper>(int,float)HashSet<ClientWrapper>(java.util.Collection)truststorePasswordtruststoreType"Error creating the transport"Error creating the transport" on port " on port  OK to use cached threadpool, because this is simply meant to timeout the calls - and is IO bound. Thrift IPC client is not thread safe, so don't allow state changes or client.append* calls unless the lock is acquired. If destroy throws, we still don't want to reuse the client, so mark it as destroyed before we actually do.Do not release this, because this client is not to be used again check in case that garbage was put in.Failed to configure, kill the client.Wrapper around a client and transport, so we can clean up when thisclient gets closed. JDK6's factory doesn't appear to pass the protocol onto the Socket properly so we have to do some magic to make sure that happens. Not an issue in JDK7 Lifted from thrift-0.9.1 to make the SSLContext Create the factory from it Create the TSocket from that The transport is already open for SSL as part of TSSLTransportFactory.getClientSocket Not a great hash code, but since this class is immutable and there is at most one instance of the components of this class, this works fine [If the objects are equal, hash code is the same] Since there is only one wrapper with any given client, direct comparison is good enough. Be cruel and close even the checked out clients. The threads writing using these will now get an exception.Lifted from ACCUMULO-3318 - Lifted from TSSLTransportFactory in Thrift-0.9.1.The method to create a client socket with an SSLContextFactory object is not visible to us.Have to use * SslConnectionParams instead of TSSLTransportParameters because no getters existon TSSLTransportParameters./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/event/EventBuilder.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/eventInstantiate an Event instance based on the provided body and headers.If <code>headers</code> is <code>null</code>, then it is ignored./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/event/JSONEvent.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/event/JSONEvent.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/event"%s encoding not supported"%s encoding not supported/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/event/SimpleEvent.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/event/SimpleEvent.classbodyLen"[Event headers = "[Event headers = ", body.length = ", body.length = " ]" ]/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/thrift/Status.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/thriftStatus(int)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/Status.classAutogenerated by Thrift Compiler (0.9.3)DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING@generatedGet the integer value of this enum value, as defined in the Thrift IDL.Find a the enum type by its integer value, as defined in the Thrift IDL.null if the value is not found./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/thrift/ThriftFlumeEvent.javareadObjectreadObject(java.io.ObjectInputStream)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftFlumeEvent.classwriteObjectwriteObject(java.io.ObjectOutputStream)org.apache.thrift.schemeIScheme<>/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/apache/thrift/scheme/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/apache/thrift/scheme/IScheme.class? extends IScheme<>Class<? extends IScheme<>>Map<Class<? extends IScheme<>>,SchemeFactory>schemesBODY_FIELD_DESCHEADERS_FIELD_DESCSTRUCT_DESCTBase<ThriftFlumeEvent,_Fields>Comparable<ThriftFlumeEvent>"cast""serial"serial"Autogenerated by Thrift Compiler (0.9.3)""2017-09-01"2017-09-01"ThriftFlumeEvent"ThriftFlumeEvent"headers""body"HashMap<Class<? extends IScheme<>>,SchemeFactory>AbstractMap<Class<? extends IScheme<>>,SchemeFactory>HashMap<Class<? extends IScheme<>>,SchemeFactory>()? super SchemeFactory? extends SchemeFactoryBiFunction<? super SchemeFactory,? super SchemeFactory,? extends SchemeFactory>merge(java.lang.Class,org.apache.thrift.scheme.SchemeFactory,java.util.function.BiFunction)? super Class<? extends IScheme<>>BiFunction<? super Class<? extends IScheme<>>,? super SchemeFactory,? extends SchemeFactory>Function<? super Class<? extends IScheme<>>,? extends SchemeFactory>replace(java.lang.Class,org.apache.thrift.scheme.SchemeFactory)replace(java.lang.Class,org.apache.thrift.scheme.SchemeFactory,org.apache.thrift.scheme.SchemeFactory)putIfAbsent(java.lang.Class,org.apache.thrift.scheme.SchemeFactory)BiConsumer<? super Class<? extends IScheme<>>,? super SchemeFactory>getOrDefault(java.lang.Object,org.apache.thrift.scheme.SchemeFactory)Entry<Class<? extends IScheme<>>,SchemeFactory>Set<Entry<Class<? extends IScheme<>>,SchemeFactory>>Collection<Entry<Class<? extends IScheme<>>,SchemeFactory>>Iterable<Entry<Class<? extends IScheme<>>,SchemeFactory>>Collection<SchemeFactory>Iterable<SchemeFactory>Set<Class<? extends IScheme<>>>Collection<Class<? extends IScheme<>>>Iterable<Class<? extends IScheme<>>>? extends Class<? extends IScheme<>>Map<? extends Class<? extends IScheme<>>,? extends SchemeFactory>put(java.lang.Class,org.apache.thrift.scheme.SchemeFactory)AbstractMap<Class<? extends IScheme<>>,SchemeFactory>()Node<Class<? extends IScheme<>>,SchemeFactory>TreeNode<Class<? extends IScheme<>>,SchemeFactory>newTreeNode(int,java.lang.Class,org.apache.thrift.scheme.SchemeFactory,java.util.HashMap.Node)newNode(int,java.lang.Class,org.apache.thrift.scheme.SchemeFactory,java.util.HashMap.Node)Node<Class<? extends IScheme<>>,SchemeFactory>[]putVal(int,java.lang.Class,org.apache.thrift.scheme.SchemeFactory,boolean,boolean)HashMap<Class<? extends IScheme<>>,SchemeFactory>(java.util.Map)HashMap<Class<? extends IScheme<>>,SchemeFactory>(int)HashMap<Class<? extends IScheme<>>,SchemeFactory>(int,float)StandardScheme<>/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/apache/thrift/scheme/StandardScheme.classClass<StandardScheme<>>TupleScheme<>/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/apache/thrift/scheme/TupleScheme.classClass<TupleScheme<>>Map<_Fields,FieldMetaData>tmpMapEnumMap<_Fields,FieldMetaData>/modules/java.base/java/util/EnumMap.classAbstractMap<_Fields,FieldMetaData>Class<_Fields>Class<K>EnumMap<_Fields,FieldMetaData>(java.lang.Class)? super FieldMetaData? extends FieldMetaDataBiFunction<? super FieldMetaData,? super FieldMetaData,? extends FieldMetaData>merge(org.apache.flume.thrift.ThriftFlumeEvent._Fields,org.apache.thrift.meta_data.FieldMetaData,java.util.function.BiFunction)? super _FieldsBiFunction<? super _Fields,? super FieldMetaData,? extends FieldMetaData>compute(org.apache.flume.thrift.ThriftFlumeEvent._Fields,java.util.function.BiFunction)computeIfPresent(org.apache.flume.thrift.ThriftFlumeEvent._Fields,java.util.function.BiFunction)Function<? super _Fields,? extends FieldMetaData>computeIfAbsent(org.apache.flume.thrift.ThriftFlumeEvent._Fields,java.util.function.Function)replace(org.apache.flume.thrift.ThriftFlumeEvent._Fields,org.apache.thrift.meta_data.FieldMetaData)replace(org.apache.flume.thrift.ThriftFlumeEvent._Fields,org.apache.thrift.meta_data.FieldMetaData,org.apache.thrift.meta_data.FieldMetaData)putIfAbsent(org.apache.flume.thrift.ThriftFlumeEvent._Fields,org.apache.thrift.meta_data.FieldMetaData)BiConsumer<? super _Fields,? super FieldMetaData>getOrDefault(java.lang.Object,org.apache.thrift.meta_data.FieldMetaData)Entry<_Fields,FieldMetaData>Set<Entry<_Fields,FieldMetaData>>Collection<Entry<_Fields,FieldMetaData>>Iterable<Entry<_Fields,FieldMetaData>>Collection<FieldMetaData>Iterable<FieldMetaData>Set<_Fields>Collection<_Fields>Iterable<_Fields>? extends _FieldsMap<? extends _Fields,? extends FieldMetaData>put(org.apache.flume.thrift.ThriftFlumeEvent._Fields,org.apache.thrift.meta_data.FieldMetaData)AbstractMap<_Fields,FieldMetaData>()EnumMap<>put(java.lang.Enum,java.lang.Object)EnumMapEnumMap(java.util.Map)Map<_Fields,? extends FieldMetaData>EnumMap<_Fields,FieldMetaData>(java.util.Map)EnumMap<K,? extends V>AbstractMap<K,? extends V>EnumMap(java.util.EnumMap)EnumMap<_Fields,? extends FieldMetaData>AbstractMap<_Fields,? extends FieldMetaData>EnumMap<_Fields,FieldMetaData>(java.util.EnumMap)EnumMap(java.lang.Class)Class<? extends TBase<>>? extends TFieldIdEnumMap<? extends TFieldIdEnum,FieldMetaData>Class<ThriftFlumeEvent>_Fields_Fields(short,java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftFlumeEvent$_Fields.class_fieldName_thriftIdMap<String,_Fields>byNameHashMap<String,_Fields>AbstractMap<String,_Fields>HashMap<String,_Fields>()BiFunction<? super _Fields,? super _Fields,? extends _Fields>merge(java.lang.String,org.apache.flume.thrift.ThriftFlumeEvent._Fields,java.util.function.BiFunction)BiFunction<? super String,? super _Fields,? extends _Fields>Function<? super String,? extends _Fields>replace(java.lang.String,org.apache.flume.thrift.ThriftFlumeEvent._Fields)replace(java.lang.String,org.apache.flume.thrift.ThriftFlumeEvent._Fields,org.apache.flume.thrift.ThriftFlumeEvent._Fields)putIfAbsent(java.lang.String,org.apache.flume.thrift.ThriftFlumeEvent._Fields)BiConsumer<? super String,? super _Fields>getOrDefault(java.lang.Object,org.apache.flume.thrift.ThriftFlumeEvent._Fields)Entry<String,_Fields>Set<Entry<String,_Fields>>Collection<Entry<String,_Fields>>Iterable<Entry<String,_Fields>>Map<? extends String,? extends _Fields>put(java.lang.String,org.apache.flume.thrift.ThriftFlumeEvent._Fields)AbstractMap<String,_Fields>()Node<String,_Fields>TreeNode<String,_Fields>newTreeNode(int,java.lang.String,org.apache.flume.thrift.ThriftFlumeEvent._Fields,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.flume.thrift.ThriftFlumeEvent._Fields,java.util.HashMap.Node)Node<String,_Fields>[]putVal(int,java.lang.String,org.apache.flume.thrift.ThriftFlumeEvent._Fields,boolean,boolean)HashMap<String,_Fields>(java.util.Map)HashMap<String,_Fields>(int)HashMap<String,_Fields>(int,float)EnumSet<_Fields>/modules/java.base/java/util/EnumSet.classAbstractSet<_Fields>AbstractCollection<_Fields>EnumSet<>fieldId"Field "Field " doesn't exist!" doesn't exist!thriftId__this__headersthatthis_present_headersthat_present_headersthis_present_bodythat_present_bodyList<Object>SequencedCollection<Object>ArrayList<Object>AbstractList<Object>AbstractCollection<Object>ArrayList<Object>()Spliterator<Object>Consumer<? super Object>Iterator<Object>Stream<Object>BaseStream<Object,Stream<Object>>Predicate<? super Object>Collection<? extends Object>Iterable<? extends Object>AbstractCollection<Object>()ListIterator<Object>Comparator<? super Object>UnaryOperator<Object>Function<Object,Object>AbstractList<Object>()ArrayList<Object>(java.util.Collection)ArrayList<Object>(int)present_headerspresent_bodylastComparisonClass<? extends ThriftFlumeEvent>Map<String,? extends ThriftFlumeEvent>ThriftFlumeEvent[]Constructor<? extends ThriftFlumeEvent>TypeVariable<Class<? extends ThriftFlumeEvent>>TypeVariable<Class<? extends ThriftFlumeEvent>>[]iprotread(org.apache.thrift.protocol.TProtocol,org.apache.thrift.TBase)write(org.apache.thrift.protocol.TProtocol,org.apache.thrift.TBase)oprot"ThriftFlumeEvent("ThriftFlumeEvent("headers:"headers:"body:"body:"Required field 'headers' was not present! Struct: "Required field 'headers' was not present! Struct: "Required field 'body' was not present! Struct: "Required field 'body' was not present! Struct: teThriftFlumeEventStandardSchemeFactoryThriftFlumeEventStandardSchemeFactory()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftFlumeEvent$ThriftFlumeEventStandardSchemeFactory.classThriftFlumeEventStandardSchemeThriftFlumeEventStandardScheme()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftFlumeEvent$ThriftFlumeEventStandardScheme.classStandardScheme<ThriftFlumeEvent>IScheme<ThriftFlumeEvent>structschemeField_map0_key1_val2_i3_iter4ThriftFlumeEventTupleSchemeFactoryThriftFlumeEventTupleSchemeFactory()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftFlumeEvent$ThriftFlumeEventTupleSchemeFactory.classThriftFlumeEventTupleSchemeThriftFlumeEventTupleScheme()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftFlumeEvent$ThriftFlumeEventTupleScheme.classTupleScheme<ThriftFlumeEvent>prot_iter5_map6_key7_val8_i9 requiredThe set of fields this struct contains, along with convenience methods for finding and manipulating them.Find the _Fields constant that matches fieldId, or null if its not found. HEADERS BODYFind the _Fields constant that matches fieldId, throwing an exceptionif it is not found.Find the _Fields constant that matches name, or null if its not found. isset id assignmentsPerforms a deep copy on <i>other</i>.Returns true if field headers is set (has been assigned a value) and false otherwiseReturns true if field body is set (has been assigned a value) and false otherwiseReturns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise check for required fields check for sub-struct validity check for required fields of primitive type, which can't be checked in the validate method/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/thrift/ThriftSourceProtocol.javaorg.apache.thrift.asyncAsyncMethodCallback<>/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/apache/thrift/async/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/apache/thrift/async/AsyncMethodCallback.classresultHandlerTServiceClientFactory<Client>/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/apache/thrift/TServiceClientFactory.classTBase<?,?>Comparable<?>"append""append failed: unknown result"append failed: unknown result"appendBatch""appendBatch failed: unknown result"appendBatch failed: unknown result/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$AsyncClient$Factory.classclientManagerTAsyncClientFactory<AsyncClient>/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/apache/thrift/async/TAsyncClientFactory.classmethod_callTAsyncMethodCall<>/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/apache/thrift/async/TAsyncMethodCall.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$AsyncClient$append_call.classAsyncMethodCallback<T>memoryTransportgetFrameBuffergetFrameBuffer()onErroronError(java.lang.Exception)transitiontransition(java.nio.channels.SelectionKey)registerForFirstWriteregisterForFirstWrite(java.nio.channels.SelectionKey)start(java.nio.channels.Selector)prepareMethodCallprepareMethodCall()write_argswrite_args(org.apache.thrift.protocol.TProtocol)getTimeoutTimestampgetTimeoutTimestamp()hasTimeouthasTimeout()getSequenceIdgetSequenceId()getStartTimegetStartTime()isFinishedisFinished()getStategetState()TAsyncMethodCallTAsyncMethodCall(org.apache.thrift.async.TAsyncClient,org.apache.thrift.protocol.TProtocolFactory,org.apache.thrift.transport.TNonblockingTransport,org.apache.thrift.async.AsyncMethodCallback,boolean)TAsyncMethodCall<>(org.apache.thrift.async.TAsyncClient,org.apache.thrift.protocol.TProtocolFactory,org.apache.thrift.transport.TNonblockingTransport,org.apache.thrift.async.AsyncMethodCallback,boolean)"Method call not finished!"Method call not finished!/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$AsyncClient$appendBatch_call.classgetProcessMapgetProcessMap(java.util.Map)TBaseProcessor<I>Processor<>TBaseProcessor<>Class<Processor<>>Map<String,Processor<>>Processor[]Processor<>[]Constructor<Processor<>>? super Processor<>Class<? super Processor<>>TypeVariable<Class<Processor<>>>TypeVariable<Class<Processor<>>>[]ifaceHashMap<String,ProcessFunction<I,? extends TBase<>>>AbstractMap<String,ProcessFunction<I,? extends TBase<>>>HashMap<String,ProcessFunction<I,? extends TBase<>>>()? super ProcessFunction<I,? extends TBase<>>? extends ProcessFunction<I,? extends TBase<>>BiFunction<? super ProcessFunction<I,? extends TBase<>>,? super ProcessFunction<I,? extends TBase<>>,? extends ProcessFunction<I,? extends TBase<>>>ProcessFunction<>merge(java.lang.String,org.apache.thrift.ProcessFunction,java.util.function.BiFunction)BiFunction<? super String,? super ProcessFunction<I,? extends TBase<>>,? extends ProcessFunction<I,? extends TBase<>>>Function<? super String,? extends ProcessFunction<I,? extends TBase<>>>replace(java.lang.String,org.apache.thrift.ProcessFunction)replace(java.lang.String,org.apache.thrift.ProcessFunction,org.apache.thrift.ProcessFunction)putIfAbsent(java.lang.String,org.apache.thrift.ProcessFunction)BiConsumer<? super String,? super ProcessFunction<I,? extends TBase<>>>getOrDefault(java.lang.Object,org.apache.thrift.ProcessFunction)Entry<String,ProcessFunction<I,? extends TBase<>>>Set<Entry<String,ProcessFunction<I,? extends TBase<>>>>Collection<Entry<String,ProcessFunction<I,? extends TBase<>>>>Iterable<Entry<String,ProcessFunction<I,? extends TBase<>>>>Collection<ProcessFunction<I,? extends TBase<>>>Iterable<ProcessFunction<I,? extends TBase<>>>Map<? extends String,? extends ProcessFunction<I,? extends TBase<>>>put(java.lang.String,org.apache.thrift.ProcessFunction)AbstractMap<String,ProcessFunction<I,? extends TBase<>>>()Node<String,ProcessFunction<I,? extends TBase<>>>TreeNode<String,ProcessFunction<I,? extends TBase<>>>newTreeNode(int,java.lang.String,org.apache.thrift.ProcessFunction,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.thrift.ProcessFunction,java.util.HashMap.Node)Node<String,ProcessFunction<I,? extends TBase<>>>[]putVal(int,java.lang.String,org.apache.thrift.ProcessFunction,boolean,boolean)HashMap<String,ProcessFunction<I,? extends TBase<>>>(java.util.Map)HashMap<String,ProcessFunction<I,? extends TBase<>>>(int)HashMap<String,ProcessFunction<I,? extends TBase<>>>(int,float)processMapappend<>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$Processor$append.classappend<>()getMethodNamegetMethodName()getEmptyArgsInstancegetEmptyArgsInstance()getResult(java.lang.Object,org.apache.thrift.TBase)isOnewayisOneway()rethrowUnhandledExceptionsrethrowUnhandledExceptions()process(int,org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol,java.lang.Object)ProcessFunctionProcessFunction(java.lang.String)ProcessFunction<>(java.lang.String)getResult(org.apache.flume.thrift.ThriftSourceProtocol.Iface,org.apache.flume.thrift.ThriftSourceProtocol.append_args)append()appendBatch<>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$Processor$appendBatch.classappendBatch<>()getResult(org.apache.flume.thrift.ThriftSourceProtocol.Iface,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args)appendBatch()ProcessFunction<I,append_args>ProcessFunction<I,appendBatch_args>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$AsyncProcessor.classAsyncProcessFunction<I,? extends TBase<>,?>/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/apache/thrift/AsyncProcessFunction.classMap<String,AsyncProcessFunction<I,? extends TBase<>,?>>TBaseAsyncProcessor<I>/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.14.2/libthrift-0.14.2.jar/org/apache/thrift/TBaseAsyncProcessor.classAsyncProcessor<>TBaseAsyncProcessor<>Class<AsyncProcessor<>>Map<String,AsyncProcessor<>>AsyncProcessor[]AsyncProcessor<>[]Constructor<AsyncProcessor<>>? super AsyncProcessor<>Class<? super AsyncProcessor<>>TypeVariable<Class<AsyncProcessor<>>>TypeVariable<Class<AsyncProcessor<>>>[]HashMap<String,AsyncProcessFunction<I,? extends TBase<>,?>>AbstractMap<String,AsyncProcessFunction<I,? extends TBase<>,?>>HashMap<String,AsyncProcessFunction<I,? extends TBase<>,?>>()? super AsyncProcessFunction<I,? extends TBase<>,?>? extends AsyncProcessFunction<I,? extends TBase<>,?>BiFunction<? super AsyncProcessFunction<I,? extends TBase<>,?>,? super AsyncProcessFunction<I,? extends TBase<>,?>,? extends AsyncProcessFunction<I,? extends TBase<>,?>>AsyncProcessFunction<>merge(java.lang.String,org.apache.thrift.AsyncProcessFunction,java.util.function.BiFunction)BiFunction<? super String,? super AsyncProcessFunction<I,? extends TBase<>,?>,? extends AsyncProcessFunction<I,? extends TBase<>,?>>Function<? super String,? extends AsyncProcessFunction<I,? extends TBase<>,?>>replace(java.lang.String,org.apache.thrift.AsyncProcessFunction)replace(java.lang.String,org.apache.thrift.AsyncProcessFunction,org.apache.thrift.AsyncProcessFunction)putIfAbsent(java.lang.String,org.apache.thrift.AsyncProcessFunction)BiConsumer<? super String,? super AsyncProcessFunction<I,? extends TBase<>,?>>getOrDefault(java.lang.Object,org.apache.thrift.AsyncProcessFunction)Entry<String,AsyncProcessFunction<I,? extends TBase<>,?>>Set<Entry<String,AsyncProcessFunction<I,? extends TBase<>,?>>>Collection<Entry<String,AsyncProcessFunction<I,? extends TBase<>,?>>>Iterable<Entry<String,AsyncProcessFunction<I,? extends TBase<>,?>>>Collection<AsyncProcessFunction<I,? extends TBase<>,?>>Iterable<AsyncProcessFunction<I,? extends TBase<>,?>>Map<? extends String,? extends AsyncProcessFunction<I,? extends TBase<>,?>>put(java.lang.String,org.apache.thrift.AsyncProcessFunction)AbstractMap<String,AsyncProcessFunction<I,? extends TBase<>,?>>()Node<String,AsyncProcessFunction<I,? extends TBase<>,?>>TreeNode<String,AsyncProcessFunction<I,? extends TBase<>,?>>newTreeNode(int,java.lang.String,org.apache.thrift.AsyncProcessFunction,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.thrift.AsyncProcessFunction,java.util.HashMap.Node)Node<String,AsyncProcessFunction<I,? extends TBase<>,?>>[]putVal(int,java.lang.String,org.apache.thrift.AsyncProcessFunction,boolean,boolean)HashMap<String,AsyncProcessFunction<I,? extends TBase<>,?>>(java.util.Map)HashMap<String,AsyncProcessFunction<I,? extends TBase<>,?>>(int)HashMap<String,AsyncProcessFunction<I,? extends TBase<>,?>>(int,float)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$AsyncProcessor$append.classsendResponsesendResponse(org.apache.thrift.server.AbstractNonblockingServer.AsyncFrameBuffer,org.apache.thrift.TSerializable,byte,int)AsyncMethodCallback<R>getResultHandlergetResultHandler(org.apache.thrift.server.AbstractNonblockingServer.AsyncFrameBuffer,int)start(java.lang.Object,org.apache.thrift.TBase,org.apache.thrift.async.AsyncMethodCallback)AsyncProcessFunctionAsyncProcessFunction(java.lang.String)AsyncProcessFunction<>(java.lang.String)AsyncMethodCallback<Status>start(org.apache.flume.thrift.ThriftSourceProtocol.AsyncIface,org.apache.flume.thrift.ThriftSourceProtocol.append_args,org.apache.thrift.async.AsyncMethodCallback)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$AsyncProcessor$appendBatch.classstart(org.apache.flume.thrift.ThriftSourceProtocol.AsyncIface,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args,org.apache.thrift.async.AsyncMethodCallback)AsyncProcessFunction<I,append_args,Status>fbseqidfcall/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$AsyncProcessor$append$1.classonCompleteonComplete(java.lang.Object)onComplete(org.apache.flume.thrift.Status)"Exception writing to internal frame buffer"Exception writing to internal frame buffermsgTypeAsyncProcessFunction<I,appendBatch_args,Status>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$AsyncProcessor$appendBatch$1.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$append_args.classEVENT_FIELD_DESCTBase<append_args,_Fields>Comparable<append_args>"append_args"append_args"event"merge(org.apache.flume.thrift.ThriftSourceProtocol.append_args._Fields,org.apache.thrift.meta_data.FieldMetaData,java.util.function.BiFunction)compute(org.apache.flume.thrift.ThriftSourceProtocol.append_args._Fields,java.util.function.BiFunction)computeIfPresent(org.apache.flume.thrift.ThriftSourceProtocol.append_args._Fields,java.util.function.BiFunction)computeIfAbsent(org.apache.flume.thrift.ThriftSourceProtocol.append_args._Fields,java.util.function.Function)replace(org.apache.flume.thrift.ThriftSourceProtocol.append_args._Fields,org.apache.thrift.meta_data.FieldMetaData)replace(org.apache.flume.thrift.ThriftSourceProtocol.append_args._Fields,org.apache.thrift.meta_data.FieldMetaData,org.apache.thrift.meta_data.FieldMetaData)putIfAbsent(org.apache.flume.thrift.ThriftSourceProtocol.append_args._Fields,org.apache.thrift.meta_data.FieldMetaData)put(org.apache.flume.thrift.ThriftSourceProtocol.append_args._Fields,org.apache.thrift.meta_data.FieldMetaData)Class<append_args>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$append_args$_Fields.classmerge(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.append_args._Fields,java.util.function.BiFunction)replace(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.append_args._Fields)replace(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.append_args._Fields,org.apache.flume.thrift.ThriftSourceProtocol.append_args._Fields)putIfAbsent(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.append_args._Fields)getOrDefault(java.lang.Object,org.apache.flume.thrift.ThriftSourceProtocol.append_args._Fields)put(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.append_args._Fields)newTreeNode(int,java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.append_args._Fields,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.append_args._Fields,java.util.HashMap.Node)putVal(int,java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.append_args._Fields,boolean,boolean)this_present_eventthat_present_eventpresent_event? extends append_argsClass<? extends append_args>Map<String,? extends append_args>append_args[]Constructor<? extends append_args>TypeVariable<Class<? extends append_args>>TypeVariable<Class<? extends append_args>>[]"append_args("append_args("event:"event:append_argsStandardSchemeFactoryappend_argsStandardSchemeFactory()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$append_args$append_argsStandardSchemeFactory.classappend_argsStandardSchemeappend_argsStandardScheme()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$append_args$append_argsStandardScheme.classStandardScheme<append_args>IScheme<append_args>append_argsTupleSchemeFactoryappend_argsTupleSchemeFactory()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$append_args$append_argsTupleSchemeFactory.classappend_argsTupleSchemeappend_argsTupleScheme()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$append_args$append_argsTupleScheme.classTupleScheme<append_args>optionalsincoming/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$append_result.classSUCCESS_FIELD_DESCTBase<append_result,_Fields>Comparable<append_result>"append_result"append_result"success"merge(org.apache.flume.thrift.ThriftSourceProtocol.append_result._Fields,org.apache.thrift.meta_data.FieldMetaData,java.util.function.BiFunction)compute(org.apache.flume.thrift.ThriftSourceProtocol.append_result._Fields,java.util.function.BiFunction)computeIfPresent(org.apache.flume.thrift.ThriftSourceProtocol.append_result._Fields,java.util.function.BiFunction)computeIfAbsent(org.apache.flume.thrift.ThriftSourceProtocol.append_result._Fields,java.util.function.Function)replace(org.apache.flume.thrift.ThriftSourceProtocol.append_result._Fields,org.apache.thrift.meta_data.FieldMetaData)replace(org.apache.flume.thrift.ThriftSourceProtocol.append_result._Fields,org.apache.thrift.meta_data.FieldMetaData,org.apache.thrift.meta_data.FieldMetaData)putIfAbsent(org.apache.flume.thrift.ThriftSourceProtocol.append_result._Fields,org.apache.thrift.meta_data.FieldMetaData)put(org.apache.flume.thrift.ThriftSourceProtocol.append_result._Fields,org.apache.thrift.meta_data.FieldMetaData)? extends TEnumClass<? extends TEnum>Class<append_result>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$append_result$_Fields.classmerge(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.append_result._Fields,java.util.function.BiFunction)replace(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.append_result._Fields)replace(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.append_result._Fields,org.apache.flume.thrift.ThriftSourceProtocol.append_result._Fields)putIfAbsent(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.append_result._Fields)getOrDefault(java.lang.Object,org.apache.flume.thrift.ThriftSourceProtocol.append_result._Fields)put(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.append_result._Fields)newTreeNode(int,java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.append_result._Fields,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.append_result._Fields,java.util.HashMap.Node)putVal(int,java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.append_result._Fields,boolean,boolean)this_present_successthat_present_successpresent_success? extends append_resultClass<? extends append_result>Map<String,? extends append_result>append_result[]Constructor<? extends append_result>TypeVariable<Class<? extends append_result>>TypeVariable<Class<? extends append_result>>[]"append_result("append_result("success:"success:append_resultStandardSchemeFactoryappend_resultStandardSchemeFactory()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$append_result$append_resultStandardSchemeFactory.classappend_resultStandardSchemeappend_resultStandardScheme()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$append_result$append_resultStandardScheme.classStandardScheme<append_result>IScheme<append_result>append_resultTupleSchemeFactoryappend_resultTupleSchemeFactory()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$append_result$append_resultTupleSchemeFactory.classappend_resultTupleSchemeappend_resultTupleScheme()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$append_result$append_resultTupleScheme.classTupleScheme<append_result>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$appendBatch_args.classEVENTS_FIELD_DESCTBase<appendBatch_args,_Fields>Comparable<appendBatch_args>"appendBatch_args"appendBatch_args"events"merge(org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args._Fields,org.apache.thrift.meta_data.FieldMetaData,java.util.function.BiFunction)compute(org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args._Fields,java.util.function.BiFunction)computeIfPresent(org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args._Fields,java.util.function.BiFunction)computeIfAbsent(org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args._Fields,java.util.function.Function)replace(org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args._Fields,org.apache.thrift.meta_data.FieldMetaData)replace(org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args._Fields,org.apache.thrift.meta_data.FieldMetaData,org.apache.thrift.meta_data.FieldMetaData)putIfAbsent(org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args._Fields,org.apache.thrift.meta_data.FieldMetaData)put(org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args._Fields,org.apache.thrift.meta_data.FieldMetaData)Class<appendBatch_args>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$appendBatch_args$_Fields.classmerge(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args._Fields,java.util.function.BiFunction)replace(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args._Fields)replace(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args._Fields,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args._Fields)putIfAbsent(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args._Fields)getOrDefault(java.lang.Object,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args._Fields)put(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args._Fields)newTreeNode(int,java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args._Fields,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args._Fields,java.util.HashMap.Node)putVal(int,java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_args._Fields,boolean,boolean)__this__eventsother_elementelemthis_present_eventsthat_present_eventspresent_events? extends appendBatch_argsClass<? extends appendBatch_args>Map<String,? extends appendBatch_args>appendBatch_args[]Constructor<? extends appendBatch_args>TypeVariable<Class<? extends appendBatch_args>>TypeVariable<Class<? extends appendBatch_args>>[]"appendBatch_args("appendBatch_args("events:"events:appendBatch_argsStandardSchemeFactoryappendBatch_argsStandardSchemeFactory()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$appendBatch_args$appendBatch_argsStandardSchemeFactory.classappendBatch_argsStandardSchemeappendBatch_argsStandardScheme()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$appendBatch_args$appendBatch_argsStandardScheme.classStandardScheme<appendBatch_args>IScheme<appendBatch_args>_list10_elem11_i12_iter13appendBatch_argsTupleSchemeFactoryappendBatch_argsTupleSchemeFactory()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$appendBatch_args$appendBatch_argsTupleSchemeFactory.classappendBatch_argsTupleSchemeappendBatch_argsTupleScheme()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$appendBatch_args$appendBatch_argsTupleScheme.classTupleScheme<appendBatch_args>_iter14_list15_elem16_i17/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$appendBatch_result.classTBase<appendBatch_result,_Fields>Comparable<appendBatch_result>"appendBatch_result"appendBatch_resultmerge(org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_result._Fields,org.apache.thrift.meta_data.FieldMetaData,java.util.function.BiFunction)compute(org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_result._Fields,java.util.function.BiFunction)computeIfPresent(org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_result._Fields,java.util.function.BiFunction)computeIfAbsent(org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_result._Fields,java.util.function.Function)replace(org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_result._Fields,org.apache.thrift.meta_data.FieldMetaData)replace(org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_result._Fields,org.apache.thrift.meta_data.FieldMetaData,org.apache.thrift.meta_data.FieldMetaData)putIfAbsent(org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_result._Fields,org.apache.thrift.meta_data.FieldMetaData)put(org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_result._Fields,org.apache.thrift.meta_data.FieldMetaData)Class<appendBatch_result>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$appendBatch_result$_Fields.classmerge(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_result._Fields,java.util.function.BiFunction)replace(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_result._Fields)replace(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_result._Fields,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_result._Fields)putIfAbsent(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_result._Fields)getOrDefault(java.lang.Object,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_result._Fields)put(java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_result._Fields)newTreeNode(int,java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_result._Fields,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_result._Fields,java.util.HashMap.Node)putVal(int,java.lang.String,org.apache.flume.thrift.ThriftSourceProtocol.appendBatch_result._Fields,boolean,boolean)? extends appendBatch_resultClass<? extends appendBatch_result>Map<String,? extends appendBatch_result>appendBatch_result[]Constructor<? extends appendBatch_result>TypeVariable<Class<? extends appendBatch_result>>TypeVariable<Class<? extends appendBatch_result>>[]"appendBatch_result("appendBatch_result(appendBatch_resultStandardSchemeFactoryappendBatch_resultStandardSchemeFactory()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$appendBatch_result$appendBatch_resultStandardSchemeFactory.classappendBatch_resultStandardSchemeappendBatch_resultStandardScheme()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$appendBatch_result$appendBatch_resultStandardScheme.classStandardScheme<appendBatch_result>IScheme<appendBatch_result>appendBatch_resultTupleSchemeFactoryappendBatch_resultTupleSchemeFactory()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$appendBatch_result$appendBatch_resultTupleSchemeFactory.classappendBatch_resultTupleSchemeappendBatch_resultTupleScheme()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/thrift/ThriftSourceProtocol$appendBatch_result$appendBatch_resultTupleScheme.classTupleScheme<appendBatch_result> EVENTReturns true if field event is set (has been assigned a value) and false otherwise SUCCESSReturns true if field success is set (has been assigned a value) and false otherwise EVENTSReturns true if field events is set (has been assigned a value) and false otherwise/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/util/OrderSelector.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/utilmaxTimeoutMap<T,FailureState>stateMapMAX_TIMEOUTCONSIDER_SEQUENTIAL_RANGEEXP_BACKOFF_COUNTER_LIMITLinkedHashMap<T,FailureState>HashMap<T,FailureState>AbstractMap<T,FailureState>SequencedMap<T,FailureState>LinkedHashMap<T,FailureState>()? super FailureState? extends FailureStateBiFunction<? super FailureState,? super FailureState,? extends FailureState>merge(java.lang.Object,org.apache.flume.util.OrderSelector.FailureState,java.util.function.BiFunction)BiFunction<? super T,? super FailureState,? extends FailureState>Function<? super T,? extends FailureState>replace(java.lang.Object,org.apache.flume.util.OrderSelector.FailureState)replace(java.lang.Object,org.apache.flume.util.OrderSelector.FailureState,org.apache.flume.util.OrderSelector.FailureState)putIfAbsent(java.lang.Object,org.apache.flume.util.OrderSelector.FailureState)BiConsumer<? super T,? super FailureState>getOrDefault(java.lang.Object,org.apache.flume.util.OrderSelector.FailureState)Entry<T,FailureState>Set<Entry<T,FailureState>>Collection<Entry<T,FailureState>>Iterable<Entry<T,FailureState>>Collection<FailureState>Iterable<FailureState>Map<? extends T,? extends FailureState>put(java.lang.Object,org.apache.flume.util.OrderSelector.FailureState)AbstractMap<T,FailureState>()Node<T,FailureState>TreeNode<T,FailureState>newTreeNode(int,java.lang.Object,org.apache.flume.util.OrderSelector.FailureState,java.util.HashMap.Node)newNode(int,java.lang.Object,org.apache.flume.util.OrderSelector.FailureState,java.util.HashMap.Node)Node<T,FailureState>[]putVal(int,java.lang.Object,org.apache.flume.util.OrderSelector.FailureState,boolean,boolean)HashMap<T,FailureState>(java.util.Map)HashMap<T,FailureState>()HashMap<T,FailureState>(int)HashMap<T,FailureState>(int,float)SequencedSet<Entry<T,FailureState>>SequencedCollection<Entry<T,FailureState>>SequencedCollection<FailureState>putLast(java.lang.Object,org.apache.flume.util.OrderSelector.FailureState)putFirst(java.lang.Object,org.apache.flume.util.OrderSelector.FailureState)LinkedHashMap<T,FailureState>(int,float,boolean)LinkedHashMap<T,FailureState>(java.util.Map)LinkedHashMap<T,FailureState>(int)LinkedHashMap<T,FailureState>(int,float)30000LobjectsArrayList<T>AbstractList<T>Collection<? extends T>Iterable<? extends T>ArrayList<T>(java.util.Collection)AbstractCollection<T>()ListIterator<T>AbstractList<T>()ArrayList<T>()ArrayList<T>(int)failedObjectlastBackoffLengthallowableDiffindexListArrayList<Integer>()AbstractList<Integer>()ArrayList<Integer>(java.util.Collection)ArrayList<Integer>(int)sequentialFails/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/util/OrderSelector$FailureState.classrestoreTimelastFailFailureStateFailureState()A basic implementation of an order selector that implements a simpleexponential backoff algorithm. Subclasses can use the same algorithm forbackoff by simply overriding <tt>createIterator</tt> method to order thelist of active sinks returned by <tt>getIndexList</tt> method. Classesinstantiating subclasses of this class are expected to call <tt>informFailure</tt>method when an object passed to this class should be marked as failed and backed off.When implementing a different backoff algorithm, a subclass shouldminimally override <tt>informFailure</tt> and <tt>getIndexList</tt> methods.- The class on which ordering is to be doneSet the list of objects which this class should return in order.Order is the same as the original order.Get the list of objects to be ordered. This list is in the same orderas originally passed in, not in the algorithmically reordered order.- list of objects to be ordered.- list of algorithmically ordered active sinksInform this class of the failure of an object so it can be backed off.If there is no backoff this method is a no-op.When do we increase the backoff period?We basically calculate the time difference between the last failureand the current one. If this failure happened within one hour of thelast backoff period getting over, then we increase the timeout,since the object did not recover yet. Else we assume this is a freshfailure and reset the count.Depending on the number of sequential failures this component had, delayits restore time. Each time it fails, delay the restore by 1000 ms,until the maxTimeOut is reached.- List of indices currently active objects/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/util/RandomOrderSelector.javaOrderSelector<T>OrderSelector<T>(boolean)int[]indexOrderSpecificOrderIterator<T>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/util/SpecificOrderIterator.classSpecificOrderIterator<T>(int[],java.util.List)SpecificOrderIteratorSpecificOrderIterator(int[],java.util.List)An implementation of OrderSelector which returns objects in random order.Also supports backoff./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/util/RoundRobinOrderSelector.javanextHeadactiveIndicesbeginAn implementation of OrderSelector which returns objects in round robin order. possible that the size has shrunk so gotta adjust nextHead for that/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/util/SSLUtil.javanormalizePropertynormalizeProperty(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/util/SSLUtil.classinitSysPropFromEnvVarinitSysPropFromEnvVar(java.lang.String,java.lang.String,java.lang.String)DESCR_EXCLUDE_CIPHERSUITESDESCR_INCLUDE_CIPHERSUITESDESCR_EXCLUDE_PROTOCOLSDESCR_INCLUDE_PROTOCOLSDESCR_TRUSTSTORE_TYPEDESCR_TRUSTSTORE_PASSWORDDESCR_TRUSTSTORE_PATHDESCR_KEYSTORE_TYPEDESCR_KEYSTORE_PASSWORDDESCR_KEYSTORE_PATHENV_VAR_EXCLUDE_CIPHERSUITESENV_VAR_INCLUDE_CIPHERSUITESENV_VAR_EXCLUDE_PROTOCOLSENV_VAR_INCLUDE_PROTOCOLSENV_VAR_TRUSTSTORE_TYPEENV_VAR_TRUSTSTORE_PASSWORDENV_VAR_TRUSTSTORE_PATHENV_VAR_KEYSTORE_TYPEENV_VAR_KEYSTORE_PASSWORDENV_VAR_KEYSTORE_PATHSYS_PROP_EXCLUDE_CIPHERSUITESSYS_PROP_INCLUDE_CIPHERSUITESSYS_PROP_EXCLUDE_PROTOCOLSSYS_PROP_INCLUDE_PROTOCOLSSYS_PROP_TRUSTSTORE_TYPESYS_PROP_TRUSTSTORE_PASSWORDSYS_PROP_TRUSTSTORE_PATHSYS_PROP_KEYSTORE_TYPESYS_PROP_KEYSTORE_PASSWORDSYS_PROP_KEYSTORE_PATHClass<SSLUtil>"javax.net.ssl.keyStore"javax.net.ssl.keyStore"javax.net.ssl.keyStorePassword"javax.net.ssl.keyStorePassword"javax.net.ssl.keyStoreType"javax.net.ssl.keyStoreType"javax.net.ssl.trustStore"javax.net.ssl.trustStore"javax.net.ssl.trustStorePassword"javax.net.ssl.trustStorePassword"javax.net.ssl.trustStoreType"javax.net.ssl.trustStoreType"flume.ssl.include.protocols"flume.ssl.include.protocols"flume.ssl.exclude.protocols"flume.ssl.exclude.protocols"flume.ssl.include.cipherSuites"flume.ssl.include.cipherSuites"flume.ssl.exclude.cipherSuites"flume.ssl.exclude.cipherSuites"FLUME_SSL_KEYSTORE_PATH"FLUME_SSL_KEYSTORE_PATH"FLUME_SSL_KEYSTORE_PASSWORD"FLUME_SSL_KEYSTORE_PASSWORD"FLUME_SSL_KEYSTORE_TYPE"FLUME_SSL_KEYSTORE_TYPE"FLUME_SSL_TRUSTSTORE_PATH"FLUME_SSL_TRUSTSTORE_PATH"FLUME_SSL_TRUSTSTORE_PASSWORD"FLUME_SSL_TRUSTSTORE_PASSWORD"FLUME_SSL_TRUSTSTORE_TYPE"FLUME_SSL_TRUSTSTORE_TYPE"FLUME_SSL_INCLUDE_PROTOCOLS"FLUME_SSL_INCLUDE_PROTOCOLS"FLUME_SSL_EXCLUDE_PROTOCOLS"FLUME_SSL_EXCLUDE_PROTOCOLS"FLUME_SSL_INCLUDE_CIPHERSUITES"FLUME_SSL_INCLUDE_CIPHERSUITES"FLUME_SSL_EXCLUDE_CIPHERSUITES"FLUME_SSL_EXCLUDE_CIPHERSUITES"keystore path"keystore path"keystore password"keystore password"keystore type"keystore type"truststore path"truststore path"truststore password"truststore password"truststore type"truststore type"include protocols"include protocols"exclude protocols"exclude protocols"include cipher suites"include cipher suites"exclude cipher suites"exclude cipher suitessysPropName"Global SSL "Global SSL " has been initialized from system property." has been initialized from system property." has been initialized from environment variable." has been initialized from environment variable."No global SSL "No global SSL " specified." specified.sysPropValue/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/main/java/org/apache/flume/util/SpecificOrderIterator.javaorderorderArrayitemListA utility class that iterates over the given ordered list of items viathe specified order array. The entries of the order array indicate theindex within the ordered list of items that needs to be picked over thecourse of iteration./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/test/resources/log4j2.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/test/resources/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/src/test/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/generated-sources/avro/org/apache/flume/source/avro/AvroFlumeEvent.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/generated-sources/avro/org/apache/flume/source/avro/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/generated-sources/avro/org/apache/flume/source/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/generated-sources/avro/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/generated-sources/avro/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/generated-sources/avro/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/generated-sources/avro/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/generated-sourcesorg.apache.flume.source.avroDatumReader<AvroFlumeEvent>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/source/avro/AvroFlumeEvent.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/source/avro/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/sourceDatumWriter<AvroFlumeEvent>BinaryMessageDecoder<AvroFlumeEvent>BaseDecoder<AvroFlumeEvent>MessageDecoder<AvroFlumeEvent>BinaryMessageEncoder<AvroFlumeEvent>MessageEncoder<AvroFlumeEvent>727610275711926115L727610275711926115"{\"type\":\"record\",\"name\":\"AvroFlumeEvent\",\"namespace\":\"org.apache.flume.source.avro\",\"fields\":[{\"name\":\"headers\",\"type\":{\"type\":\"map\",\"values\":\"string\"}},{\"name\":\"body\",\"type\":\"bytes\"}]}"{"type":"record","name":"AvroFlumeEvent","namespace":"org.apache.flume.source.avro","fields":[{"name":"headers","type":{"type":"map","values":"string"}},{"name":"body","type":"bytes"}]}BinaryMessageEncoder<AvroFlumeEvent>(org.apache.avro.generic.GenericData,org.apache.avro.Schema)encode(org.apache.flume.source.avro.AvroFlumeEvent,java.io.OutputStream)encode(org.apache.flume.source.avro.AvroFlumeEvent)BinaryMessageEncoder<AvroFlumeEvent>(org.apache.avro.generic.GenericData,org.apache.avro.Schema,boolean)BinaryMessageDecoder<AvroFlumeEvent>(org.apache.avro.generic.GenericData,org.apache.avro.Schema)decode(byte[],org.apache.flume.source.avro.AvroFlumeEvent)decode(java.nio.ByteBuffer,org.apache.flume.source.avro.AvroFlumeEvent)decode(java.io.InputStream,org.apache.flume.source.avro.AvroFlumeEvent)BaseDecoder<AvroFlumeEvent>()BinaryMessageDecoder<AvroFlumeEvent>(org.apache.avro.generic.GenericData,org.apache.avro.Schema,org.apache.avro.message.SchemaStore)Builder(org.apache.flume.source.avro.AvroFlumeEvent)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/source/avro/AvroFlumeEvent$Builder.classBuilder(org.apache.flume.source.avro.AvroFlumeEvent.Builder)SpecificRecordBuilderBase<AvroFlumeEvent>RecordBuilderBase<AvroFlumeEvent>RecordBuilder<AvroFlumeEvent>RecordBuilderBase<AvroFlumeEvent>(org.apache.avro.data.RecordBuilderBase,org.apache.avro.generic.GenericData)RecordBuilderBase<AvroFlumeEvent>(org.apache.avro.Schema,org.apache.avro.generic.GenericData)write(org.apache.flume.source.avro.AvroFlumeEvent,org.apache.avro.io.Encoder)read(org.apache.flume.source.avro.AvroFlumeEvent,org.apache.avro.io.Decoder)size0actualSize0e0v0"Map-size written was "Map-size written was ", but element count was ", but element count was m0k0Serializes this AvroFlumeEvent to a ByteBuffer.Deserializes a AvroFlumeEvent from a ByteBuffer.a AvroFlumeEvent instance decoded from the given bufferThe new value for headersThe new value for bodyGets the value of the 'headers' field.The value of the 'headers' field.Sets the value of the 'headers' field.Gets the value of the 'body' field.The value of the 'body' field.Sets the value of the 'body' field.Creates a new AvroFlumeEvent RecordBuilder.A new AvroFlumeEvent RecordBuilderCreates a new AvroFlumeEvent RecordBuilder by copying an existing Builder.Creates a new AvroFlumeEvent RecordBuilder by copying an existing AvroFlumeEvent instance.RecordBuilder for AvroFlumeEvent instances.Creates a Builder by copying an existing AvroFlumeEvent instanceThe value of 'headers'.Checks whether the 'headers' field has been set.True if the 'headers' field has been set, false otherwise.Clears the value of the 'headers' field.The value of 'body'.Checks whether the 'body' field has been set.True if the 'body' field has been set, false otherwise.Clears the value of the 'body' field. Need fresh name due to limitation of macro system/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/generated-sources/avro/org/apache/flume/source/avro/AvroSourceProtocol.java"{\"protocol\":\"AvroSourceProtocol\",\"namespace\":\"org.apache.flume.source.avro\",\"doc\":\"Licensed to the Apache Software Foundation (ASF) under one\\nor more contributor license agreements.  See the NOTICE file\\ndistributed with this work for additional information\\nregarding copyright ownership.  The ASF licenses this file\\nto you under the Apache License, Version 2.0 (the\\n\\\"License\\\"); you may not use this file except in compliance\\nwith the License.  You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an\\n\\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\\nKIND, either express or implied.  See the License for the\\nspecific language governing permissions and limitations\\nunder the License.\",\"types\":[{\"type\":\"enum\",\"name\":\"Status\",\"symbols\":[\"OK\",\"FAILED\",\"UNKNOWN\"]},{\"type\":\"record\",\"name\":\"AvroFlumeEvent\",\"fields\":[{\"name\":\"headers\",\"type\":{\"type\":\"map\",\"values\":\"string\"}},{\"name\":\"body\",\"type\":\"bytes\"}]}],\"messages\":{\"append\":{\"request\":[{\"name\":\"event\",\"type\":\"AvroFlumeEvent\"}],\"response\":\"Status\"},\"appendBatch\":{\"request\":[{\"name\":\"events\",\"type\":{\"type\":\"array\",\"items\":\"AvroFlumeEvent\"}}],\"response\":\"Status\"}}}"{"protocol":"AvroSourceProtocol","namespace":"org.apache.flume.source.avro","doc":"Licensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.","types":[{"type":"enum","name":"Status","symbols":["OK","FAILED","UNKNOWN"]},{"type":"record","name":"AvroFlumeEvent","fields":[{"name":"headers","type":{"type":"map","values":"string"}},{"name":"body","type":"bytes"}]}],"messages":{"append":{"request":[{"name":"event","type":"AvroFlumeEvent"}],"response":"Status"},"appendBatch":{"request":[{"name":"events","type":{"type":"array","items":"AvroFlumeEvent"}}],"response":"Status"}}}callbackThe async call could not be completed./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/generated-sources/avro/org/apache/flume/source/avro/Status.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sdk/target/classes/org/apache/flume/source/avro/Status.classGenericEnumSymbol<Status>/Users/burakyetistiren/.m2/repository/org/apache/avro/avro/1.11.1/avro-1.11.1.jar/org/apache/avro/generic/GenericEnumSymbol.class"{\"type\":\"enum\",\"name\":\"Status\",\"namespace\":\"org.apache.flume.source.avro\",\"symbols\":[\"OK\",\"FAILED\",\"UNKNOWN\"]}"{"type":"enum","name":"Status","namespace":"org.apache.flume.source.avro","symbols":["OK","FAILED","UNKNOWN"]}/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinksflume-ng-sinksFlume HTTP/S Sinkorg.apache.flume.sink.http${guava.version}com.github.tomakehurstwiremock${wiremock.version}/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/src/main/java/org/apache/flume/sink/http/HttpSink.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/src/main/java/org/apache/flume/sink/http/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/src/main/java/org/apache/flume/sink/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/src/main/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/src/main/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/src/main/java/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/src/main/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/srcsetSinkCountersetSinkCounter(org.apache.flume.instrumentation.SinkCounter)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/target/classes/org/apache/flume/sink/http/HttpSink.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/target/classes/org/apache/flume/sink/http/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/target/classes/org/apache/flume/sink/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/target/classes/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/target/classes/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/target/classes/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/target/classes/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/targetsetConnectionBuildersetConnectionBuilder(org.apache.flume.sink.http.HttpSink.ConnectionBuilder)HashMap<String,Boolean>AbstractMap<String,Boolean>Map<String,Boolean>findOverrideValuefindOverrideValue(java.lang.String,java.util.HashMap,boolean)parseConfigOverridesparseConfigOverrides(java.lang.String,org.apache.flume.Context,java.util.Map)connectionBuilderincrementMetricsOverridesrollbackOverridesbackoffOverridesdefaultIncrementMetricsdefaultRollbackdefaultBackoffacceptHeadercontentTypeHeaderconnectTimeoutendpointUrlDEFAULT_ACCEPT_HEADERDEFAULT_CONTENT_TYPEDEFAULT_REQUEST_TIMEOUTDEFAULT_CONNECT_TIMEOUTHTTP_STATUS_CONTINUEHashMap<String,Boolean>()? super Boolean? extends BooleanBiFunction<? super Boolean,? super Boolean,? extends Boolean>merge(java.lang.String,java.lang.Boolean,java.util.function.BiFunction)BiFunction<? super String,? super Boolean,? extends Boolean>Function<? super String,? extends Boolean>replace(java.lang.String,java.lang.Boolean)replace(java.lang.String,java.lang.Boolean,java.lang.Boolean)putIfAbsent(java.lang.String,java.lang.Boolean)BiConsumer<? super String,? super Boolean>getOrDefault(java.lang.Object,java.lang.Boolean)Entry<String,Boolean>Set<Entry<String,Boolean>>Collection<Entry<String,Boolean>>Iterable<Entry<String,Boolean>>Collection<Boolean>Iterable<Boolean>Map<? extends String,? extends Boolean>put(java.lang.String,java.lang.Boolean)AbstractMap<String,Boolean>()Node<String,Boolean>TreeNode<String,Boolean>newTreeNode(int,java.lang.String,java.lang.Boolean,java.util.HashMap.Node)newNode(int,java.lang.String,java.lang.Boolean,java.util.HashMap.Node)Node<String,Boolean>[]putVal(int,java.lang.String,java.lang.Boolean,boolean,boolean)HashMap<String,Boolean>(java.util.Map)HashMap<String,Boolean>(int)HashMap<String,Boolean>(int,float)Class<HttpSink>configuredEndpoint"endpoint"endpoint"Read endpoint URL from configuration : "Read endpoint URL from configuration : "Endpoint URL invalid"Endpoint URL invalid"connectTimeout""Connect timeout must be a non-zero and positive"Connect timeout must be a non-zero and positive"Using connect timeout : "Using connect timeout : "requestTimeout""Request timeout must be a non-zero and positive"Request timeout must be a non-zero and positive"Using request timeout : "Using request timeout : "acceptHeader""Using Accept header value : "Using Accept header value : "contentTypeHeader""Using Content-Type header value : "Using Content-Type header value : "defaultBackoff""Channel backoff by default is "Channel backoff by default is "defaultRollback""Transaction rollback by default is "Transaction rollback by default is "defaultIncrementMetrics""Incrementing metrics by default is "Incrementing metrics by default is "rollback""incrementMetrics"incrementMetrics"Starting HttpSink"Starting HttpSink"Stopping HttpSink"Stopping HttpSinktxn"Sending request : "Sending request : httpStatusCode"Got status code : "Got status code : "bad request"bad request"Response processed and closed"Response processed and closedhttpStatusStringshouldRollbackshouldBackoffshouldIncrementMetrics"Got status code %d from HTTP server."
                    + " Rolled back event and backed off."Got status code %d from HTTP server. Rolled back event and backed off."Got status code %d from HTTP server."
                    + " Rolled back event for retry."Got status code %d from HTTP server. Rolled back event for retry."Malformed response returned from server, retrying"Malformed response returned from server, retrying"Error opening connection, or request timed out"Error opening connection, or request timed out"Processed empty event"Processed empty event"Error sending HTTP request, retrying"Error sending HTTP request, retryingoverride"Read %s value for status code %s as %s"Read %s value for status code %s as %s"Ignoring duplicate config value for %s.%s"Ignoring duplicate config value for %s.%sstatusCodeoverridesoverrideValue"XX"XXnewSinkCounterConnectionBuilderConnectionBuilder()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/target/classes/org/apache/flume/sink/http/HttpSink$ConnectionBuilder.class"POST"POST"Accept"AcceptImplementation of an HTTP sink. Events are POSTed to an HTTP / HTTPSendpoint. The error handling behaviour is configurable, and can responddifferently depending on the response status returned by the endpoint.Rollback of the Flume transaction, and backoff can be specified globally,then overridden for ranges (or individual) status codes.Class logger.Lowest valid HTTP status code.Default setting for the connection timeout when calling endpoint.Default setting for the request timeout when calling endpoint.Default setting for the HTTP content type header.Default setting for the HTTP accept header.Endpoint URL to POST events to.Counter used to monitor event throughput.Actual connection timeout value in use.Actual request timeout value in use.Actual content type header value in use.Actual accept header value in use.Backoff value to use if a specific override is not defined.Rollback value to use if a specific override is not defined.Increment metrics value to use if a specific override is not defined.Holds all overrides for backoff. The key is a string of the format "500" or"5XX", and the value is the backoff value to use for the individual code,or code range.Holds all overrides for rollback. The key is a string of the format "500"or "5XX", and the value is the rollback value to use for the individualcode, or code range.Holds all overrides for increment metrics. The key is a string of theformat "500" or "5XX", and the value is the increment metrics value to usefor the individual code, or code range.Used to create HTTP connections to the endpoint. re-throw all Errors ignore errorsReads a set of override values from the context configuration and storesthe results in the Map provided.the prefix of the config property namesthe context to use to read config propertiesthe override Map to store results inQueries the specified override map to find the most appropriate value. Themost specific match is found.the String representation of the HTTP status codethe map of status code overridesthe default value to use if no override is configuredthe value of the most specific match to the given status codeUpdate the connection builder.the new valueUpdate the sinkCounter.Class used to allow extending the connection building functionality.Creates an HTTP connection to the configured endpoint address. Thisconnection is setup for a POST request, and uses the content type andaccept header values in the configuration.the connection objecton any connection error/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/src/main/java/org/apache/flume/sink/http/package-info.javaThis package provides an HTTP sink for Flume so that events can be sent outto a target HTTP endpoint./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/src/test/resources/log4j2.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/src/test/resources/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-http-sink/src/testALL/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-irc-sink/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-irc-sinkFlume NG IRC Sinkorg.apache.flume.sink.ircorg.schweringirclib/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-irc-sink/src/main/java/org/apache/flume/sink/irc/IRCSink.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-irc-sink/src/main/java/org/apache/flume/sink/irc/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-irc-sink/src/main/java/org/apache/flume/sink/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-irc-sink/src/main/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-irc-sink/src/main/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-irc-sink/src/main/java/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-irc-sink/src/main/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-irc-sink/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-irc-sink/srcsendLinesendLine(org.apache.flume.Event)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-irc-sink/target/classes/org/apache/flume/sink/irc/IRCSink.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-irc-sink/target/classes/org/apache/flume/sink/irc/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-irc-sink/target/classes/org/apache/flume/sink/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-irc-sink/target/classes/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-irc-sink/target/classes/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-irc-sink/target/classes/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-irc-sink/target/classes/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-irc-sink/targetsplitCharssplitLinesnickIRC_CHANNEL_PREFIXDEFAULT_SPLIT_CHARSClass<IRCSink>"#""IRC sink disconnected"IRC sink disconnected"IRC sink error: {}"IRC sink error: {}"IRC sink error: {} - {}"IRC sink error: {} - {}nickPassmpnickNewtopic"nick""password""chan""splitlines"splitlines"splitchars"splitchars"No nick specified"No nick specified"No chan specified"No chan specified"Creating new connection to hostname:{} port:{}"Creating new connection to hostname:{} port:{}"join "join "Destroying connection to: {}:{}"Destroying connection to: {}:{}"IRC sink starting"IRC sink starting"Unable to create irc client using hostname:"Unable to create irc client using hostname:" port:" port:"IRC sink {} started"IRC sink {} started"IRC sink {} stopping"IRC sink {} stopping"IRC sink {} stopped. Metrics:{}"IRC sink {} stopped. Metrics:{}"event.empty"event.empty"event.irc"event.irc"Unable to get event from channel. Exception follows."Unable to get event from channel. Exception follows."Unable to communicate with IRC server. Exception follows."Unable to communicate with IRC server. Exception follows.FIXME: Mark ourselves as failed./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sinkFlume NG Morphline Solr Sink TODO fix spotbugspmd violations project.build.sourceEncodingsolr.version${solr-global.version}solr.expected.version sanity check to verify we actually run against the expected version rather than some outdated version org.apache.flume.sink.solr.morphline Morphlines needs the ua-parser library, which isn't published to Maven central.
         This repo from Twitter is where Morphlines gets this dependency from normally,
         but we need to specify HTTPS so that Maven won't ignore the repo.
    maven-twttrTwitter Public Maven Repohttps://maven.twttr.com Morphlines brings in a version of solr-core that needs the restlet jar which isn't
         in Maven central. We need to include the HTTPS version of the repo Solr gives in
         order to keep Maven using the repo.
    maven-restletRestlet Public Maven Repohttps://maven.restlet.talend.comorg.kitesdkkite-morphlines-all${kite.version}com.twitterparquet-avroorg.apache.parquet see http://lucene.apache.org/solr org.apache.solrsolr-test-framework${solr.version}slf4j-jdk14 instead use slf4j on top of log4j or logback  org.jdomjdomkite-morphlines-solr-coremaven-surefire-pluginargLine-Dtests.locale=en_usredirectTestOutputToFileorg.apache.ratapache-rat-pluginsrc/test/resources/**/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/main/java/org/apache/flume/sink/solr/morphline/BlobDeserializer.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/main/java/org/apache/flume/sink/solr/morphline/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/main/java/org/apache/flume/sink/solr/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/main/java/org/apache/flume/sink/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/main/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/main/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/main/java/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/main/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/target/classes/org/apache/flume/sink/solr/morphline/BlobDeserializer.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/target/classes/org/apache/flume/sink/solr/morphline/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/target/classes/org/apache/flume/sink/solr/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/target/classes/org/apache/flume/sink/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/target/classes/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/target/classes/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/target/classes/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/target/classes/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/targetDEFAULT_BUFFER_SIZEmaxBlobLength"maxBlobLength"Class<BlobDeserializer>"Configuration parameter "Configuration parameter " must be greater than zero: " must be greater than zero: blobblobLength"File length exceeds maxBlobLength ({}), truncating BLOB event!"File length exceeds maxBlobLength ({}), truncating BLOB event!contributor license agreements.  See the NOTICE file distributed withThe ASF licenses this file to You under the Apache License, Version 2.0the License.  You may obtain a copy of the License atA deserializer that reads a Binary Large Object (BLOB) per event, typicallyone BLOB per file; To be used in conjunction with Flume SpoolDirectorySource.Note that this approach is not suitable for very large objects because itbuffers up the entire BLOB.Reads a BLOB from a file and returns an eventEvent containing a BLOBBatch BLOB readList of events containing read BLOBs///////////////////////////////////////////////////////////////////////////// Nested classes:Builder implementations MUST have a public no-arg constructor/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/main/java/org/apache/flume/sink/solr/morphline/BlobHandler.javagetHeadersgetHeaders(javax.servlet.http.HttpServletRequest)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/target/classes/org/apache/flume/sink/solr/morphline/BlobHandler.classClass<BlobHandler>array"Request length exceeds maxBlobLength ({}), truncating BLOB event!"Request length exceeds maxBlobLength ({}), truncating BLOB event!"blobEvent: {}"blobEvent: {}HashMap<>()AbstractMap<>()HashMap<>(java.util.Map)HashMap<>(int)HashMap<>(int,float)requestHeaders"requestHeaders: {}"requestHeaders: {}BlobHandler for HTTPSource that returns event that contains the requestparameters as well as the Binary Large Object (BLOB) uploaded with thisrequest.Example client usage:curl --data-binary @sample-statuses-20120906-141433-medium.avro 'http://127.0.0.1:5140?resourceName=sample-statuses-20120906-141433-medium.avro' --header 'Content-Type:application/octet-stream' --verbose/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/main/java/org/apache/flume/sink/solr/morphline/MorphlineHandler.javaInterface to load Flume events into SolrBegins a transactionLoads the given event into SolrSends any outstanding documents to Solr and waits for a positiveor negative ack (i.e. exception). Depending on the outcome the callershould then commit or rollback the current flume transactioncorrespondingly.If there is a low-level I/O error.Performs a rollback of all non-committed documents pending.Note that this is not a true rollback as in databases. Content you have previously added toSolr may have already been committed due to autoCommit, buffer full, other client performing acommit etc. So this is only a best-effort rollback.Releases allocated resources/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/main/java/org/apache/flume/sink/solr/morphline/MorphlineHandlerImpl.javasetFinalChildsetFinalChild(org.kitesdk.morphline.api.Command)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/target/classes/org/apache/flume/sink/solr/morphline/MorphlineHandlerImpl.classsetMorphlineContextsetMorphlineContext(org.kitesdk.morphline.api.MorphlineContext)numExceptionRecordsnumFailedRecordsnumRecordsmappingTimermorphlineFileAndIdfinalChildmorphlinemorphlineContext"morphlineFile"morphlineFile"morphlineId"morphlineId"morphlineVariable"morphlineVariableClass<MorphlineHandlerImpl>"Missing parameter: "Missing parameter: "@"@faultToleranceConfig[]"morphline.app"morphline.app"numFailedRecords""numExceptionRecords"timerContext"Morphline {} failed to process record: {}"Morphline {} failed to process record: {}A {@link MorphlineHandler} that processes it's events using a morphline {@link Command} chain.Morphline variables can be passed from flume.conf to the morphline, e.g.:agent.sinks.solrSink.morphlineVariable.zkHost=127.0.0.1:2181/solr For test injection for interceptor/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/main/java/org/apache/flume/sink/solr/morphline/MorphlineInterceptor.javaborrowFromPoolborrowFromPool()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/target/classes/org/apache/flume/sink/solr/morphline/MorphlineInterceptor.classreturnToPoolreturnToPool(org.apache.flume.sink.solr.morphline.MorphlineInterceptor.LocalMorphlineInterceptor)Queue<LocalMorphlineInterceptor>Collection<LocalMorphlineInterceptor>Iterable<LocalMorphlineInterceptor>poolConcurrentLinkedQueue<LocalMorphlineInterceptor>/modules/java.base/java/util/concurrent/ConcurrentLinkedQueue.classAbstractQueue<LocalMorphlineInterceptor>AbstractCollection<LocalMorphlineInterceptor>ConcurrentLinkedQueue<LocalMorphlineInterceptor>()Spliterator<LocalMorphlineInterceptor>? super LocalMorphlineInterceptorConsumer<? super LocalMorphlineInterceptor>Iterator<LocalMorphlineInterceptor>Stream<LocalMorphlineInterceptor>BaseStream<LocalMorphlineInterceptor,Stream<LocalMorphlineInterceptor>>Predicate<? super LocalMorphlineInterceptor>? extends LocalMorphlineInterceptorCollection<? extends LocalMorphlineInterceptor>Iterable<? extends LocalMorphlineInterceptor>add(org.apache.flume.sink.solr.morphline.MorphlineInterceptor.LocalMorphlineInterceptor)AbstractCollection<LocalMorphlineInterceptor>()offer(org.apache.flume.sink.solr.morphline.MorphlineInterceptor.LocalMorphlineInterceptor)AbstractQueue<LocalMorphlineInterceptor>()/modules/java.base/java/util/concurrent/ConcurrentLinkedQueue$Node.classforEachFrom(java.util.function.Consumer,java.util.concurrent.ConcurrentLinkedQueue.Node)Node<LocalMorphlineInterceptor>succ(java.util.concurrent.ConcurrentLinkedQueue.Node)updateHeadupdateHead(java.util.concurrent.ConcurrentLinkedQueue.Node,java.util.concurrent.ConcurrentLinkedQueue.Node)ConcurrentLinkedQueueConcurrentLinkedQueue(java.util.Collection)ConcurrentLinkedQueue<LocalMorphlineInterceptor>(java.util.Collection)ConcurrentLinkedQueue()NEXTITEMresults/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/target/classes/org/apache/flume/sink/solr/morphline/MorphlineInterceptor$Builder.classtoEventtoEvent(org.kitesdk.morphline.api.Record)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/target/classes/org/apache/flume/sink/solr/morphline/MorphlineInterceptor$LocalMorphlineInterceptor.classcollectorArrayList<>(int)AbstractCollection<>()AbstractList<>()ArrayList<>(java.util.Collection)ArrayList<>()List<Record>SequencedCollection<Record>Collection<Record>Iterable<Record>Spliterator<Record>? super RecordConsumer<? super Record>Iterator<Record>Stream<Record>BaseStream<Record,Stream<Record>>Predicate<? super Record>? extends RecordCollection<? extends Record>Iterable<? extends Record>add(org.kitesdk.morphline.api.Record)addLast(org.kitesdk.morphline.api.Record)addFirst(org.kitesdk.morphline.api.Record)ListIterator<Record>add(int,org.kitesdk.morphline.api.Record)set(int,org.kitesdk.morphline.api.Record)Comparator<? super Record>UnaryOperator<Record>Function<Record,Record>Class<? extends LocalMorphlineInterceptor>Map<String,? extends LocalMorphlineInterceptor>LocalMorphlineInterceptor[]Constructor<? extends LocalMorphlineInterceptor>TypeVariable<Class<? extends LocalMorphlineInterceptor>>TypeVariable<Class<? extends LocalMorphlineInterceptor>>[]" must not generate more than one output record per input event" must not generate more than one output record per input eventMap<String,Collection<Object>>recordMapListMultimap<String,Object>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/ListMultimap.classMultimap<String,Object>BiConsumer<? super String,? super Object>Entry<String,Object>Collection<Entry<String,Object>>Iterable<Entry<String,Object>>Multiset<String>replaceValues(java.lang.String,java.lang.Iterable)Multimap<? extends String,? extends Object>putAll(java.lang.String,java.lang.Iterable)put(java.lang.String,java.lang.Object)Entry<String,Collection<Object>>Set<Entry<String,Collection<Object>>>Collection<Entry<String,Collection<Object>>>Iterable<Entry<String,Collection<Object>>>? super Collection<Object>? extends Collection<Object>BiFunction<? super Collection<Object>,? super Collection<Object>,? extends Collection<Object>>merge(java.lang.String,java.util.Collection,java.util.function.BiFunction)BiFunction<? super String,? super Collection<Object>,? extends Collection<Object>>Function<? super String,? extends Collection<Object>>replace(java.lang.String,java.util.Collection)replace(java.lang.String,java.util.Collection,java.util.Collection)putIfAbsent(java.lang.String,java.util.Collection)BiConsumer<? super String,? super Collection<Object>>getOrDefault(java.lang.Object,java.util.Collection)Collection<Collection<Object>>Iterable<Collection<Object>>Map<? extends String,? extends Collection<Object>>put(java.lang.String,java.util.Collection)firstValuesetValue(java.util.Collection)" must not generate more than one output value per record field" must not generate more than one output value per record field" must non generate attachments that are not a byte[] or InputStream" must non generate attachments that are not a byte[] or InputStream/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/target/classes/org/apache/flume/sink/solr/morphline/MorphlineInterceptor$Collector.classCollectorCollector()notificationFlume Interceptor that executes a morphline on events that are intercepted.Currently, there is a restriction in that the morphline must not generate more than one outputrecord for each input event. fail fast on morphline compilation exception guava guarantees that/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/main/java/org/apache/flume/sink/solr/morphline/MorphlineSink.javagetMaxBatchDurationMillisgetMaxBatchDurationMillis()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/target/classes/org/apache/flume/sink/solr/morphline/MorphlineSink.classgetMaxBatchSizegetMaxBatchSize()handlerClassmaxBatchDurationMillismaxBatchSize"batchDurationMillis"batchDurationMillis"handlerClass"Class<MorphlineSink>Map<String,MorphlineHandlerImpl>MorphlineHandlerImpl[]Constructor<MorphlineHandlerImpl>? super MorphlineHandlerImplClass<? super MorphlineHandlerImpl>TypeVariable<Class<MorphlineHandlerImpl>>TypeVariable<Class<MorphlineHandlerImpl>>[]"Starting Morphline Sink {} ..."Starting Morphline Sink {} ...tmpHandler"Morphline Sink {} started."Morphline Sink {} started."Morphline Sink {} stopping..."Morphline Sink {} stopping..."Morphline Sink {} stopped. Metrics: {}"Morphline Sink {} stopped. Metrics: {}batchEndTimemyChannelisMorphlineTransactionCommittednumEventsTaken"Flume event arrived {}"Flume event arrived {}"Morphline Sink "Morphline Sink ": Unable to process event from channel ": Unable to process event from channel t2": Unable to rollback morphline transaction. Exception follows.": Unable to rollback morphline transaction. Exception follows.t4": Unable to rollback Flume transaction. " +
              "Exception follows.": Unable to rollback Flume transaction. Exception follows.? extends MorphlineSinkClass<? extends MorphlineSink>Map<String,? extends MorphlineSink>MorphlineSink[]Constructor<? extends MorphlineSink>TypeVariable<Class<? extends MorphlineSink>>TypeVariable<Class<? extends MorphlineSink>>[]shortClassName" (" (Flume sink that extracts search documents from Flume events and processes them using a morphline{@link Command} chain.For testing onlyReturns the maximum number of events to take per flume transaction;override to customizeReturns the maximum duration per flume transaction; override to customize repeatedly take and process events from the Flume queueStreamEvent streamEvent = createStreamEvent(event); update metrics Ooops - need to rollback and back off rethrow original exception rethrow and backoff/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/main/java/org/apache/flume/sink/solr/morphline/MorphlineSolrSink.java"org.apache.solr.client.solrj.SolrServerException"org.apache.solr.client.solrj.SolrServerExceptionFlume sink that extracts search documents from Flume events, processes them using a morphline{@link Command} chain, and loads them into Apache Solr./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/main/java/org/apache/flume/sink/solr/morphline/UUIDInterceptor.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/target/classes/org/apache/flume/sink/solr/morphline/UUIDInterceptor.class"id"/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/target/classes/org/apache/flume/sink/solr/morphline/UUIDInterceptor$Builder.classFlume Interceptor that sets a universally unique identifier on all eventsthat are intercepted. By default this event header is named "id". we must preserve the existing id/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/test/resources/custom-mimetypes.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/test/resources/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/test Licensed to the Apache Software Foundation (ASF) under one or more contributor
  license agreements. See the NOTICE file distributed with this work for additional
  information regarding copyright ownership. The ASF licenses this file to
  You under the Apache License, Version 2.0 (the "License"); you may not use
  this file except in compliance with the License. You may obtain a copy of
  the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required
  by applicable law or agreed to in writing, software distributed under the
  License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS
  OF ANY KIND, either express or implied. See the License for the specific
  language governing permissions and limitations under the License. mime-infomime-typetext/space-separated-values*.ssvavro/binarymagicmatch0x4f626a01*.avromytwittertest/json+delimited+length[0-9]+(\r)?\n\\{"0:16application/hadoop-sequence-fileSEQ[\0-\6]/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/test/resources/log4j2.xml%-4r [%t] %-5p %c %x - %m%norg.kitesdk.morphlineorg.apache.solr.morphlineorg.apache.solr.update.processor.LogUpdateProcessororg.apache.solr.core.SolrCoreorg.apache.solr.search.SolrIndexSearcher/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/test/resources/solr/collection1/conf/currency.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/test/resources/solr/collection1/conf/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/test/resources/solr/collection1/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/test/resources/solr
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
 this work for additional information regarding copyright ownership.
 The ASF licenses this file to You under the Apache License, Version 2.0
 (the "License"); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
 Example exchange rates file for CurrencyField type named "currency" in example schema currencyConfigrates Updated from http://www.exchangerate.com/ at 2011-09-27 rateUSDARS4.333871commentARGENTINA PesoAUD1.025768AUSTRALIA DollarEUR0.743676European EuroBRL1.881093BRAZIL RealCAD1.030815CANADA DollarCLP519.0996CHILE PesoCNY6.387310CHINA YuanCZK18.47134CZECH REP. KorunaDKK5.515436DENMARK KroneHKD7.801922HONG KONG DollarHUF215.6169HUNGARY ForintISK118.1280ICELAND KronaINR49.49088INDIA RupeeXDR0.641358INTNL MON. FUND SDRILS3.709739ISRAEL SheqelJPY76.32419JAPAN YenKRW1169.173KOREA (SOUTH) WonKWD0.275142KUWAIT DinarMXN13.85895MEXICO PesoNZD1.285159NEW ZEALAND DollarNOK5.859035NORWAY KronePKR87.57007PAKISTAN RupeePEN2.730683PERU SolPHP43.62039PHILIPPINES PesoPLN3.310139POLAND ZlotyRON3.100932ROMANIA LeuRUB32.14663RUSSIA RubleSAR3.750465SAUDI ARABIA RiyalSGD1.299352SINGAPORE DollarZAR8.329761SOUTH AFRICA RandSEK6.883442SWEDEN KronaCHF0.906035SWITZERLAND FrancTWD30.40283TAIWAN DollarTHB30.89487THAILAND BahtAED3.672955U.A.E. DirhamUAH7.988582UKRAINE HryvniaGBP0.647910UNITED KINGDOM Pound Cross-rates for some common currencies 0.8699147.8000958.966508/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/test/resources/solr/collection1/conf/elevate.xml If this file is found in the config directory, it will only be
     loaded once at startup.  If it is found in Solr's data
     directory, it will be re-loaded every commit.

   See http://wiki.apache.org/solr/QueryElevationComponent for more info

elevatequerytextfoo bardocipodMA147LL/A put the actual ipod at the top IW-02 exclude this cable /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/test/resources/solr/collection1/conf/schema.xml
 This is the Solr schema file. This file should be named "schema.xml" and
 should be in the conf directory under the solr home
 (i.e. ./solr/conf/schema.xml by default)
 or located where the classloader for the Solr webapp can find it.

 This example schema is the recommended starting point for users.
 It should be kept correct and concise, usable out-of-the-box.

 For more information, on how to customize this file, please see
 http://wiki.apache.org/solr/SchemaXml

 PERFORMANCE NOTE: this schema includes many optional features and should not
 be used for benchmarking.  To improve performance one could
  - set stored="false" for all fields possible (esp large fields) when you
    only need to search on the field but don't need to return the original
    value.
  - set indexed="false" if you don't need to search on the field, but only
    return the field as a result of searching on other indexed fields.
  - remove all unneeded copyField statements
  - for best index size and searching performance, set "index" to false
    for all general text fields, use copyField to copy them to the
    catchall "text" field, and use that for searching.
  - For maximum indexing performance, use the StreamingUpdateSolrServer
    java client.
  - Remember to run the JVM in server mode, and use a higher logging level
    that avoids logging every request
example1.5 attribute "name" is the name of this schema and is only used for display purposes.
       version="x.y" is Solr's version number for the schema syntax and
       semantics.  It should not normally be changed by applications.

       1.0: multiValued attribute did not exist, all fields are multiValued
            by nature
       1.1: multiValued attribute introduced, false by default
       1.2: omitTermFreqAndPositions attribute introduced, true by default
            except for text fields.
       1.3: removed optional field compress feature
       1.4: autoGeneratePhraseQueries attribute introduced to drive QueryParser
            behavior when a single string produces multiple tokens.  Defaults
            to off for version >= 1.4
       1.5: omitNorms defaults to true for primitive field types
            (int, float, boolean, string...)
      Valid attributes for fields:
     name: mandatory - the name for the field
     type: mandatory - the name of a field type from the
       <types> fieldType section
     indexed: true if this field should be indexed (searchable or sortable)
     stored: true if this field should be retrievable
     multiValued: true if this field may contain multiple values per document
     omitNorms: (expert) set to true to omit the norms associated with
       this field (this disables length normalization and index-time
       boosting for the field, and saves some memory).  Only full-text
       fields or fields that need an index-time boost need norms.
       Norms are omitted for primitive (non-analyzed) types by default.
     termVectors: [false] set to true to store the term vector for a
       given field.
       When using MoreLikeThis, fields used for similarity should be
       stored for best performance.
     termPositions: Store position information with the term vector.
       This will increase storage costs.
     termOffsets: Store offset information with the term vector. This
       will increase storage costs.
     required: The field is required.  It will throw an error if the
       value does not exist
     default: a value that should be used if no value is specified
       when adding a document.
    field names should consist of alphanumeric or underscore characters only and
      not start with a digit.  This is not currently strictly enforced,
      but other field names will not have first class support from all components
      and back compatibility is not guaranteed.  Names with both leading and
      trailing underscores (e.g. _version_) are reserved.
   indexedstoredrequiredmultiValueduser_friends_counttintuser_locationlowercaseuser_descriptiontext_enuser_statuses_countuser_followers_countuser_nameuser_screen_namecreated_attdateretweet_countretweetedin_reply_to_user_idin_reply_to_status_idmedia_url_httpsexpanded_url file metadata file_download_urlfile_upload_urlfile_schemefile_hostfile_portfile_pathfile_namefile_lengthtlongfile_last_modifiedfile_ownerfile_groupfile_permissions_userfile_permissions_groupfile_permissions_otherfile_permissions_stickybit tika metadata content_type_version_dynamicFieldignored_* Field to use to determine and enforce document uniqueness.
      Unless this field is marked with required="false", it will be a required field
   uniqueKey DEPRECATED: The defaultSearchField is consulted by various query parsers when
  parsing a query string that isn't explicit about the field.  Machine (non-user)
  generated queries are best made explicit, or they can use the "df" request parameter
  which takes precedence over this.
  Note: Un-commenting defaultSearchField will be insufficient if your request handler
  in solrconfig.xml defines "df", which takes precedence. That would need to be removed.
 <defaultSearchField>text</defaultSearchField>  DEPRECATED: The defaultOperator (AND|OR) is consulted by various query parsers
  when parsing a query string to determine if a clause of the query should be marked as
  required or optional, assuming the clause isn't already marked by some operator.
  The default is OR, which is generally assumed so it is not a good idea to change it
  globally here.  The "q.op" request parameter takes precedence over this.
 <solrQueryParser defaultOperator="OR"/>  copyField commands copy one field to another at the time a document
        is added to the index.  It's used either to index the same field differently,
        or to add multiple fields to the same field for easier/faster searching.  types field type definitions. The "name" attribute is
       just a label to be used by field definitions.  The "class"
       attribute and any other attributes determine the real
       behavior of the fieldType.
         Class names starting with "solr" refer to java classes in a
       standard package such as org.apache.solr.analysis
     The StrField type is not analyzed, but indexed/stored verbatim. classsolr.StrFieldsortMissingLast boolean type: "true" or "false" solr.BoolField sortMissingLast and sortMissingFirst attributes are optional attributes are
         currently supported on types that are sorted internally as strings
         and on numeric types.
         This includes "string","boolean", and, as of 3.5 (and 4.x),
         int, float, long, date, double, including the "Trie" variants.
       - If sortMissingLast="true", then a sort on this field will cause documents
         without the field to come after documents with the field,
         regardless of the requested sort order (asc or desc).
       - If sortMissingFirst="true", then a sort on this field will cause documents
         without the field to come before documents with the field,
         regardless of the requested sort order.
       - If sortMissingLast="false" and sortMissingFirst="false" (the default),
         then default lucene sorting will be used which places docs without the
         field first in an ascending sort and last in a descending sort.
    
      Default numeric field types. For faster range queries, consider the tint/tfloat/tlong/tdouble types.
    solr.TrieIntFieldprecisionSteppositionIncrementGapsolr.TrieFloatFieldsolr.TrieLongFieldsolr.TrieDoubleField
     Numeric field types that index each value at various levels of precision
     to accelerate range queries when the number of values between the range
     endpoints is large. See the javadoc for NumericRangeQuery for internal
     implementation details.

     Smaller precisionStep values (specified in bits) will lead to more tokens
     indexed per value, slightly larger index size, and faster range queries.
     A precisionStep of 0 disables indexing at different precision levels.
    tfloattdouble The format for this date field is of the form 1995-12-31T23:59:59Z, and
         is a more restricted form of the canonical representation of dateTime
         http://www.w3.org/TR/xmlschema-2/#dateTime
         The trailing "Z" designates UTC time and is mandatory.
         Optional fractional seconds are allowed: 1995-12-31T23:59:59.999Z
         All other components are mandatory.

         Expressions can also be used to denote calculations that should be
         performed relative to "NOW" to determine the value, ie...

               NOW/HOUR
                  ... Round to the start of the current hour
               NOW-1DAY
                  ... Exactly 1 day prior to now
               NOW/DAY+6MONTHS+3DAYS
                  ... 6 months and 3 days in the future from the start of
                      the current day

         Consult the DateField javadocs for more information.

         Note: For faster range queries, consider the tdate type
      solr.TrieDateField A Trie based date field for faster date range queries and date faceting. Binary data type. The data should be sent/retrieved in as Base64 encoded Strings fieldtypesolr.BinaryField
      Note:
      These should only be used for compatibility with existing indexes (created with lucene or older Solr versions).
      Use Trie based fields instead. As of Solr 3.5 and 4.x, Trie based fields support sortMissingFirst/Last

      Plain numeric field types that store and index the text
      value verbatim (and hence don't correctly support range queries, since the
      lexicographic ordering isn't equal to the numeric ordering)
    pintsolr.IntFieldplongsolr.LongFieldpfloatsolr.FloatFieldpdoublesolr.DoubleFieldpdatesolr.DateField The "RandomSortField" is not used to store or search any
         data.  You can declare fields of this type it in your schema
         to generate pseudo-random orderings of your docs for sorting
         or function purposes.  The ordering is generated based on the field
         name and the version of the index. As long as the index version
         remains unchanged, and the same field name is reused,
         the ordering of the docs will be consistent.
         If you want different psuedo-random orderings of documents,
         for the same version of the index, use a dynamicField and
         change the field name in the request.
     solr.RandomSortField solr.TextField allows the specification of custom text analyzers
         specified as a tokenizer and a list of token filters. Different
         analyzers may be specified for indexing and querying.

         The optional positionIncrementGap puts space between multiple fields of
         this type on the same document, with the purpose of preventing false phrase
         matching across fields.

         For more info on customizing your analyzer chain, please see
         http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters
      One can also specify an existing Analyzer class that has a
         default constructor via the class attribute on the analyzer element.
         Example:
    <fieldType name="text_greek" class="solr.TextField">
      <analyzer class="org.apache.lucene.analysis.el.GreekAnalyzer"/>
    </fieldType>
     A text field that only splits on whitespace for exact matching of words text_wssolr.TextFieldanalyzertokenizersolr.WhitespaceTokenizerFactory A general text field that has reasonable, generic
         cross-language defaults: it tokenizes with StandardTokenizer,
     removes stop words from case-insensitive "stopwords.txt"
     (empty by default), and down cases.  At query time only, it
     also applies synonyms. text_generalsolr.StandardTokenizerFactorysolr.StopFilterFactoryignoreCasewordsstopwords.txtenablePositionIncrements in this example, we will only use synonyms at query time
        <filter class="solr.SynonymFilterFactory" synonyms="index_synonyms.txt" ignoreCase="true" expand="false"/>
        solr.LowerCaseFilterFactorysolr.SynonymFilterFactorysynonymssynonyms.txtexpand A text field with defaults appropriate for English: it
         tokenizes with StandardTokenizer, removes English stop words
         (lang/stopwords_en.txt), down cases, protects words from protwords.txt, and
         finally applies Porter's stemming.  The query time analyzer
         also applies synonyms from synonyms.txt.  Case insensitive stop word removal.
          add enablePositionIncrements=true in both the index and query
          analyzers to leave a 'gap' for more accurate phrase queries.
        lang/stopwords_en.txtsolr.EnglishPossessiveFilterFactorysolr.KeywordMarkerFilterFactoryprotwords.txt Optionally you may want to use this less aggressive stemmer instead of PorterStemFilterFactory:
        <filter class="solr.EnglishMinimalStemFilterFactory"/>
    solr.PorterStemFilterFactory A text field with defaults appropriate for English, plus
      aggressive word-splitting and autophrase features enabled.
      This field is just like text_en, except it adds
      WordDelimiterFilter to enable splitting and matching of
      words on case-change, alpha numeric boundaries, and
      non-alphanumeric chars.  This means certain compound word
      cases will work, for example query "wi fi" will match
      document "WiFi" or "wi-fi".
        text_en_splittingautoGeneratePhraseQueriessolr.WordDelimiterFilterFactorygenerateWordPartsgenerateNumberPartscatenateWordscatenateNumberscatenateAllsplitOnCaseChange Less flexible matching, but less false matches.  Probably not ideal for product names,
         but may be good for SKUs.  Can insert dashes in the wrong place and still match. text_en_splitting_tightsolr.EnglishMinimalStemFilterFactory this filter can remove any duplicate tokens that appear at the same position - sometimes
             possible with WordDelimiterFilter in conjuncton with stemming. solr.RemoveDuplicatesTokenFilterFactory Just like text_general except it reverses the characters of
     each token, to enable more efficient leading wildcard queries. text_general_revsolr.ReversedWildcardFilterFactorywithOriginalmaxPosAsteriskmaxPosQuestionmaxFractionAsterisk0.33 charFilter + WhitespaceTokenizer  
    <fieldType name="text_char_norm" class="solr.TextField" positionIncrementGap="100" >
      <analyzer>
        <charFilter class="solr.MappingCharFilterFactory" mapping="mapping-ISOLatin1Accent.txt"/>
        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
      </analyzer>
    </fieldType>
     This is an example of using the KeywordTokenizer along
         With various TokenFilterFactories to produce a sortable field
         that does not include some properties of the source text
      alphaOnlySortomitNorms KeywordTokenizer does no actual tokenizing, so the entire
             input string is preserved as a single token
          solr.KeywordTokenizerFactory The LowerCase TokenFilter does what you expect, which can be
             when you want your sorting to be case insensitive
           The TrimFilter removes any leading or trailing whitespace solr.TrimFilterFactory The PatternReplaceFilter gives you the flexibility to use
             Java Regular expression to replace any sequence of characters
             matching a pattern with an arbitrary replacement string,
             which may include back references to portions of the original
             string matched by the pattern.

             See the Java Regular Expression documentation for more
             information on pattern and replacement string syntax.

             http://java.sun.com/j2se/1.6.0/docs/api/java/util/regex/package-summary.html
          solr.PatternReplaceFilterFactory([^a-z])
    <fieldtype name="phonetic" stored="false" indexed="true" class="solr.TextField" >
      <analyzer>
        <tokenizer class="solr.StandardTokenizerFactory"/>
        <filter class="solr.DoubleMetaphoneFilterFactory" inject="false"/>
      </analyzer>
    </fieldtype>
    payloads
        The DelimitedPayloadTokenFilter can put payloads on tokens... for example,
        a token of "foo|1.4"  would be indexed as "foo" with a payload of 1.4f
        Attributes of the DelimitedPayloadTokenFilterFactory :
         "delimiter" - a one character delimiter. Default is | (pipe)
        "encoder" - how to encode the following value into a playload
        float -> org.apache.lucene.analysis.payloads.FloatEncoder,
        integer -> o.a.l.a.p.IntegerEncoder
        identity -> o.a.l.a.p.IdentityEncoder
            Fully Qualified class name implementing PayloadEncoder, Encoder must have a no arg constructor.
         solr.DelimitedPayloadTokenFilterFactory lowercases the entire field value, keeping it as a single token.  
      Example of using PathHierarchyTokenizerFactory at index time, so
      queries for paths match documents at that path, or in descendent paths
    descendent_pathsolr.PathHierarchyTokenizerFactorydelimiter
      Example of using PathHierarchyTokenizerFactory at query time, so
      queries for paths match documents at that path, or in ancestor paths
    ancestor_path since fields of this type are by default not stored or indexed,
         any data added to them will be ignored outright.   This point type indexes the coordinates as separate fields (subFields)
      If subFieldType is defined, it references a type, and a dynamic field
      definition is created matching *___<typename>.  Alternately, if
      subFieldSuffix is defined, that is used to create the subFields.
      Example: if subFieldType="double", then the coordinates would be
        indexed in fields myloc_0___double,myloc_1___double.
      Example: if subFieldSuffix="_d" then the coordinates would be indexed
        in fields myloc_0_d,myloc_1_d
      The subFields are an implementation detail of the fieldType, and end
      users normally should not need to know about them.
     pointsolr.PointTypedimensionsubFieldSuffix_d A specialized field for geospatial search. If indexed, this fieldType must not be multivalued. locationsolr.LatLonType_coordinate
    A Geohash is a compact representation of a latitude longitude pair in a single field.
    See http://wiki.apache.org/solr/SpatialSearch
   geohashsolr.GeoHashField Money/currency field type. See http://wiki.apache.org/solr/MoneyFieldType
        Parameters:
          defaultCurrency: Specifies the default currency if none specified. Defaults to "USD"
          precisionStep:   Specifies the precisionStep for the TrieLong field used for the amount
          providerClass:   Lets you plug in other exchange provider backend:
                           solr.FileExchangeRateProvider is the default and takes one parameter:
                             currencyConfig: name of an xml file holding exhange rates
                           solr.OpenExchangeRatesOrgProvider uses rates from openexchangerates.org:
                             ratesFileLocation: URL or path to rates JSON file (default latest.json on the web)
                             refreshInterval: Number of minutes between each rates fetch (default: 1440, min: 60)
   currencysolr.CurrencyFielddefaultCurrencycurrency.xml some examples for different languages (generally ordered by ISO code)  Arabic text_ar for any non-arabic lang/stopwords_ar.txt normalizes  to , etc solr.ArabicNormalizationFilterFactorysolr.ArabicStemFilterFactory Bulgarian text_bglang/stopwords_bg.txtsolr.BulgarianStemFilterFactory Catalan text_ca removes l', etc solr.ElisionFilterFactoryarticleslang/contractions_ca.txtlang/stopwords_ca.txtsolr.SnowballPorterFilterFactorylanguageCatalan CJK bigram (see text_ja for a Japanese configuration using morphological analysis) text_cjk normalize width before bigram, as e.g. half-width dakuten combine  solr.CJKWidthFilterFactory for any non-CJK solr.CJKBigramFilterFactory Czech text_czlang/stopwords_cz.txtsolr.CzechStemFilterFactory Danish text_dalang/stopwords_da.txtsnowballDanish German text_delang/stopwords_de.txtsolr.GermanNormalizationFilterFactorysolr.GermanLightStemFilterFactory less aggressive: <filter class="solr.GermanMinimalStemFilterFactory"/>  more aggressive: <filter class="solr.SnowballPorterFilterFactory" language="German2"/>  Greek text_el greek specific lowercase for sigma solr.GreekLowerCaseFilterFactorylang/stopwords_el.txtsolr.GreekStemFilterFactory Spanish text_eslang/stopwords_es.txtsolr.SpanishLightStemFilterFactory more aggressive: <filter class="solr.SnowballPorterFilterFactory" language="Spanish"/>  Basque text_eulang/stopwords_eu.txtBasque Persian text_fa for ZWNJ charFiltersolr.PersianCharFilterFactorysolr.PersianNormalizationFilterFactorylang/stopwords_fa.txt Finnish text_filang/stopwords_fi.txtFinnish less aggressive: <filter class="solr.FinnishLightStemFilterFactory"/>  French text_frlang/contractions_fr.txtlang/stopwords_fr.txtsolr.FrenchLightStemFilterFactory less aggressive: <filter class="solr.FrenchMinimalStemFilterFactory"/>  more aggressive: <filter class="solr.SnowballPorterFilterFactory" language="French"/>  Irish text_ga removes d', etc lang/contractions_ga.txt removes n-, etc. position increments is intentionally false! lang/hyphenations_ga.txtsolr.IrishLowerCaseFilterFactorylang/stopwords_ga.txtIrish Galician text_gllang/stopwords_gl.txtsolr.GalicianStemFilterFactory less aggressive: <filter class="solr.GalicianMinimalStemFilterFactory"/>  Hindi text_hi normalizes unicode representation solr.IndicNormalizationFilterFactory normalizes variation in spelling solr.HindiNormalizationFilterFactorylang/stopwords_hi.txtsolr.HindiStemFilterFactory Hungarian text_hulang/stopwords_hu.txtHungarian less aggressive: <filter class="solr.HungarianLightStemFilterFactory"/>  Armenian text_hylang/stopwords_hy.txtArmenian Indonesian text_idlang/stopwords_id.txt for a less aggressive approach (only inflectional suffixes), set stemDerivational to false solr.IndonesianStemFilterFactorystemDerivational Italian text_itlang/contractions_it.txtlang/stopwords_it.txtsolr.ItalianLightStemFilterFactory more aggressive: <filter class="solr.SnowballPorterFilterFactory" language="Italian"/>  Latvian text_lvlang/stopwords_lv.txtsolr.LatvianStemFilterFactory Dutch text_nllang/stopwords_nl.txtsolr.StemmerOverrideFilterFactorydictionarylang/stemdict_nl.txtDutch Norwegian text_nolang/stopwords_no.txtNorwegian less aggressive: <filter class="solr.NorwegianLightStemFilterFactory"/>  singular/plural: <filter class="solr.NorwegianMinimalStemFilterFactory"/>  Portuguese text_ptlang/stopwords_pt.txtsolr.PortugueseLightStemFilterFactory less aggressive: <filter class="solr.PortugueseMinimalStemFilterFactory"/>  more aggressive: <filter class="solr.SnowballPorterFilterFactory" language="Portuguese"/>  most aggressive: <filter class="solr.PortugueseStemFilterFactory"/>  Romanian text_rolang/stopwords_ro.txtRomanian Russian text_rulang/stopwords_ru.txtRussian less aggressive: <filter class="solr.RussianLightStemFilterFactory"/>  Swedish text_svlang/stopwords_sv.txtSwedish less aggressive: <filter class="solr.SwedishLightStemFilterFactory"/>  Thai text_thsolr.ThaiWordFilterFactorylang/stopwords_th.txt Turkish text_trsolr.TurkishLowerCaseFilterFactorylang/stopwords_tr.txtTurkish Similarity is the scoring routine for each document vs. a query.
       A custom Similarity or SimilarityFactory may be specified here, but
       the default is fine for most applications.
       For more info: http://wiki.apache.org/solr/SchemaXml#Similarity
    
     <similarity class="com.example.solr.CustomSimilarityFactory">
       <str name="paramkey">param value</str>
     </similarity>
    /Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/test/resources/solr/collection1/conf/solrconfig.xml 
     For more details about configurations options that may appear in
     this file, see http://wiki.apache.org/solr/SolrConfigXml. 
 In all configuration below, a prefix of "solr." for class names
       is an alias that causes solr to search appropriate packages,
       including org.apache.solr.(search|update|request|core|analysis)

       You may also specify a fully qualified Java classname if you
       have your own custom plugins.
     Controls what version of Lucene various components of Solr
       adhere to.  Generally, you want to use the latest version to
       get all bug fixes and improvements. It is highly recommended
       that you fully re-index after changing this setting as it can
       affect both how text is indexed and queried.
  luceneMatchVersionLUCENE_43 lib directives can be used to instruct Solr to load an Jars
       identified and use them to resolve any "plugins" specified in
       your solrconfig.xml or schema.xml (ie: Analyzers, Request
       Handlers, etc...).

       All directories and paths are resolved relative to the
       instanceDir.

       If a "./lib" directory exists in your instanceDir, all files
       found in it are included as if you had used the following
       syntax...
       
              <lib dir="./lib" />
     A 'dir' option by itself adds any files found in the directory 
       to the classpath, this is useful for including all jars in a
       directory.
    
     <lib dir="../add-everything-found-in-this-dir-to-the-classpath" />
   When a 'regex' is specified in addition to a 'dir', only the
       files in that directory which completely match the regex
       (anchored on both ends) will be included.
    ../../../contrib/extraction/lib.*\.jar../../../dist/solr-cell-\d.*\.jar../../../contrib/clustering/lib/solr-clustering-\d.*\.jar../../../contrib/langid/lib/solr-langid-\d.*\.jar../../../contrib/velocity/libsolr-velocity-\d.*\.jar If a 'dir' option (with or without a regex) is used and nothing
       is found that matches, it will be ignored
    /non/existent/dir/yields/warning an exact 'path' can be used instead of a 'dir' to specify a 
       specific file.  This will cause a serious error to be logged if 
       it can't be loaded.
    
     <lib path="../a-jar-that-does-not-exist.jar" /> 
   Data Directory

       Used to specify an alternate directory to hold all index data
       other than the default ./data under the Solr home.  If
       replication is in use, this should match the replication
       configuration.
    <dataDir>/data/3/collection1/data</dataDir>dataDir${solr.data.dir:} The DirectoryFactory to use for indexes.
       
       solr.StandardDirectoryFactory is filesystem
       based and tries to pick the best implementation for the current
       JVM and platform.  solr.NRTCachingDirectoryFactory, the default,
       wraps solr.StandardDirectoryFactory and caches small files in memory
       for better NRT performance.

       One can force a particular implementation via solr.MMapDirectoryFactory,
       solr.NIOFSDirectoryFactory, or solr.SimpleFSDirectoryFactory.

       solr.RAMDirectoryFactory is memory based, not
       persistent, and doesn't work with replication.
    directoryFactoryDirectoryFactory${solr.directoryFactory:solr.NRTCachingDirectoryFactory} The CodecFactory for defining the format of the inverted index.
       The default implementation is SchemaCodecFactory, which is the official Lucene
       index format, but hooks into the schema to provide per-field customization of
       the postings lists and per-document values in the fieldType element
       (postingsFormat/docValuesFormat). Note that most of the alternative implementations
       are experimental, so if you choose to customize the index format, its a good
       idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
       before upgrading to a newer version to avoid unnecessary reindexing.
  solr.SchemaCodecFactory ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
       Index Config - These settings control low-level behavior of indexing
       Most example settings here show the default value, but are commented
       out, to more easily see where customizations have been made.
       
       Note: This replaces <indexDefaults> and <mainIndex> from older versions
       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ indexConfig maxFieldLength was removed in 4.0. To get similar behavior, include a 
         LimitTokenCountFilterFactory in your fieldType definition. E.g. 
     <filter class="solr.LimitTokenCountFilterFactory" maxTokenCount="10000"/>
     Maximum time to wait for a write lock (ms) for an IndexWriter. Default: 1000  <writeLockTimeout>1000</writeLockTimeout>   The maximum number of simultaneous threads that may be
         indexing documents at once in IndexWriter; if more than this
         many threads arrive they will wait for others to finish.
         Default in Solr/Lucene is 8. maxIndexingThreads${solr.maxIndexingThreads:8} Expert: Enabling compound file will use less files for the index, 
         using fewer file descriptors on the expense of performance decrease. 
         Default in Lucene is "true". Default in Solr is "false" (since 3.6)  <useCompoundFile>false</useCompoundFile>  ramBufferSizeMB sets the amount of RAM that may be used by Lucene
         indexing for buffering added documents and deletions before they are
         flushed to the Directory.
         maxBufferedDocs sets a limit on the number of documents buffered
         before flushing.
         If both ramBufferSizeMB and maxBufferedDocs is set, then
         Lucene will flush based on whichever limit is hit first.  ramBufferSizeMB <maxBufferedDocs>1000</maxBufferedDocs>  Expert: Merge Policy 
         The Merge Policy in Lucene controls how merging of segments is done.
         The default since Solr/Lucene 3.3 is TieredMergePolicy.
         The default since Lucene 2.3 was the LogByteSizeMergePolicy,
         Even older versions of Lucene used LogDocMergePolicy.
      
        <mergePolicy class="org.apache.lucene.index.TieredMergePolicy">
          <int name="maxMergeAtOnce">10</int>
          <int name="segmentsPerTier">10</int>
        </mergePolicy>
       Merge Factor
         The merge factor controls how many segments will get merged at a time.
         For TieredMergePolicy, mergeFactor is a convenience parameter which
         will set both MaxMergeAtOnce and SegmentsPerTier at once.
         For LogByteSizeMergePolicy, mergeFactor decides how many new segments
         will be allowed before they are merged into one.
         Default is 10 for both merge policies.
       
    <mergeFactor>10</mergeFactor>
       Expert: Merge Scheduler
         The Merge Scheduler in Lucene controls how merges are
         performed.  The ConcurrentMergeScheduler (Lucene 2.3 default)
         can perform merges in the background using separate threads.
         The SerialMergeScheduler (Lucene 2.2 default) does not.
      
       <mergeScheduler class="org.apache.lucene.index.ConcurrentMergeScheduler"/>
        LockFactory 

         This option specifies which Lucene LockFactory implementation
         to use.
      
         single = SingleInstanceLockFactory - suggested for a
                  read-only index or when there is no possibility of
                  another process trying to modify the index.
         native = NativeFSLockFactory - uses OS native file locking.
                  Do not use when multiple solr webapps in the same
                  JVM are attempting to share a single index.
         simple = SimpleFSLockFactory  - uses a plain file for locking

         Defaults: 'native' is default for Solr3.6 and later, otherwise
                   'simple' is the default

         More details on the nuances of each LockFactory...
         http://wiki.apache.org/lucene-java/AvailableLockFactories
     <lockType>native</lockType>  Unlock On Startup

         If true, unlock any held write or commit locks on startup.
         This defeats the locking mechanism that allows multiple
         processes to safely access a lucene index, and should be used
         with care. Default is "false".

         This is not needed if lock type is 'none' or 'single'
     
    <unlockOnStartup>false</unlockOnStartup>
       Expert: Controls how often Lucene loads terms into memory
         Default is 128 and is likely good for most everyone.
       <termIndexInterval>128</termIndexInterval>  If true, IndexReaders will be reopened (often more efficient)
         instead of closed and then opened. Default: true
       
    <reopenReaders>true</reopenReaders>
       Commit Deletion Policy

         Custom deletion policies can be specified here. The class must
         implement org.apache.lucene.index.IndexDeletionPolicy.

         http://lucene.apache.org/java/3_5_0/api/core/org/apache/lucene/index/IndexDeletionPolicy.html

         The default Solr IndexDeletionPolicy implementation supports
         deleting index commit points on number of commits, age of
         commit point and optimized status.
         
         The latest commit point should always be preserved regardless
         of the criteria.
     
    <deletionPolicy class="solr.SolrDeletionPolicy">
     The number of commit points to be kept  <str name="maxCommitsToKeep">1</str>  The number of optimized commit points to be kept  <str name="maxOptimizedCommitsToKeep">0</str> 
          Delete all commit points once they have reached the given age.
          Supports DateMathParser syntax e.g.
        
         <str name="maxCommitAge">30MINUTES</str>
         <str name="maxCommitAge">1DAY</str>
       
    </deletionPolicy>
     Lucene Infostream
       
         To aid in advanced debugging, Lucene provides an "InfoStream"
         of detailed information when indexing.

         Setting The value to true will instruct the underlying Lucene
         IndexWriter to write its debugging info the specified file
       <infoStream file="INFOSTREAM.txt">false</infoStream>  JMX
       
       This example enables JMX if and only if an existing MBeanServer
       is found, use this if you want to configure JMX through JVM
       parameters. Remove this to disable exposing Solr configuration
       and statistics to JMX.

       For more details see http://wiki.apache.org/solr/SolrJmx
    jmx If you want to connect to a particular server, specify the
       agentId 
     <jmx agentId="myAgent" />  If you want to start a new MBeanServer, specify the serviceUrl  <jmx serviceUrl="service:jmx:rmi:///jndi/rmi://localhost:9999/solr"/>
     The default high-performance update handler updateHandlersolr.DirectUpdateHandler2 Enables a transaction log, used for real-time get, durability, and
         and solr cloud replica recovery.  The log can grow as big as
         uncommitted changes to the index, so use of a hard autoCommit
         is recommended (see below).
         "dir" - the target directory for transaction logs, defaults to the
                solr data directory.  updateLogstr${solr.ulog.dir:} AutoCommit

         Perform a hard commit automatically under certain conditions.
         Instead of enabling autoCommit, consider using "commitWithin"
         when adding documents. 

         http://wiki.apache.org/solr/UpdateXmlMessages

         maxDocs - Maximum number of documents to add since the last
                   commit before automatically triggering a new commit.

         maxTime - Maximum amount of time in ms that is allowed to pass
                   since a document was added before automaticly
                   triggering a new commit. 
         openSearcher - if false, the commit causes recent index changes
         to be flushed to stable storage, but does not cause a new
         searcher to be opened to make those changes visible.
      autoCommitmaxTime${solr.autoCommit.maxTime:60000}openSearcher softAutoCommit is like autoCommit except it causes a
         'soft' commit which only ensures that changes are visible
         but does not ensure that data is synced to disk.  This is
         faster and more near-realtime friendly than a hard commit.
      autoSoftCommit${solr.autoSoftCommit.maxTime:1000} Update Related Event Listeners
         
         Various IndexWriter related events can trigger Listeners to
         take actions.

         postCommit - fired after every commit or optimize command
         postOptimize - fired after every optimize command
       The RunExecutableListener executes an external command from a
         hook such as postCommit or postOptimize.
         
         exe - the name of the executable to run
         dir - dir to use as the current working directory. (default=".")
         wait - the calling thread waits until the executable returns. 
                (default="true")
         args - the arguments to pass to the program.  (default is none)
         env - environment variables to set.  (default is none)
       This example shows how RunExecutableListener could be used
         with the script based replication...
         http://wiki.apache.org/solr/CollectionDistribution
      
       <listener event="postCommit" class="solr.RunExecutableListener">
         <str name="exe">solr/bin/snapshooter</str>
         <str name="dir">.</str>
         <bool name="wait">true</bool>
         <arr name="args"> <str>arg1</str> <str>arg2</str> </arr>
         <arr name="env"> <str>MYVAR=val1</str> </arr>
       </listener>
       IndexReaderFactory

       Use the following format to specify a custom IndexReaderFactory,
       which allows for alternate IndexReader implementations.

       ** Experimental Feature **

       Please note - Using a custom IndexReaderFactory may prevent
       certain other features from working. The API to
       IndexReaderFactory may change without warning or may even be
       removed from future releases if the problems cannot be
       resolved.


       ** Features that may not work with custom IndexReaderFactory **

       The ReplicationHandler assumes a disk-resident index. Using a
       custom IndexReader implementation may cause incompatibility
       with ReplicationHandler and may cause replication to not work
       correctly. See SOLR-1366 for details.

    
  <indexReaderFactory name="IndexReaderFactory" class="package.class">
    <str name="someArg">Some Value</str>
  </indexReaderFactory >
   By explicitly declaring the Factory, the termIndexDivisor can
       be specified.
    
     <indexReaderFactory name="IndexReaderFactory" 
                         class="solr.StandardIndexReaderFactory">
       <int name="setTermIndexDivisor">12</int>
     </indexReaderFactory >
     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
       Query section - these settings control query time things like caches
       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Max Boolean Clauses

         Maximum number of clauses in each BooleanQuery,  an exception
         is thrown if exceeded.

         ** WARNING **
         
         This option actually modifies a global Lucene property that
         will affect all SolrCores.  If multiple solrconfig.xml files
         disagree on this property, the value at any given moment will
         be based on the last SolrCore to be initialized.
         
      maxBooleanClauses Solr Internal Query Caches

         There are two implementations of cache available for Solr,
         LRUCache, based on a synchronized LinkedHashMap, and
         FastLRUCache, based on a ConcurrentHashMap.  

         FastLRUCache has faster gets and slower puts in single
         threaded operation and thus is generally faster than LRUCache
         when the hit ratio of the cache is high (> 75%), and may be
         faster under other scenarios on multi-cpu systems.
     Filter Cache

         Cache used by SolrIndexSearcher for filters (DocSets),
         unordered sets of *all* documents that match a query.  When a
         new searcher is opened, its caches may be prepopulated or
         "autowarmed" using data from caches in the old searcher.
         autowarmCount is the number of items to prepopulate.  For
         LRUCache, the autowarmed items will be the most recently
         accessed items.

         Parameters:
           class - the SolrCache implementation LRUCache or
               (LRUCache or FastLRUCache)
           size - the maximum number of entries in the cache
           initialSize - the initial capacity (number of entries) of
               the cache.  (see java.util.HashMap)
           autowarmCount - the number of entries to prepopulate from
               and old cache.  
      filterCachesolr.FastLRUCacheinitialSizeautowarmCount Query Result Cache
         
         Caches results of searches - ordered lists of document ids
         (DocList) based on a query, a sort, and the range of documents requested.  
      queryResultCachesolr.LRUCache Document Cache

         Caches Lucene Document objects (the stored fields for each
         document).  Since Lucene internal document ids are transient,
         this cache will not be autowarmed.  
      documentCache Field Value Cache
         
         Cache used to hold field values that are quickly accessible
         by document id.  The fieldValueCache is created by default
         even if not configured here.
      
       <fieldValueCache class="solr.FastLRUCache"
                        size="512"
                        autowarmCount="128"
                        showItems="32" />
       Custom Cache

         Example of a generic cache.  These caches may be accessed by
         name through SolrIndexSearcher.getCache(),cacheLookup(), and
         cacheInsert().  The purpose is to enable easy caching of
         user/application level data.  The regenerator argument should
         be specified as an implementation of solr.CacheRegenerator 
         if autowarming is desired.  
      
       <cache name="myUserCache"
              class="solr.LRUCache"
              size="4096"
              initialSize="1024"
              autowarmCount="1024"
              regenerator="com.mycompany.MyRegenerator"
              />
       Lazy Field Loading

         If true, stored fields that are not requested will be loaded
         lazily.  This can result in a significant speed improvement
         if the usual case is to not load all stored fields,
         especially if the skipped fields are large compressed text
         fields.
    enableLazyFieldLoading Use Filter For Sorted Query

        A possible optimization that attempts to use a filter to
        satisfy a search.  If the requested sort does not include
        score, then the filterCache will be checked for a filter
        matching the query. If found, the filter will be used as the
        source of document ids, and then the sort will be applied to
        that.

        For most situations, this will not be useful unless you
        frequently get the same search repeatedly with different sort
        options, and none of them ever use "score"
     
      <useFilterForSortedQuery>true</useFilterForSortedQuery>
      Result Window Size

        An optimization for use with the queryResultCache.  When a search
        is requested, a superset of the requested number of document ids
        are collected.  For example, if a search for a particular query
        requests matching documents 10 through 19, and queryWindowSize is 50,
        then documents 0 through 49 will be collected and cached.  Any further
        requests in that range can be satisfied via the cache.  
     queryResultWindowSize Maximum number of documents to cache for any entry in the
        queryResultCache. 
     queryResultMaxDocsCached Query Related Event Listeners

        Various IndexSearcher related events can trigger Listeners to
        take actions.

        newSearcher - fired whenever a new searcher is being prepared
        and there is a current searcher handling requests (aka
        registered).  It can be used to prime certain caches to
        prevent long request times for certain requests.

        firstSearcher - fired whenever a new searcher is being
        prepared but there is no current registered searcher to handle
        requests or to gain autowarming data from.

        
      QuerySenderListener takes an array of NamedList and executes a
         local query request for each NamedList in sequence. 
      listenernewSearchersolr.QuerySenderListenerarrqueries
           <lst><str name="q">solr</str><str name="sort">price asc</str></lst>
           <lst><str name="q">rocks</str><str name="sort">weight asc</str></lst>
          firstSearcherlstqstatic firstSearcher warming in solrconfig.xml Use Cold Searcher

         If a search request comes in and there is no current
         registered searcher, then immediately register the still
         warming searcher and use it.  If "false" then all requests
         will block until the first searcher is done warming.
      useColdSearcher Max Warming Searchers
         
         Maximum number of searchers that may be warming in the
         background concurrently.  An error is returned if this limit
         is exceeded.

         Recommend values of 1-2 for read-only slaves, higher for
         masters w/o cache warming.
      maxWarmingSearchers Request Dispatcher

       This section contains instructions for how the SolrDispatchFilter
       should behave when processing requests for this SolrCore.

       handleSelect is a legacy option that affects the behavior of requests
       such as /select?qt=XXX

       handleSelect="true" will cause the SolrDispatchFilter to process
       the request and dispatch the query to a handler specified by the
       "qt" param, assuming "/select" isn't already registered.

       handleSelect="false" will cause the SolrDispatchFilter to
       ignore "/select" requests, resulting in a 404 unless a handler
       is explicitly registered with the name "/select"

       handleSelect="true" is not recommended for new users, but is the default
       for backwards compatibility
    requestDispatcherhandleSelect Request Parsing

         These settings indicate how Solr Requests may be parsed, and
         what restrictions may be placed on the ContentStreams from
         those requests

         enableRemoteStreaming - enables use of the stream.file
         and stream.url parameters for specifying remote streams.

         multipartUploadLimitInKB - specifies the max size of
         Multipart File Uploads that Solr will allow in a Request.
         
         *** WARNING ***
         The settings below authorize Solr to fetch remote files, You
         should make sure your system has some authentication before
         using enableRemoteStreaming="true"

      requestParsersenableRemoteStreamingmultipartUploadLimitInKBformdataUploadLimitInKB HTTP Caching

         Set HTTP caching related parameters (for proxy caches and clients).

         The options below instruct Solr not to output any HTTP Caching
         related headers
      httpCachingnever304 If you include a <cacheControl> directive, it will be used to
         generate a Cache-Control header (as well as an Expires header
         if the value contains "max-age=")
         
         By default, no Cache-Control header is generated.
         
         You can use the <cacheControl> option even if you have set
         never304="true"
      
       <httpCaching never304="true" >
         <cacheControl>max-age=30, public</cacheControl> 
       </httpCaching>
       To enable Solr to respond with automatically generated HTTP
         Caching headers, and to response to Cache Validation requests
         correctly, set the value of never304="false"
         
         This will cause Solr to generate Last-Modified and ETag
         headers based on the properties of the Index.

         The following options can also be specified to affect the
         values of these headers...

         lastModFrom - the default value is "openTime" which means the
         Last-Modified value (and validation against If-Modified-Since
         requests) will all be relative to when the current Searcher
         was opened.  You can change it to lastModFrom="dirLastMod" if
         you want the value to exactly correspond to when the physical
         index was last modified.

         etagSeed="..." is an option you can change to force the ETag
         header (and validation against If-None-Match requests) to be
         different even if the index has not changed (ie: when making
         significant changes to your config file)

         (lastModifiedFrom and etagSeed are both ignored if you use
         the never304="true" option)
      
       <httpCaching lastModifiedFrom="openTime"
                    etagSeed="Solr">
         <cacheControl>max-age=30, public</cacheControl> 
       </httpCaching>
       Request Handlers 

       http://wiki.apache.org/solr/SolrRequestHandler

       Incoming queries will be dispatched to a specific handler by name
       based on the path specified in the request.

       Legacy behavior: If the request path uses "/select" but no Request
       Handler has that name, and if handleSelect="true" has been specified in
       the requestDispatcher, then the Request Handler is dispatched based on
       the qt parameter.  Handlers without a leading '/' are accessed this way
       like so: http://host/app/[core/]select?qt=name  If no qt is
       given, then the requestHandler that declares default="true" will be
       used or the one named "standard".
       
       If a Request Handler is declared with startup="lazy", then it will
       not be initialized until the first request that uses it.

     SearchHandler

       http://wiki.apache.org/solr/SearchHandler

       For processing Search Queries, the primary Request Handler
       provided with Solr is "SearchHandler" It delegates to a sequent
       of SearchComponents (see below) and supports distributed
       queries across multiple shards
    requestHandler/selectsolr.SearchHandler default values for query parameters can be specified, these
         will be overridden by parameters in the request
      defaultsechoParamsexplicitrowsdf In addition to defaults, "appends" params can be specified
         to identify values which should be appended to the list of
         multi-val params from the query (or the existing "defaults").
       In this example, the param "fq=instock:true" would be appended to
         any query time fq params the user may specify, as a mechanism for
         partitioning the index, independent of any user selected filtering
         that may also be desired (perhaps as a result of faceted searching).

         NOTE: there is *absolutely* nothing a client can do to prevent these
         "appends" values from being used, so don't use this mechanism
         unless you are sure you always want it.
      
       <lst name="appends">
         <str name="fq">inStock:true</str>
       </lst>
       "invariants" are a way of letting the Solr maintainer lock down
         the options available to Solr clients.  Any params values
         specified here are used regardless of what values may be specified
         in either the query, the "defaults", or the "appends" params.

         In this example, the facet.field and facet.query params would
         be fixed, limiting the facets clients can use.  Faceting is
         not turned on by default - but if the client does specify
         facet=true in the request, these are the only facets they
         will be able to see counts for; regardless of what other
         facet.field or facet.query params they may specify.

         NOTE: there is *absolutely* nothing a client can do to prevent these
         "invariants" values from being used, so don't use this mechanism
         unless you are sure you always want it.
      
       <lst name="invariants">
         <str name="facet.field">cat</str>
         <str name="facet.field">manu_exact</str>
         <str name="facet.query">price:[* TO 500]</str>
         <str name="facet.query">price:[500 TO *]</str>
       </lst>
       If the default list of SearchComponents is not desired, that
         list can either be overridden completely, or components can be
         prepended or appended to the default list.  (see below)
      
       <arr name="components">
         <str>nameOfCustomComponent1</str>
         <str>nameOfCustomComponent2</str>
       </arr>
       A request handler that returns indented JSON by default /querywt realtime get handler, guaranteed to return the latest stored fields of
       any document, without the need to commit or open a new searcher.  The
       current implementation relies on the updateLog feature being enabled. /getsolr.RealTimeGetHandleromitHeader A Robust Example 
       
       This example SearchHandler declaration shows off usage of the
       SearchHandler with many defaults declared

       Note that multiple instances of the same Request Handler
       (SearchHandler) can be registered multiple times with different
       names (and different init parameters)
    /browse VelocityResponseWriter settings velocityv.templatebrowsev.layoutlayouttitleSolritas Query settings defTypeedismaxqf
          text^0.5 features^1.0 name^1.2 sku^1.5 id^10.0 manu^1.1 cat^1.4
          title^10.0 description^5.0 keywords^5.0 author^2.0 resourcename^1.0
       100%q.alt*:*fl*,scoremlt.qf
         text^0.5 features^1.0 name^1.2 sku^1.5 id^10.0 manu^1.1 cat^1.4
         title^10.0 description^5.0 keywords^5.0 author^2.0 resourcename^1.0
       mlt.fltext,features,name,sku,id,manu,cat,title,description,keywords,author,resourcenamemlt.count Faceting defaults facetonfacet.fieldcatmanu_exactauthor_sfacet.queryGBfacet.mincountfacet.pivotcat,inStockfacet.range.otherafterfacet.rangepricef.price.facet.range.startf.price.facet.range.end600f.price.facet.range.gappopularityf.popularity.facet.range.startf.popularity.facet.range.endf.popularity.facet.range.gapmanufacturedate_dtf.manufacturedate_dt.facet.range.startNOW/YEAR-10YEARSf.manufacturedate_dt.facet.range.endNOWf.manufacturedate_dt.facet.range.gap+1YEARf.manufacturedate_dt.facet.range.otherbefore Highlighting defaults hlhl.flcontent features title namehl.encoderhtmlhl.simple.pre<b>hl.simple.post</b>f.title.hl.fragsizef.title.hl.alternateFieldf.name.hl.fragsizef.name.hl.alternateFieldf.content.hl.snippetsf.content.hl.fragsizef.content.hl.alternateFieldf.content.hl.maxAlternateFieldLength750 Spell checking defaults spellcheckspellcheck.extendedResultsspellcheck.countspellcheck.alternativeTermCountspellcheck.maxResultsForSuggestspellcheck.collatespellcheck.collateExtendedResultsspellcheck.maxCollationTriesspellcheck.maxCollations append spellchecking to our list of components last-components Update Request Handler.  
       
       http://wiki.apache.org/solr/UpdateXmlMessages

       The canonical Request Handler for Modifying the Index through
       commands specified using XML, JSON, CSV, or JAVABIN

       Note: Since solr1.1 requestHandlers requires a valid content
       type header if posted in the body. For example, curl now
       requires: -H 'Content-type:text/xml; charset=utf-8'
       
       To override the request content type and force a specific 
       Content-type, use the request parameter: 
         ?update.contentType=text/csv
       
       This handler will pick a response format to match the input
       if the 'wt' parameter is not explicit
    /updatesolr.UpdateRequestHandler See below for information on defining 
         updateRequestProcessorChains that can be used by name 
         on each Update Request
      
       <lst name="defaults">
         <str name="update.chain">dedupe</str>
       </lst>
        for back compat with clients using /update/json and /update/csv /update/jsonsolr.JsonUpdateRequestHandlerstream.contentType/update/csvsolr.CSVRequestHandlerapplication/csv Solr Cell Update Request Handler

       http://wiki.apache.org/solr/ExtractingRequestHandler 

    /update/extractstartuplazysolr.extraction.ExtractingRequestHandlerlowernamesuprefixignored_<str name="uprefix">attr_</str>fmap.content twitter feed schema capturecontent-type capture link hrefs but ignore div attributes 
      <str name="captureAttr">true</str>
      <str name="fmap.a">links</str>
      <str name="fmap.div">ignored_</str>
       Field Analysis Request Handler

       RequestHandler that provides much the same functionality as
       analysis.jsp. Provides the ability to specify multiple field
       types and field names in the same request and outputs
       index-time and query-time analysis for each of them.

       Request parameters are:
       analysis.fieldname - field name whose analyzers are to be used

       analysis.fieldtype - field type whose analyzers are to be used
       analysis.fieldvalue - text for index-time analysis
       q (or analysis.q) - text for query time analysis
       analysis.showmatch (true|false) - When set to true and when
           query analysis is performed, the produced tokens of the
           field value analysis will be marked as "matched" for every
           token that is produces by the query analysis
   /analysis/fieldsolr.FieldAnalysisRequestHandler Document Analysis Handler

       http://wiki.apache.org/solr/AnalysisRequestHandler

       An analysis handler that provides a breakdown of the analysis
       process of provided documents. This handler expects a (single)
       content stream with the following format:

       <docs>
         <doc>
           <field name="id">1</field>
           <field name="name">The Name</field>
           <field name="text">The Text Value</field>
         </doc>
         <doc>...</doc>
         <doc>...</doc>
         ...
       </docs>

    Note: Each document must contain a field which serves as the
    unique key. This key is used in the returned response to associate
    an analysis breakdown to the analyzed document.

    Like the FieldAnalysisRequestHandler, this handler also supports
    query analysis by sending either an "analysis.query" or "q"
    request parameter that holds the query text to be analyzed. It
    also supports the "analysis.showmatch" parameter which when set to
    true, all field tokens that match the query tokens will be marked
    as a "match". 
  /analysis/documentsolr.DocumentAnalysisRequestHandler Admin Handlers

       Admin Handlers - This will register all the standard admin
       RequestHandlers.  
    /admin/solr.admin.AdminHandlers This single handler is equivalent to the following... 
     <requestHandler name="/admin/luke"       class="solr.admin.LukeRequestHandler" />
     <requestHandler name="/admin/system"     class="solr.admin.SystemInfoHandler" />
     <requestHandler name="/admin/plugins"    class="solr.admin.PluginInfoHandler" />
     <requestHandler name="/admin/threads"    class="solr.admin.ThreadDumpHandler" />
     <requestHandler name="/admin/properties" class="solr.admin.PropertiesRequestHandler" />
     <requestHandler name="/admin/file"       class="solr.admin.ShowFileRequestHandler" >
     If you wish to hide files under ${solr.home}/conf, explicitly
       register the ShowFileRequestHandler using: 
    
     <requestHandler name="/admin/file" 
                     class="solr.admin.ShowFileRequestHandler" >
       <lst name="invariants">
         <str name="hidden">synonyms.txt</str> 
         <str name="hidden">anotherfile.txt</str> 
       </lst>
     </requestHandler>
     ping/healthcheck /admin/pingsolr.PingRequestHandlerinvariantssolrpingquery An optional feature of the PingRequestHandler is to configure the 
         handler with a "healthcheckFile" which can be used to enable/disable 
         the PingRequestHandler.
         relative paths are resolved against the data dir 
       <str name="healthcheckFile">server-enabled.txt</str>  Echo the request contents back to the client /debug/dumpsolr.DumpRequestHandlerechoHandler Solr Replication

       The SolrReplicationHandler supports replicating indexes from a
       "master" used for indexing and "slaves" used for queries.

       http://wiki.apache.org/solr/SolrReplication 

       It is also neccessary for SolrCloud to function (in Cloud mode, the 
       replication handler is used to bulk transfer segments when nodes 
       are added or need to recover).

       https://wiki.apache.org/solr/SolrCloud/
    /replicationsolr.ReplicationHandler
       To enable simple master/slave replication, uncomment one of the 
       sections below, depending on wether this solr instance should be 
       the "master" or a "slave".  If this instance is a "slave" you will 
       also need to fill in the masterUrl to point to a real machine.
    
       <lst name="master">
         <str name="replicateAfter">commit</str>
         <str name="replicateAfter">startup</str>
         <str name="confFiles">schema.xml,stopwords.txt</str>
       </lst>
    
       <lst name="slave">
         <str name="masterUrl">http://your-master-hostname:8983/solr</str>
         <str name="pollInterval">00:00:60</str>
       </lst>
     Search Components

       Search components are registered to SolrCore and used by 
       instances of SearchHandler (which can access them by name)
       
       By default, the following components are available:
       
       <searchComponent name="query"     class="solr.QueryComponent" />
       <searchComponent name="facet"     class="solr.FacetComponent" />
       <searchComponent name="mlt"       class="solr.MoreLikeThisComponent" />
       <searchComponent name="highlight" class="solr.HighlightComponent" />
       <searchComponent name="stats"     class="solr.StatsComponent" />
       <searchComponent name="debug"     class="solr.DebugComponent" />
   
       Default configuration in a requestHandler would look like:

       <arr name="components">
         <str>query</str>
         <str>facet</str>
         <str>mlt</str>
         <str>highlight</str>
         <str>stats</str>
         <str>debug</str>
       </arr>

       If you register a searchComponent to one of the standard names, 
       that will be used instead of the default.

       To insert components before or after the 'standard' components, use:
    
       <arr name="first-components">
         <str>myFirstComponentName</str>
       </arr>
    
       <arr name="last-components">
         <str>myLastComponentName</str>
       </arr>

       NOTE: The component registered with the name "debug" will
       always be executed after the "last-components" 
       
      Spell Check

        The spell check component can return a list of alternative spelling
        suggestions.  

        http://wiki.apache.org/solr/SpellCheckComponent
     searchComponentsolr.SpellCheckComponentqueryAnalyzerFieldTypetextSpell Multiple "Spell Checkers" can be declared and used by this
         component
       a spellchecker built from a field of the main index spellcheckersolr.DirectSolrSpellChecker the spellcheck distance measure used, the default is the internal levenshtein distanceMeasureinternal minimum accuracy needed to be considered a valid spellcheck suggestion accuracy0.5 the maximum #edits we consider when enumerating terms: can be 1 or 2 maxEdits the minimum shared prefix when enumerating terms minPrefix maximum number of inspections per result. maxInspections minimum length of a query term to be considered for correction minQueryLength maximum threshold of documents a query term can appear to be considered for correction maxQueryFrequency uncomment this to require suggestions to occur in 1% of the documents
      	<float name="thresholdTokenFrequency">.01</float>
       a spellchecker that can break or combine words.  See "/spell" handler below for usage wordbreaksolr.WordBreakSolrSpellCheckercombineWordsbreakWordsmaxChanges a spellchecker that uses a different distance measure 
       <lst name="spellchecker">
         <str name="name">jarowinkler</str>
         <str name="field">spell</str>
         <str name="classname">solr.DirectSolrSpellChecker</str>
         <str name="distanceMeasure">
           org.apache.lucene.search.spell.JaroWinklerDistance
         </str>
       </lst>
      a spellchecker that use an alternate comparator 

         comparatorClass be one of:
          1. score (default)
          2. freq (Frequency first, then score)
          3. A fully qualified class name
      
       <lst name="spellchecker">
         <str name="name">freq</str>
         <str name="field">lowerfilt</str>
         <str name="classname">solr.DirectSolrSpellChecker</str>
         <str name="comparatorClass">freq</str>
       A spellchecker that reads the list of words from a file 
       <lst name="spellchecker">
         <str name="classname">solr.FileBasedSpellChecker</str>
         <str name="name">file</str>
         <str name="sourceLocation">spellings.txt</str>
         <str name="characterEncoding">UTF-8</str>
         <str name="spellcheckIndexDir">spellcheckerFile</str>
       </lst>
       A request handler for demonstrating the spellcheck component.  

       NOTE: This is purely as an example.  The whole purpose of the
       SpellCheckComponent is to hook it into the request handler that
       handles your normal user queries so that a separate request is
       not needed to get suggestions.

       IN OTHER WORDS, THERE IS REALLY GOOD CHANCE THE SETUP BELOW IS
       NOT WHAT YOU WANT FOR YOUR PRODUCTION SYSTEM!
       
       See http://wiki.apache.org/solr/SpellCheckComponent for details
       on the request parameters.
    /spell Solr will use suggestions from both the 'default' spellchecker
           and from the 'wordbreak' spellchecker and combine them.
           collations (re-written queries) can include a combination of
           corrections from both spellcheckers spellcheck.dictionary Term Vector Component

       http://wiki.apache.org/solr/TermVectorComponent
    tvComponentsolr.TermVectorComponent A request handler for demonstrating the term vector component

       This is purely as an example.

       In reality you will likely want to add the component to your 
       already specified request handlers. 
    /tvrhbooltv Clustering Component

       http://wiki.apache.org/solr/ClusteringComponent

       You'll need to set the solr.cluster.enabled system property
       when running solr to run with clustering enabled:

            java -Dsolr.clustering.enabled=true -jar start.jar

    clusteringenable${solr.clustering.enabled:false}solr.clustering.ClusteringComponent Declare an engine  The name, only one can be named "default"  Class name of Carrot2 clustering algorithm.

           Currently available algorithms are:
           
           * org.carrot2.clustering.lingo.LingoClusteringAlgorithm
           * org.carrot2.clustering.stc.STCClusteringAlgorithm
           * org.carrot2.clustering.kmeans.BisectingKMeansClusteringAlgorithm
           
           See http://project.carrot2.org/algorithms.html for the
           algorithm's characteristics.
        carrot.algorithmorg.carrot2.clustering.lingo.LingoClusteringAlgorithm Overriding values for Carrot2 default algorithm attributes.

           For a description of all available attributes, see:
           http://download.carrot2.org/stable/manual/#chapter.components.
           Use attribute key as name attribute of str elements
           below. These can be further overridden for individual
           requests by specifying attribute key as request parameter
           name and attribute value as parameter value.
        LingoClusteringAlgorithm.desiredClusterCountBase Location of Carrot2 lexical resources.

           A directory from which to load Carrot2-specific stop words
           and stop labels. Absolute or relative to Solr config directory.
           If a specific resource (e.g. stopwords.en) is present in the
           specified dir, it will completely override the corresponding
           default one that ships with Carrot2.

           For an overview of Carrot2 lexical resources, see:
           http://download.carrot2.org/head/manual/#chapter.lexical-resources
        carrot.lexicalResourcesDirclustering/carrot2 The language to assume for the documents.

           For a list of allowed values, see:
           http://download.carrot2.org/stable/manual/#section.attribute.lingo.MultilingualClustering.defaultLanguage
       MultilingualClustering.defaultLanguageENGLISHstcorg.carrot2.clustering.stc.STCClusteringAlgorithm A request handler for demonstrating the clustering component

       This is purely as an example.

       In reality you will likely want to add the component to your 
       already specified request handlers. 
    /clusteringclustering.engineclustering.results The title field carrot.titlecarrot.url The field to cluster on carrot.snippetfeatures produce summaries carrot.produceSummary the maximum number of labels per cluster <int name="carrot.numDescriptions">5</int> produce sub clusters carrot.outputSubClusters
         text^0.5 features^1.0 name^1.2 sku^1.5 id^10.0 manu^1.1 cat^1.4
        Terms Component

       http://wiki.apache.org/solr/TermsComponent

       A component to return terms and document frequency of those
       terms
    termssolr.TermsComponent A request handler for demonstrating the terms component /terms Query Elevation Component

       http://wiki.apache.org/solr/QueryElevationComponent

       a search component that enables you to configure the top
       results for a given query regardless of the normal lucene
       scoring.
    elevatorsolr.QueryElevationComponent pick a fieldType to analyze queries queryFieldTypeconfig-fileelevate.xml A request handler for demonstrating the elevator component /elevate Highlighting Component

       http://wiki.apache.org/solr/HighlightingParameters
    solr.HighlightComponenthighlighthighlighting Configure the standard fragmenter  This could most likely be commented out in the "default" case fragmentergapsolr.highlight.GapFragmenterhl.fragsize A regular-expression-based fragmenter 
           (for sentence extraction) 
        solr.highlight.RegexFragmenter slightly smaller fragsizes work better because of slop 70 allow 50% slop on fragment sizes hl.regex.slop a basic sentence pattern hl.regex.pattern[-\w ,/\n\"']{20,200} Configure the standard formatter solr.highlight.HtmlFormatter<em></em> Configure the standard encoder solr.highlight.HtmlEncoder Configure the standard fragListBuilder fragListBuildersimplesolr.highlight.SimpleFragListBuilder Configure the single fragListBuilder solr.highlight.SingleFragListBuilder Configure the weighted fragListBuilder weightedsolr.highlight.WeightedFragListBuilder default tag FragmentsBuilder fragmentsBuildersolr.highlight.ScoreOrderFragmentsBuilder 
        <lst name="defaults">
          <str name="hl.multiValuedSeparatorChar">/</str>
        </lst>
         multi-colored tag FragmentsBuilder coloredhl.tag.pre
               <b style="background:yellow">,<b style="background:lawgreen">,
               <b style="background:aquamarine">,<b style="background:magenta">,
               <b style="background:palegreen">,<b style="background:coral">,
               <b style="background:wheat">,<b style="background:khaki">,
               <b style="background:lime">,<b style="background:deepskyblue">hl.tag.postboundaryScannersolr.highlight.SimpleBoundaryScannerhl.bs.maxScanhl.bs.chars.,!? 	
breakIteratorsolr.highlight.BreakIteratorBoundaryScanner type should be one of CHARACTER, WORD(default), LINE and SENTENCE hl.bs.typeWORD language and country are used when constructing Locale object.   And the Locale object will be used when getting instance of BreakIterator hl.bs.languageenhl.bs.countryUS Update Processors

       Chains of Update Processor Factories for dealing with Update
       Requests can be declared, and then used by name in Update
       Request Processors

       http://wiki.apache.org/solr/UpdateRequestProcessor

     Deduplication

       An example dedup update processor that creates the "id" field
       on the fly based on the hash code of some other fields.  This
       example has overwriteDupes set to false since we are using the
       id field as the signatureField and Solr will maintain
       uniqueness based on that anyway.  
       
    
     <updateRequestProcessorChain name="dedupe">
       <processor class="solr.processor.SignatureUpdateProcessorFactory">
         <bool name="enabled">true</bool>
         <str name="signatureField">id</str>
         <bool name="overwriteDupes">false</bool>
         <str name="fields">name,features,cat</str>
         <str name="signatureClass">solr.processor.Lookup3Signature</str>
       </processor>
       <processor class="solr.LogUpdateProcessorFactory" />
       <processor class="solr.RunUpdateProcessorFactory" />
     </updateRequestProcessorChain>
     Language identification

       This example update chain identifies the language of the incoming
       documents using the langid contrib. The detected language is
       written to field language_s. No field name mapping is done.
       The fields used for detection are text, title, subject and description,
       making this example suitable for detecting languages form full-text
       rich documents injected via ExtractingRequestHandler.
       See more about langId at http://wiki.apache.org/solr/LanguageDetection
    
     <updateRequestProcessorChain name="langid">
       <processor class="org.apache.solr.update.processor.TikaLanguageIdentifierUpdateProcessorFactory">
         <str name="langid.fl">text,title,subject,description</str>
         <str name="langid.langField">language_s</str>
         <str name="langid.fallback">en</str>
       </processor>
       <processor class="solr.LogUpdateProcessorFactory" />
       <processor class="solr.RunUpdateProcessorFactory" />
     </updateRequestProcessorChain>
     Script update processor

    This example hooks in an update processor implemented using JavaScript.

    See more about the script update processor at http://wiki.apache.org/solr/ScriptUpdateProcessor
  
    <updateRequestProcessorChain name="script">
      <processor class="solr.StatelessScriptUpdateProcessorFactory">
        <str name="script">update-script.js</str>
        <lst name="params">
          <str name="config_param">example config parameter</str>
        </lst>
      </processor>
      <processor class="solr.RunUpdateProcessorFactory" />
    </updateRequestProcessorChain>
   Response Writers

       http://wiki.apache.org/solr/QueryResponseWriter

       Request responses will be written using the writer specified by
       the 'wt' request parameter matching the name of a registered
       writer.

       The "default" writer is the default and will be used if 'wt' is
       not specified in the request.
     The following response writers are implicitly configured unless
       overridden...
    
     <queryResponseWriter name="xml" 
                          default="true"
                          class="solr.XMLResponseWriter" />
     <queryResponseWriter name="json" class="solr.JSONResponseWriter"/>
     <queryResponseWriter name="python" class="solr.PythonResponseWriter"/>
     <queryResponseWriter name="ruby" class="solr.RubyResponseWriter"/>
     <queryResponseWriter name="php" class="solr.PHPResponseWriter"/>
     <queryResponseWriter name="phps" class="solr.PHPSerializedResponseWriter"/>
     <queryResponseWriter name="csv" class="solr.CSVResponseWriter"/>
    queryResponseWritersolr.JSONResponseWriter For the purposes of the tutorial, JSON responses are written as
      plain text so that they are easy to read in *any* browser.
      If you expect a MIME type of "application/json" just remove this override.
     text/plain; charset=UTF-8
     Custom response writers can be declared as needed...
    solr.VelocityResponseWriter XSLT response writer transforms the XML output by any xslt file found
       in Solr's conf/xslt directory.  Changes to xslt files are checked for
       every xsltCacheLifetimeSeconds.  
    xsltsolr.XSLTResponseWriterxsltCacheLifetimeSeconds Query Parsers

       http://wiki.apache.org/solr/SolrQuerySyntax

       Multiple QParserPlugins can be registered by name, and then
       used in either the "defType" param for the QueryComponent (used
       by SearchHandler) or in LocalParams
     example of registering a query parser 
     <queryParser name="myparser" class="com.mycompany.MyQParserPlugin"/>
     Function Parsers

       http://wiki.apache.org/solr/FunctionQuery

       Multiple ValueSourceParsers can be registered by name, and then
       used as function names when using the "func" QParser.
     example of registering a custom function parser  
     <valueSourceParser name="myfunc" 
                        class="com.mycompany.MyValueSourceParser" />
     Document Transformers
       http://wiki.apache.org/solr/DocTransformers
    
     Could be something like:
     <transformer name="db" class="com.mycompany.LoadFromDatabaseTransformer" >
       <int name="connection">jdbc://....</int>
     </transformer>
     
     To add a constant value to all docs, use:
     <transformer name="mytrans2" class="org.apache.solr.response.transform.ValueAugmenterFactory" >
       <int name="value">5</int>
     </transformer>
     
     If you want the user to still be able to change it with _value:something_ use this:
     <transformer name="mytrans3" class="org.apache.solr.response.transform.ValueAugmenterFactory" >
       <double name="defaultValue">5</double>
     </transformer>

      If you are using the QueryElevationComponent, you may wish to mark documents that get boosted.  The
      EditorialMarkerFactory will do exactly that:
     <transformer name="qecBooster" class="org.apache.solr.response.transform.EditorialMarkerFactory" />
     Legacy config for the admin interface admindefaultQuery/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/test/resources/test-documents/testXML.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/flume-ng-morphline-solr-sink/src/test/resources/test-documentsdchttp://purl.org/dc/elements/1.1/oaidchttp://www.openarchives.org/OAI/2.0/oai_dc/Tika test documentcreatorRida BenjellounsubjectJavaXSLTJDOMIndexationFramework d'indexation des documents XML, HTML, PDF etc.. identifierhttp://www.apache.org2000-12-01T00:00:00.000Zapplication/mswordFrrightsArchimde et Lius  Chteauneuf testing chars en t/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sinks/pom.xmlFlume NG Sinksflume.sink.kudu.enabled/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sourcesflume-ng-sourcesFlume JMS Sourceorg.apache.flume.source.jmsorg.apache.geronimo.specsgeronimo-jms_1.1_specorg.apache.activemqactivemq-core/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/src/main/java/org/apache/flume/source/jms/DefaultJMSMessageConverter.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/src/main/java/org/apache/flume/source/jms/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/src/main/java/org/apache/flume/source/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/src/main/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/src/main/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/src/main/java/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/src/main/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/srcDefaultJMSMessageConverterDefaultJMSMessageConverter(java.lang.String)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/target/classes/org/apache/flume/source/jms/DefaultJMSMessageConverter.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/target/classes/org/apache/flume/source/jms/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/target/classes/org/apache/flume/source/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/target/classes/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/target/classes/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/target/classes/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/target/classes/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/targetbytesMessage"Unable to process message " + "of size "Unable to process message of size "Unable to read full message. " +
              "Read "Unable to read full message. Read " of total " of total textMessageobjectMessagebos"Error serializing object"Error serializing object"Error closing ObjectOutputStream"Error closing ObjectOutputStream"Error closing ByteArrayOutputStream"Error closing ByteArrayOutputStream<p>Converts BytesMessage, TextMessage, and ObjectMessageto a Flume Event. All Message Property names are addedas headers to the Event. The conversion of the body isas follows:</p><p><strong>BytesMessage:</strong> Body from message isset as the body of the Event.</p><p><strong>TextMessage:</strong> String body converted to a bytearray byte getBytes(charset). Charset defaults to UTF-8 but can beconfigured.</p><p><strong>ObjectMessage:</strong> Object is written toan ByteArrayOutputStream wrapped by an ObjectOutputStreamand the resulting byte array is the body of the message.</p>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/src/main/java/org/apache/flume/source/jms/InitialContextFactory.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/src/main/java/org/apache/flume/source/jms/JMSDestinationLocator.javaJMSDestinationLocatorJMSDestinationLocator()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/target/classes/org/apache/flume/source/jms/JMSDestinationLocator.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/src/main/java/org/apache/flume/source/jms/JMSDestinationType.javaJMSDestinationTypeJMSDestinationType()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/target/classes/org/apache/flume/source/jms/JMSDestinationType.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/src/main/java/org/apache/flume/source/jms/JMSMessageConsumer.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/target/classes/org/apache/flume/source/jms/JMSMessageConsumer.classrollback()commit()receiveNoWaitreceiveNoWait()receivereceive()JMSMessageConsumerJMSMessageConsumer(javax.naming.InitialContext,javax.jms.ConnectionFactory,java.lang.String,org.apache.flume.source.jms.JMSDestinationLocator,org.apache.flume.source.jms.JMSDestinationType,java.lang.String,int,long,org.apache.flume.source.jms.JMSMessageConverter,com.google.common.base.Optional,com.google.common.base.Optional,com.google.common.base.Optional,boolean,java.lang.String)messageConsumerdestinationmessageConverterpollTimeoutClass<JMSMessageConsumer>initialContextconnectionFactorydestinationNamedestinationLocatordestinationTypemessageSelectorclientIdcreateDurableSubscriptiondurableSubscriptionName"Batch size must be greater "
        + "than zero"Batch size must be greater than zero"Poll timeout cannot be " +
        "negative"Poll timeout cannot be negativestartupMsg"Connected to '%s' of type '%s' with " +
                      "user '%s', batch size '%d', selector '%s' "Connected to '%s' of type '%s' with user '%s', batch size '%d', selector '%s' Function<? super String,V>Supplier<? extends String>Optional<? extends String>or(java.lang.String)Optional<String>()"Could not create connection to broker"Could not create connection to broker"Could not create session"Could not create sessionEnum<JMSDestinationLocator>Comparable<JMSDestinationLocator>compareTo(org.apache.flume.source.jms.JMSDestinationLocator)EnumDesc<JMSDestinationLocator>DynamicConstantDesc<JMSDestinationLocator>Optional<EnumDesc<JMSDestinationLocator>>Class<JMSDestinationLocator>Enum<JMSDestinationLocator>(java.lang.String,int)"Could not create destination "Could not create destination "Could not find destination "Could not find destination "Could not create consumer"Could not create consumer"Took batch of %s from %s"Took batch of %s from %sruntimeExceptionjmsException"JMS provider has thrown runtime exception: "JMS provider has thrown runtime exception: "JMS Exception processing commit"JMS Exception processing commit"Runtime Exception processing commit"Runtime Exception processing commit"JMS Exception processing rollback"JMS Exception processing rollback"Runtime Exception processing rollback"Runtime Exception processing rollback"Could not destroy session"Could not destroy session"Could not destroy connection"Could not destroy connection/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/src/main/java/org/apache/flume/source/jms/JMSMessageConverter.javaConverts a JMS Message to an Event. It's recommendedthat sub-classes define a static sub-class of theinner Builder class to handle configuration. Alternatively,the sub-class can the Configurable interface.Implementors of JMSMessageConverter must either providea suitable builder or implement the Configurable interface./Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/src/main/java/org/apache/flume/source/jms/JMSSource.javacreateConsumercreateConsumer()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/target/classes/org/apache/flume/source/jms/JMSSource.classassertNotEmptyassertNotEmpty(java.lang.String,java.lang.String)getAllowedProtocolsgetAllowedProtocols()allowedSchemesjmsExceptionCountererrorThresholdproviderUrlinitialContextFactoryNameconsumerconverterinitialContextFactoryJAVA_SCHEMEClass<JMSSource>"JndiAllowedProtocols"JndiAllowedProtocolsalloweditemscheme"Invalid JNDI URI: "Invalid JNDI URI: "{}} is not a valid URI"{}} is not a valid URIdestinationTypeNamedestinationLocatorNameconverterClassNameconverterContextconnectionFactoryName"Could not read password file %s"Could not read password file %sisBuilderClass<JMSMessageConverter>Map<String,JMSMessageConverter>JMSMessageConverter[]Constructor<JMSMessageConverter>? super JMSMessageConverterClass<? super JMSMessageConverter>TypeVariable<Class<JMSMessageConverter>>TypeVariable<Class<JMSMessageConverter>>[]"Class %s is not a subclass of JMSMessageConverter"Class %s is not a subclass of JMSMessageConverter"Attempted configuration of %s, result = %s"Attempted configuration of %s, result = %s"Unable to create instance of converter %s"Unable to create instance of converter %s"Initial Context Factory is empty. This is specified by %s"Initial Context Factory is empty. This is specified by %s"Provider URL is empty. This is specified by %s"Provider URL is empty. This is specified by %s"Destination Name is empty. This is specified by %s"Destination Name is empty. This is specified by %s"Destination Type is empty. This is specified by %s"Destination Type is empty. This is specified by %s"Destination type '%s' is " +
          "invalid."Destination type '%s' is invalid."Only Destination type '%s' supports durable subscriptions."Only Destination type '%s' supports durable subscriptions.Enum<JMSDestinationType>Comparable<JMSDestinationType>compareTo(org.apache.flume.source.jms.JMSDestinationType)EnumDesc<JMSDestinationType>DynamicConstantDesc<JMSDestinationType>Optional<EnumDesc<JMSDestinationType>>Class<JMSDestinationType>Enum<JMSDestinationType>(java.lang.String,int)"You have to specify '%s' when using durable subscriptions."You have to specify '%s' when using durable subscriptions."If '%s' is set to true, '%s' has to be specified."If '%s' is set to true, '%s' has to be specified."'%s' is set, but '%s' is false."
          + "If you want to create a durable subscription, set %s to true."'%s' is set, but '%s' is false.If you want to create a durable subscription, set %s to true."Destination locator '%s' is " +
          "invalid."Destination locator '%s' is invalid."Batch size must be greater than 0"Batch size must be greater than 0contextProperties"Could not create initial context %s provider %s"Could not create initial context %s provider %s"Could not lookup ConnectionFactory"Could not lookup ConnectionFactorychannelException"Error appending event to channel. "
          + "Channel might be full. Consider increasing the channel "
          + "capacity or make sure the sinks perform faster.""JMSException consuming events"JMSException consuming events"Exceeded JMSException threshold, closing consumer"Exceeded JMSException threshold, closing consumer"Unexpected error processing events"Unexpected error processing events"Unable to create consumer"Unable to create consumer"Creating new consumer for "Creating new consumer for  setup by constructor setup by configuration Provide properties for connecting via JNDI/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-jms-source/src/main/java/org/apache/flume/source/jms/JMSSourceConfiguration.java"initialContextFactory""connectionFactory""ConnectionFactory"ConnectionFactory"providerURL"providerURL"destinationName""destinationType""destinationLocator""CDI"CDI"topic""messageSelector""clientId""userName""errorThreshold""pollTimeout""converter"".type".type".charset".charset"createDurableSubscription""durableSubscriptionName"/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source1.10.0Flume Scribe Source76org.apache.flume.source.scribemaven-compiler-plugin3.8.1 Thrift compilation script nonThrift**/generated-sources/** Specify version 1.7.304.13.10.20.01.10.19/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/src/main/java/org/apache/flume/source/scribe/LogEntry.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/src/main/java/org/apache/flume/source/scribe/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/src/main/java/org/apache/flume/source/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/src/main/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/src/main/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/src/main/java/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/src/main/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/src/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/LogEntry.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/org/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/org/apache/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/org/apache/thrift/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/org/apache/thrift/scheme/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/org/apache/thrift/scheme/IScheme.classMESSAGE_FIELD_DESCCATEGORY_FIELD_DESCTBase<LogEntry,_Fields>/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/org/apache/thrift/TBase.classComparable<LogEntry>"LogEntry"LogEntry"category"category"message"/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/org/apache/thrift/scheme/StandardScheme.class/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/org/apache/thrift/scheme/TupleScheme.classmerge(org.apache.flume.source.scribe.LogEntry._Fields,org.apache.thrift.meta_data.FieldMetaData,java.util.function.BiFunction)compute(org.apache.flume.source.scribe.LogEntry._Fields,java.util.function.BiFunction)computeIfPresent(org.apache.flume.source.scribe.LogEntry._Fields,java.util.function.BiFunction)computeIfAbsent(org.apache.flume.source.scribe.LogEntry._Fields,java.util.function.Function)replace(org.apache.flume.source.scribe.LogEntry._Fields,org.apache.thrift.meta_data.FieldMetaData)replace(org.apache.flume.source.scribe.LogEntry._Fields,org.apache.thrift.meta_data.FieldMetaData,org.apache.thrift.meta_data.FieldMetaData)putIfAbsent(org.apache.flume.source.scribe.LogEntry._Fields,org.apache.thrift.meta_data.FieldMetaData)put(org.apache.flume.source.scribe.LogEntry._Fields,org.apache.thrift.meta_data.FieldMetaData)Class<LogEntry>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/LogEntry$_Fields.classmerge(java.lang.String,org.apache.flume.source.scribe.LogEntry._Fields,java.util.function.BiFunction)replace(java.lang.String,org.apache.flume.source.scribe.LogEntry._Fields)replace(java.lang.String,org.apache.flume.source.scribe.LogEntry._Fields,org.apache.flume.source.scribe.LogEntry._Fields)putIfAbsent(java.lang.String,org.apache.flume.source.scribe.LogEntry._Fields)getOrDefault(java.lang.Object,org.apache.flume.source.scribe.LogEntry._Fields)put(java.lang.String,org.apache.flume.source.scribe.LogEntry._Fields)newTreeNode(int,java.lang.String,org.apache.flume.source.scribe.LogEntry._Fields,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.flume.source.scribe.LogEntry._Fields,java.util.HashMap.Node)putVal(int,java.lang.String,org.apache.flume.source.scribe.LogEntry._Fields,boolean,boolean)this_present_categorythat_present_categorythis_present_messagethat_present_messagepresent_categorypresent_message? extends LogEntryClass<? extends LogEntry>Map<String,? extends LogEntry>LogEntry[]Constructor<? extends LogEntry>TypeVariable<Class<? extends LogEntry>>TypeVariable<Class<? extends LogEntry>>[]"LogEntry("LogEntry("category:"category:"message:"message:LogEntryStandardSchemeFactoryLogEntryStandardSchemeFactory()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/LogEntry$LogEntryStandardSchemeFactory.classLogEntryStandardSchemeLogEntryStandardScheme()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/LogEntry$LogEntryStandardScheme.classStandardScheme<LogEntry>IScheme<LogEntry>LogEntryTupleSchemeFactoryLogEntryTupleSchemeFactory()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/LogEntry$LogEntryTupleSchemeFactory.classLogEntryTupleSchemeLogEntryTupleScheme()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/LogEntry$LogEntryTupleScheme.classTupleScheme<LogEntry> CATEGORY MESSAGEReturns true if field category is set (has been assigned a value) and false otherwiseReturns true if field message is set (has been assigned a value) and false otherwise/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/src/main/java/org/apache/flume/source/scribe/ResultCode.javaResultCodeResultCode(int)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/ResultCode.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/src/main/java/org/apache/flume/source/scribe/Scribe.javaList<LogEntry>SequencedCollection<LogEntry>Collection<LogEntry>Iterable<LogEntry>/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/org/apache/thrift/async/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/org/apache/thrift/async/AsyncMethodCallback.class/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/org/apache/thrift/TServiceClientFactory.class"Log""Log failed: unknown result"Log failed: unknown result/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Scribe$AsyncClient$Factory.class/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/org/apache/thrift/async/TAsyncClientFactory.classTAsyncMethodCall<?>/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/org/apache/thrift/async/TAsyncMethodCall.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Scribe$AsyncClient$Log_call.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Scribe$Processor.class/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/org/apache/thrift/ProcessFunction.class/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/org/apache/thrift/TBaseProcessor.classLog<>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Scribe$Processor$Log.classLog<>()getResult(org.apache.flume.source.scribe.Scribe.Iface,org.apache.flume.source.scribe.Scribe.Log_args)Log()ProcessFunction<I,Log_args>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Scribe$AsyncProcessor.class/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/org/apache/thrift/AsyncProcessFunction.class/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/org/apache/thrift/TBaseAsyncProcessor.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Scribe$AsyncProcessor$Log.classAsyncMethodCallback<ResultCode>start(org.apache.flume.source.scribe.Scribe.AsyncIface,org.apache.flume.source.scribe.Scribe.Log_args,org.apache.thrift.async.AsyncMethodCallback)AsyncProcessFunction<I,Log_args,ResultCode>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Scribe$AsyncProcessor$Log$1.classonComplete(org.apache.flume.source.scribe.ResultCode)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Scribe$Log_args.classMESSAGES_FIELD_DESCTBase<Log_args,_Fields>Comparable<Log_args>"Log_args"Log_args"messages"merge(org.apache.flume.source.scribe.Scribe.Log_args._Fields,org.apache.thrift.meta_data.FieldMetaData,java.util.function.BiFunction)compute(org.apache.flume.source.scribe.Scribe.Log_args._Fields,java.util.function.BiFunction)computeIfPresent(org.apache.flume.source.scribe.Scribe.Log_args._Fields,java.util.function.BiFunction)computeIfAbsent(org.apache.flume.source.scribe.Scribe.Log_args._Fields,java.util.function.Function)replace(org.apache.flume.source.scribe.Scribe.Log_args._Fields,org.apache.thrift.meta_data.FieldMetaData)replace(org.apache.flume.source.scribe.Scribe.Log_args._Fields,org.apache.thrift.meta_data.FieldMetaData,org.apache.thrift.meta_data.FieldMetaData)putIfAbsent(org.apache.flume.source.scribe.Scribe.Log_args._Fields,org.apache.thrift.meta_data.FieldMetaData)put(org.apache.flume.source.scribe.Scribe.Log_args._Fields,org.apache.thrift.meta_data.FieldMetaData)Class<Log_args>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Scribe$Log_args$_Fields.classmerge(java.lang.String,org.apache.flume.source.scribe.Scribe.Log_args._Fields,java.util.function.BiFunction)replace(java.lang.String,org.apache.flume.source.scribe.Scribe.Log_args._Fields)replace(java.lang.String,org.apache.flume.source.scribe.Scribe.Log_args._Fields,org.apache.flume.source.scribe.Scribe.Log_args._Fields)putIfAbsent(java.lang.String,org.apache.flume.source.scribe.Scribe.Log_args._Fields)getOrDefault(java.lang.Object,org.apache.flume.source.scribe.Scribe.Log_args._Fields)put(java.lang.String,org.apache.flume.source.scribe.Scribe.Log_args._Fields)newTreeNode(int,java.lang.String,org.apache.flume.source.scribe.Scribe.Log_args._Fields,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.flume.source.scribe.Scribe.Log_args._Fields,java.util.HashMap.Node)putVal(int,java.lang.String,org.apache.flume.source.scribe.Scribe.Log_args._Fields,boolean,boolean)__this__messagesArrayList<LogEntry>AbstractList<LogEntry>AbstractCollection<LogEntry>ArrayList<LogEntry>(int)Spliterator<LogEntry>? super LogEntryConsumer<? super LogEntry>Iterator<LogEntry>Stream<LogEntry>BaseStream<LogEntry,Stream<LogEntry>>Predicate<? super LogEntry>Collection<? extends LogEntry>Iterable<? extends LogEntry>add(org.apache.flume.source.scribe.LogEntry)AbstractCollection<LogEntry>()addLast(org.apache.flume.source.scribe.LogEntry)addFirst(org.apache.flume.source.scribe.LogEntry)ListIterator<LogEntry>add(int,org.apache.flume.source.scribe.LogEntry)set(int,org.apache.flume.source.scribe.LogEntry)Comparator<? super LogEntry>UnaryOperator<LogEntry>Function<LogEntry,LogEntry>AbstractList<LogEntry>()ArrayList<LogEntry>(java.util.Collection)ArrayList<LogEntry>()this_present_messagesthat_present_messagespresent_messages? extends Log_argsClass<? extends Log_args>Map<String,? extends Log_args>Log_args[]Constructor<? extends Log_args>TypeVariable<Class<? extends Log_args>>TypeVariable<Class<? extends Log_args>>[]"Log_args("Log_args("messages:"messages:Log_argsStandardSchemeFactoryLog_argsStandardSchemeFactory()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Scribe$Log_args$Log_argsStandardSchemeFactory.classLog_argsStandardSchemeLog_argsStandardScheme()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Scribe$Log_args$Log_argsStandardScheme.classStandardScheme<Log_args>IScheme<Log_args>_list0_elem1_i2_iter3Log_argsTupleSchemeFactoryLog_argsTupleSchemeFactory()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Scribe$Log_args$Log_argsTupleSchemeFactory.classLog_argsTupleSchemeLog_argsTupleScheme()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Scribe$Log_args$Log_argsTupleScheme.classTupleScheme<Log_args>_list5_elem6_i7/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Scribe$Log_result.classTBase<Log_result,_Fields>Comparable<Log_result>"Log_result"Log_resultmerge(org.apache.flume.source.scribe.Scribe.Log_result._Fields,org.apache.thrift.meta_data.FieldMetaData,java.util.function.BiFunction)compute(org.apache.flume.source.scribe.Scribe.Log_result._Fields,java.util.function.BiFunction)computeIfPresent(org.apache.flume.source.scribe.Scribe.Log_result._Fields,java.util.function.BiFunction)computeIfAbsent(org.apache.flume.source.scribe.Scribe.Log_result._Fields,java.util.function.Function)replace(org.apache.flume.source.scribe.Scribe.Log_result._Fields,org.apache.thrift.meta_data.FieldMetaData)replace(org.apache.flume.source.scribe.Scribe.Log_result._Fields,org.apache.thrift.meta_data.FieldMetaData,org.apache.thrift.meta_data.FieldMetaData)putIfAbsent(org.apache.flume.source.scribe.Scribe.Log_result._Fields,org.apache.thrift.meta_data.FieldMetaData)put(org.apache.flume.source.scribe.Scribe.Log_result._Fields,org.apache.thrift.meta_data.FieldMetaData)Class<ResultCode>Class<Log_result>/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Scribe$Log_result$_Fields.classmerge(java.lang.String,org.apache.flume.source.scribe.Scribe.Log_result._Fields,java.util.function.BiFunction)replace(java.lang.String,org.apache.flume.source.scribe.Scribe.Log_result._Fields)replace(java.lang.String,org.apache.flume.source.scribe.Scribe.Log_result._Fields,org.apache.flume.source.scribe.Scribe.Log_result._Fields)putIfAbsent(java.lang.String,org.apache.flume.source.scribe.Scribe.Log_result._Fields)getOrDefault(java.lang.Object,org.apache.flume.source.scribe.Scribe.Log_result._Fields)put(java.lang.String,org.apache.flume.source.scribe.Scribe.Log_result._Fields)newTreeNode(int,java.lang.String,org.apache.flume.source.scribe.Scribe.Log_result._Fields,java.util.HashMap.Node)newNode(int,java.lang.String,org.apache.flume.source.scribe.Scribe.Log_result._Fields,java.util.HashMap.Node)putVal(int,java.lang.String,org.apache.flume.source.scribe.Scribe.Log_result._Fields,boolean,boolean)Enum<ResultCode>Comparable<ResultCode>compareTo(org.apache.flume.source.scribe.ResultCode)EnumDesc<ResultCode>DynamicConstantDesc<ResultCode>Optional<EnumDesc<ResultCode>>Enum<ResultCode>(java.lang.String,int)? extends Log_resultClass<? extends Log_result>Map<String,? extends Log_result>Log_result[]Constructor<? extends Log_result>TypeVariable<Class<? extends Log_result>>TypeVariable<Class<? extends Log_result>>[]"Log_result("Log_result(Log_resultStandardSchemeFactoryLog_resultStandardSchemeFactory()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Scribe$Log_result$Log_resultStandardSchemeFactory.classLog_resultStandardSchemeLog_resultStandardScheme()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Scribe$Log_result$Log_resultStandardScheme.classStandardScheme<Log_result>IScheme<Log_result>Log_resultTupleSchemeFactoryLog_resultTupleSchemeFactory()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Scribe$Log_result$Log_resultTupleSchemeFactory.classLog_resultTupleSchemeLog_resultTupleScheme()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/Scribe$Log_result$Log_resultTupleScheme.classTupleScheme<Log_result> MESSAGESReturns true if field messages is set (has been assigned a value) and false otherwise/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/src/main/java/org/apache/flume/source/scribe/ScribeSource.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/ScribeSource.classmaxReadBufferBytesworkersDEFAULT_MAX_READ_BUFFER_BYTESDEFAULT_WORKERSClass<ScribeSource>149916384000"maxReadBufferBytes""workerThreads"workerThreadsStartupStartup()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/ScribeSource$Startup.classProcessor<>(org.apache.flume.source.scribe.Scribe.Iface)TBaseProcessor<>(java.lang.Object,java.util.Map)Processor(org.apache.flume.source.scribe.Scribe.Iface,java.util.Map)Processor<>(org.apache.flume.source.scribe.Scribe.Iface,java.util.Map)Processor(org.apache.flume.source.scribe.Scribe.Iface)AbstractServerArgs<Args>/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/org/apache/thrift/server/Users/burakyetistiren/.m2/repository/org/apache/thrift/libthrift/0.20.0/libthrift-0.20.0.jar/org/apache/thrift/server/TServer$AbstractServerArgs.classAbstractServerArgs<Args>(org.apache.thrift.transport.TServerTransport)"Starting Scribe Source on port "Starting Scribe Source on port "Scribe failed"Scribe failedstartupThread"Failed initialization of ScribeSource"Failed initialization of ScribeSource"Scribe source stopping"Scribe source stopping"Scribe source stopped. Metrics:{}"Scribe source stopped. Metrics:{}ReceiverReceiver()/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-scribe-source/target/classes/org/apache/flume/source/scribe/ScribeSource$Receiver.class"Scribe source handling failure"Scribe source handling failureFlume should adopt the Scribe entry {@code LogEntry} from existingScribe system. Mostly, we may receive message from local Scribe and Flumetake responsibility of central Scribe.We use Thrift without deserializing, throughput has 2X increasing/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-taildir-source/pom.xml/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-taildir-sourceFlume Taildir Sourceorg.apache.flume.source.taildir/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-taildir-source/src/main/java/org/apache/flume/source/taildir/ReliableTaildirEventReader.java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-taildir-source/src/main/java/org/apache/flume/source/taildir/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-taildir-source/src/main/java/org/apache/flume/source/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-taildir-source/src/main/java/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-taildir-source/src/main/java/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-taildir-source/src/main/java/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-taildir-source/src/main/java/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-taildir-source/src/main/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-taildir-source/srcopenFile(java.io.File,java.util.Map,long,long)/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-taildir-source/target/classes/org/apache/flume/source/taildir/ReliableTaildirEventReader.class/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-taildir-source/target/classes/org/apache/flume/source/taildir/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-taildir-source/target/classes/org/apache/flume/source/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-taildir-source/target/classes/org/apache/flume/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-taildir-source/target/classes/org/apache/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-taildir-source/target/classes/org/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-taildir-source/target/classes/Users/burakyetistiren/Desktop/Subject_Prog_CodeQL_Taint/src/main/java/logging/flume-ng-sources/flume-taildir-source/targetgetInodegetInode(java.io.File)Table<String,String,String>/Users/burakyetistiren/.m2/repository/com/google/guava/guava/29.0-jre/guava-29.0-jre.jar/com/google/common/collect/Table.class"Use ImmutableTable, HashBasedTable, or another implementation"Use ImmutableTable, HashBasedTable, or another implementationReliableTaildirEventReaderReliableTaildirEventReader(java.util.Map,com.google.common.collect.Table,java.lang.String,boolean,boolean,boolean,boolean,java.lang.String)cachePatternMatchingaddByteOffsetupdateTimeMap<Long,TailFile>tailFilesheaderTableList<TaildirMatcher>SequencedCollection<TaildirMatcher>Collection<TaildirMatcher>Iterable<TaildirMatcher>taildirCacheHashMap<Long,TailFile>AbstractMap<Long,TailFile>Class<ReliableTaildirEventReader>filePathspositionFilePathskipToEndArrayList<TaildirMatcher>AbstractList<TaildirMatcher>AbstractCollection<TaildirMatcher>"Initializing {} with directory={}"Initializing {} with directory={}Map<String,ReliableTaildirEventReader>ReliableTaildirEventReader[]Constructor<ReliableTaildirEventReader>? super ReliableTaildirEventReaderClass<? super ReliableTaildirEventReader>      l    